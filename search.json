[{"title":"学习的方法论（Learning How to Learn）","url":"/2022/03/04/learningHowToLearn/","content":"\n\n\n## 思考的方式：专注与分散（Focused versus Diffuse Thinking）\n\n首先设想一下，如果你在想一个问题，但始终得不到答案，你会怎么办？对没有脑子的僵尸（zombie）来说，它可以不断地撞墙，但我们比它们聪明那么多，应该怎么做才能学得更好而且不焦虑呢？这里我们就要引入思考的两种方式：**专注模式（Focused Mode）** 和 **分散模式（Diffused Mode）**。<br>\n\n专注地思考似乎很好理解，但分散地思考就感觉不太好理解了。我们可以将大脑类比为一个弹珠机，弹珠机的每个点代表着大脑的某个区域。<br>\n\n对于**专注模式**而言，你已经**熟悉**了某个/种问题如何解决，这种解决方式就可以用弹珠机的特定几个点的碰撞模式来表示，当你的想法（Thought）在这几个点碰撞之后，boom！你的问题就解决了！也就是说，对于一个你熟悉的问题，你知道你的想法在的大脑中应该走怎样的路径/模式（或者说你的思考的起始点应该在哪），你的问题就能解决，于是你会选择专注到这个路径/模式上，这就是思考的专注模式。比如你正在解你很轻松就能解出来的薛定谔方程（嗯？），又或者你在进行着你熟悉的文学批评。专注模式更利于我们解决问题。<br>\n\n而对于**分散模式**而言，你遇到了某个你从没遇到过的问题，所以首先你并不知道怎么解决，这个时候你的想法需要在你大脑的弹珠机里不断乱撞，最终才能找到一个出路。也就是说，对于一个你**陌生**的问题，你并不知道问题的解决方式是什么，你只能在大脑中不断探索，无法专注到某个路径/模式上（或者说你并不知道你的思考的起始点应该在哪），这就是思考的分散模式。分散模式更利于我们进行创造性思考。<br>\n\n![](FocusDiffsedThinking.png)\n\n所以，这样的两种思考方式对我们来说有什么启示意义？上面说了，分散模式更利于创造性思考，而专注模式利于问题的解决。也就是说，我们要学会在两种思考模式之间进行转换，特别是对于对创新性有高要求的科研工作者而言。<br>\n\n## 拖延、记忆与睡眠（Procrastination, Memory and Sleep）\n每个人或多或少都有点**拖延症**（应该吧），但是拖延是怎么发生的呢？说来其实也简单，首先我们遇到了某件事让我们不开心/焦虑，然后我们的大脑会驱使我们去做开心的事情以获得快乐。<br>\n![](Procrastination.png)\n那么怎么克服拖延症呢？著名的 **番茄工作法（Pomodoro）** 其实就是一个很有效的方式！具体操作如下：\n- 定时25分钟，大部分人都可以保持25分钟的专注时间；\n- 关掉所有会对你有打扰的源，以帮助你更加专注；\n- 专注、专注再专注；\n- 25分钟结束之后，一定要给自己一点奖赏！（促进多巴胺分泌）可以是一杯咖啡、一次闲聊、几分钟的上网甚至只是一次拉伸（休息一段时间）。\n\n另外一个克服拖延症的方法是给自己**积极的心理暗示**（比如“不能再浪费时间了！”），且为了防止拖延，应该**避免把注意力集中在结果**上，而应该是在**执行过程**中（比如告诉自己，不是“不要写完这个作业”，而是“我要花20分钟写作业上”，因为结果往往是造成你不开心/焦虑的来源）。<br>\n\n而关于记忆，我们常说的就是**熟能生巧**（Practice makes permanent）。所以要记住一个概念，练习很重要（这也是让工作记忆（working memory）转变为长期记忆（long term memory）的关键，此外练习也应该优先考虑难题）。但是，填鸭式的学习并不提倡，有效的学习应该是**间隔重复（Spaced Repetition）**的，这也是为什么番茄工作法要每25分钟休息一次的原因。做个比喻，在专注状态下，我们的大脑在用水泥讲我们大脑中的砖块（神经元）进行连接，而休息时间，我们的大脑更像是在会水泥墙砖进行修整（或者说让水泥干燥以更好支撑砖块），我们一次不能用太多水泥，而且如果没有时间进行修整的话，我们的大脑最终记忆/学习的结果可能会如下右图所示。<br>\n![](BuildingStrongNeuralStructure.png)\n\n说到记忆，另一个不得不提的事实就是，图像对记忆非常重要（我们对图像的记忆能力远大于文字）！也就是说，将要记忆的信息**图像化**，是一个重要的记忆技巧。另外要将要记忆的内容进行**有意义地分组**，也很重要（比如要记住五线谱上四个间对应的几个音（分别是F、A、C、E），那我们就可以用face这个单词来记忆）。以上两点也是**记忆宫殿**背后的机理，其很适合被用于记忆一个列表的内容，而且有趣的是，用了记忆宫殿，记忆的过程又会变成一种创造力训练。<br>\n\n对于记忆做个总结，其实要将短期记忆转变为长期记忆，有两个条件：首先，它应该是**令人印象深刻**的（足够夸张，比如一只在飞的牛）；另一个则是，内容需要我们的**重复和回顾**（重复间隔可以逐渐变长，anki在这方面有内置算法帮助实现）。实践上来说，手写（手写也能帮助记忆）索引卡（一面写着索引，另一面写着内容，同时也可以画出一些令人难忘的图像），且必要时大声朗读（建立听觉联系），也是一个有效的重复方式。<br>\n\n也许睡眠看起来是一件浪费时间的事情，但其实，在我们睡觉的时候，大脑中一天积累的有毒物质会被清理，所以睡眠不足也会让人思维能力下降，甚至产生头痛、抑郁症等后果。实际上，**睡眠**在记忆和学习中也是很重要的一部分：在睡眠的时候，大脑会将学习和思考过的想法概念进行整理，清除一些不太重要的内容，并增强重要的内容（在大脑中重复（Rehearsal）学习到的内容以增强记忆）。所以一个好的方式是：在睡前学习自己想学的内容，并不断告诉自己想梦到自己学的内容（相信潜意识的能力！），这样在睡眠中加强学到内容的记忆的概率会变大。<br>\n\n\n\n## 组块（Chunkings）\n\n首先什么是 **组块（Chunking）** ？设想你感知这个世界的一小部分（比如学习特定内容）在你脑子里就是一小块拼图，那么组块就是把这些碎片化的小拼图给拼到一起，成为一个更大的相互嵌套、有序连接的大拼图，而大拼图之间又可以组成更大的拼图。下次当你需要回忆起相关的内容，你就可以想起对应的整个拼图。这背后的神经科学其实就是：**组块就是大脑中的神经元网络，他们共同发放，接着就连在了一起（Firing together, wiring together）**。再联系到人的工作记忆容量只有4左右，我们将学到的东西组块之后，当我们对其调用时就只会占用一个工作记忆的槽位，这也变相增加了我们的记忆容量。<br>\n![](chunks-0.png)\n\n那么怎么形成我们大脑中的组块呢？可以参照以下步骤：\n- 首先，对你想要组块化的信息**全神贯注**（不要被外部信息干扰，如回复信息）。我们可以把大脑连接组块的机制想象成一只有四个触手的章鱼（人类的工作记忆容量大致只有4！），所以当其中的某个触手被其他事情占用时，你大脑里的章鱼就无法触及到新知识的各个方面；\n![](chunks.png)\n- 其次，你对建立组块的对象要有**基本的理解**。理解是一种强力胶，会帮助你找到不同碎片的联系，以建立更大的组块，且知道如何使用它们（所以记得做题、亲自操作来帮助你加强理解！）；\n![](chunks-2.png)\n- 第三，要了解组块的**背景知识**。这样你就不仅知道如何使用组块，还明白应该在什么时候用它（Top-down的big piture）。比如在阅读时，可以先过一遍文章/书籍的标题/节标题，及其中的图，会帮助你大致了解其内容。换句话说，其实就是事先了解你要建立连接的组块之间可能的联系。\n![](chunks-3.png)\n\n\n## 回顾（Recall）\n\n2011年有一篇发表在 *Science* 上的论文表明，在学习某个材料之后，拿走材料，尽力回想自己学到的东西，实在想不起来再重新学习材料，学习效果会远远好于繁杂的反复阅读、或者绘制据说很有效果的思维导图。<br>\n\n> Karpicke, J. D., & Blunt, J. R. (2011). Retrieval practice produces more learning than elaborative studying with concept mapping. Science, 331(6018), 772-775. doi: 10.1126/science.1199327\n\n在回顾的时候，我们加深了对材料的理解，这也有助于我们形成知识组块。但同时需要强调的是，思维导图并不是一种无效的学习方法，而是应该在对知识组块有个清晰的理解之后（打好地基），才能起到更好的效果。<br>\n此外，尽管回顾不起来一些内容需要我们重新阅读材料，但也别忘了前面提到的间隔重复练习才更有效果！所以，在两次阅读之间，要有足够的间隔时间。<br>\n再联系到阅读文献，每读一部分内容，用自己的话在旁边总结这部分讲了什么，效果会远远好过单纯的（自欺欺人的）对整篇文章进行高亮、划线。<br>\n总结一下：**回顾** 是很重要的学习的过程！千万不要让自己掉进以为书里的或者谷歌上的内容就印在自己脑海里的 **学习的错觉（Illusions of Competence）**！<br>\n此外，在不同于学习发生的环境中进行回顾，也可以帮助我们学得更加牢固。（避免了环境对潜意识的提示作用。）<br>\n\n## 过度识记与交叉学习（Overlearning and Interleaving）\n**过度识记** 有其自身的意义，它能够帮助我们使得 **行为自动化**（如演奏钢琴），自动性（Automaticity）在紧张的时候确实很有用。但要警惕在单一学习阶段的重复性过度识记，这可能是对宝贵的学习时间的浪费，容易产生学习的错觉（觉得一切都很简单），也会更容易让你 **陷入思维定势（Einstellung）**。<br>\n而**交叉学习**是减弱思维定势的一种好的方式，也就是说，我们可以尝试在学习中，在需要不同技术和策略的问题以及情形中来回切换。但要注意的是，交叉学习也需要发生在对学习的内容有基本的掌握之后。交叉学习会让大脑**更具有灵活性和创造性**。当你在一个学科的内容内交叉学习，你会开始在这个学科内发展创造力；而当你在多个学科的内容之间交叉学习，你会更容易找到这些学科之间的联系，这也能进一步提高你的创造力。<br>\n要做出真正有创造性的工作，一定要避开思维定势，正如Thomas S. Kuhn在 *The Structure of Scientific Revolutions* 里所说，科学中的大部分范式转变，都是由年轻人或者之前学习其他学科的人所提出。加油吧！争取做这个年轻人！<br>\n\n\n\n## 习惯（Habbits）\n我们每个人大脑中都仿佛有一个僵尸（zombie），不加思考地做着某些事情，它驱动着我们的习惯。那么怎么驾驭这个“僵尸”让它为我们服务而不是耽误我们干事呢？养成习惯有4个要素，我们分别对他们进行阐述：\n- **信号（the cue）**：信号可以是地点（location）、时间（time）、你的感觉（how you feel，对别人或者刚刚发生的事）和你的反应（reactions，对别人或者刚刚发生的事）。那么破坏掉触发坏习惯的信号，就是一个好的改掉坏习惯的方式（比如关掉手机或者断掉互联网），而在干正事之前做一点小事作为自己的开始信号，以后执行这个动作之后就会更容易就进入状态（比如喝一杯咖啡）；\n- **惯式（the routine)**：做好计划（plan），比如在看书的时候都呆在某个椅子上，将坐在这个椅子与看书强绑定在一起，这样的惯式会也会帮助你养成好习惯；\n- **奖赏（the reward）**：记住，习惯的力量之所以强大，最重要的就是因为习惯制造了神经系统的欲望。所以首先要调查一下自己喜欢的是什么？比如说要改掉玩游戏的习惯，那么你要知道什么样的奖励对你来说会比游戏更诱人，并将它附加在你想养成的行为之后。只有当大脑开始期待那个奖励，旧习惯才能得到重置；\n- **信念（the belief）**：首先你要相信你自己能做到，否则当事情变得困难的时候（很经常的情况），你就会觉得自己做不到，想着要回到之前的舒适区（旧习惯）之中。可以与志同道合的朋友组成小组，相互鼓励，帮助自己坚持信念。\n\n\n## 待办事项（To-do list）\n对大多数人来说，学习需要在不同的每天繁杂的任务中达到平衡。一个好的方式就是在每周写下 **本周关键任务列表（Weekly list of key tasks）**。每一天**睡前**，又写下接下来一天的每日任务（因为这样可以召集你大脑中的僵尸（潜意识）去帮助你完成任务）。如果你不把任务写下来，那么它们就会占用你宝贵的工作记忆的槽位，你大可以将其空出来干更有用的事情。<br>\n同时我们在安排任务的时候，应该做好**生活和学习的交叉**（比如在学习之间安排一次体育锻炼），这样也可以避免久坐成疾，且让每件事更有趣。每天也要定下一个时间点（比如下午5点），在这时间点之后就是你的休息时间（休息之后才能有足够的精力进行第二天的工作）。<br>\n此外，也要在计划本中对完成的和未完成的任务**做好批注**，这样你才能更了解自己的时间应该怎么安排，以避免任务完成不了带来的挫败感。<br>\n\n## 其他（Others）\n\nquote from Dr. Terrence Sejnowski, **充足宽广的空间/锻炼** 可能会让大脑产生新的神经元（海马体上），这也能帮助我们学习记忆！所以，没事就多跑步吧！而且，多 **跟有创造力的人交流**，也可以让你更有创造力！所以，也多跟人交流吧！<br>\n\nquote from Dr. Norman Fortenberry in MIT，学习中的**团队合作**很重要（但注意别让学习小组变成社交小组）！讨论会发现你们彼此理解的谬误。另外要重视**休息**的重要性。<br>\n\n**比喻与类比**对于我们理解概念也很重要，它们能帮助我们从形象的角度上**理解** 抽象的东西（比如将电流比作水流，这样的比喻可以让我们记住很久，因为它可以将要学习的东西与我们脑海中已经存在的神经结构建立联系）。另一方面，比喻和类比也可以帮我们保有 **创造力**，能将我们从思维定势中解救出来。<br>\n\n现代神经科学之父卡哈尔（Cajal），曾经固执而叛逆，小时候曾进过监狱，最后也获得了诺贝尔奖。他觉得自己成功的关键之一正是因为自己不太聪明，**敢于承认错误并修正**，此外还有**坚韧不拔的毅力和自信**。<br>\n\n应对**考试**，相比于从简单的题目开始，一个更好的技巧是：**从难题开始，但2分钟觉得没有思路马上跳过（基本上都是这样），回到简单的题目**，这会让难题在你脑海中留下印象，这会开始让分散模式开始作用于难题（别忘了创造力怎么来的），通常也会让你更快进入状态。如果你觉得太紧张了，一个有效的方式就是把注意力集中在你的呼吸上（**冥想**），然后缓慢腹式深呼吸一段时间。在答案检查上，选用一个与做题**相反的顺序**，可以给大脑新鲜感，这也会让你更容易揪出错误。\n\n\n---\n> Learning How to Learn: Powerful mental tools to help you master tough subjects, by Deep Teaching Solutions, Coursera\n","tags":["科研基本技能"]},{"title":"统计学基础（Fundamentals of Statistics）","url":"/2022/02/21/FundamentalsStatistics/","content":"\n## 什么是统计？（What's Statistics?）\n\n现在是一个数据驱动的时代，所以深刻地理解数据分析核心的统计学显得尤为重要。</br>\n\n先明确一个问题，统计与数据科学的关系是什么？首先，很多数据科学中的机器学习算法本质上是一些统计原则的有效实现（后文会有具体介绍）。其次，从统计的角度上看：数据来自于一个**随机过程**。所以数据科学的目的是要从有限的数据中理解这个过程如何工作，并基于此做出对未来的预测或理解数据背后揭示的机理。换句话说，在数据科学中，我们需要进行统计建模。</br>\n\n简单来说，统计建模就是：复杂模型 = 简单模型 + 随机噪声。也就是说，一个好的统计模型包含两个部分，一个是简单的（可行的）过程模型，另一个是噪声的分布模型。</br>\n\n而为了理解上述统计学中的随机性（randomness），我们又离不开概率（**Probability**）的支持。那么概率与统计的关系/区别是什么？当随机过程是确定的（比如摇骰子时每一面出现的可能性是1/6），那么我们面对的就是一个概率问题；当随机过程中的参数需要被估计（由数据估计随机过程参数），那么我们面对的就是一个统计问题。如下图所示：</br>\n![](1.png)\n学好统计学离不开对概率的深刻理解，下面先复习相关的部分概率论知识。\n\n## 概率论复习（Recap of Probability）\n\n首先我们要明确，随机变量之间相互独立，且满足相同分布（**Independent Identical Distribution, I.I.D.**）是很多统计学内容的基础（如大数定理、中心极限定理等等），这个前提条件之后不再赘述。\n\n统计学中很常见的操作就是求期望。用数据的期望来表征数据分布真实的平均的依据是什么？我们需要用到大数定理（**Laws of large numbers, LLN**）。\n![LLN](2.png)\n但这个定理并不能告诉我们我们的期望离数据真实的平均有多近（ How large is the deviation? ），这时候我们就需要用到中心极限定理（**Central Limit Theorem, CLT**）。\n![CLT](3.png)\n也就是说，红框中的左式极大概率的分布会在[-3, 3]之内（由标准正态分布决定，常数3可以依据对精度的要求自行决定），即：\n$$\n|bar(X_n)-\\mu| <= 3\\sigma/sqrt(n), with high Probability.\n$$\n所以依据中心极限定理，我们就能知道对于特定的样本大小 $n$ （$n$ 要足够大，常见如 $n>=30$），我们得到的期望偏离真正的平均的程度。</br>\n\n而当 $n$ 不够大，CLT无法适用时，我们则需要用到霍夫丁不等式（**Hoeffding's inequality**）（这也是机器学习研究中很重要的理论基础）。使用霍夫丁不等式的前提条件是样本属于某一确定范围，即 $X \\in [a, b]$ 。\n![Hoeffding's inequality](4.png)\n也就是说，在不对样本数做要求情况下，我们可以知道满足某一特定精度要求 $\\varepsilon$ 的（分布于某一确定范围内的）样本出现的概率。\n此外要注意的是，相对于CLT，Hoeffding's inequality会显得更加保守，也没有那么精确。举例来说，相同样本下，CLT估计出的偏差小于某一值的概率（如5%）要小于Hoeffding's inequality估计的结果（如35%）。这也是为什么多数情况下大家不用其来进行统计陈述的原因。\n\n回到中心极限定理CLT，它告诉我们的是当数据量足够大时，各种分布的数据的期望最终都会趋近于在真实平均值附近的一个高斯分布，这就显得高斯分布极其重要。\n![Gaussian Density](5-GD.png)\n从概率角度看，高斯概率密度是一个钟形（bell shape）分布，且其下方与x轴包含的面积必然为1（包含了所有可能性），所以当高斯概率密度分布的方差变大，则其最大值必然变小（如上图中的红线之于蓝线）。而当样本取值趋向于负无穷或正无穷，高斯分布的概率会趋向于0（但不等于0，所以不做近似的话是不能应用Hoeffding's inequality的）。而且高斯概率分布的累积分布函数（Cumulative Distribution Function， CDF）是无法解析表示的，其计算需要依赖计算器计算方法。</br>\n上面是高斯分布的简要描述，它还有什么有用的特性呢？假设$X~\\mathnormal{N}(\\mu, \\sigma^2)$，有：\n- 仿射变换（Affine Transformation）下的不变性。即对于任意实数$a, b$，有：\n$$\na·X+b ~ \\mathnormal{N}(a·\\mu+b,a^2·\\sigma^2)\n$$\n也就是说，对于一个高斯分布进行仿射变换，其结果仍然服从高斯分布。\n- 标准化/归一化/Z-score。即：\n$$\nZ = (X-\\mu)/\\sigma ~ \\mathnormal{N}(0,1)\n$$\n这一性质可方便我们进行查表（一般只有标准高斯分布的数值表）。\n- 对称性。由上面的图可以显然得到。\n\n概率的内容就先复习这么多，具体可以参见另外对概率论进行比较详尽讨论的文章。接下来我们开始进入正题：统计。\n\n## 推测基础（Foundation of Inference）\n\n首先明确统计推测的最终目标是确定观测数据的真实分布，具体而言，包含三个陈述：首先是估计（**Estimation**），其次是确定置信区间（**Confidence Intervals**），最后是假设检验（**Hypothesis Testing**）。\n为了完成上面三个统计推测的陈述，首先我们要进行一次统计建模。那么首先，怎么描述一个统计模型？如下所示：\n![](6-SM.png)\n其中， $E$ 表示观测数据所在的样本空间（也是观测数据背后真实分布所有可能产生的结果）， $\\left(\\mathbb{P}_{\\theta}\\right)_{\\theta \\in \\Theta}$ 是一组基于 $E$ 的概率分布，而 $\\Theta$ 则是统计模型的参数集。为了不过于抽象，下面举一个伯努利分布的实例（0-1分布）如下：\n$$\n\\left(\\{0,1\\},(\\operatorname{Ber}(p))_{p \\in(0.2,0.4)}\\right)\n$$\n在这个实例中，伯努利分布的参数$p$被限定在 $(0.2,0.4)$ 之间。</br>\n\n上面其实只是统计模型的一种类型，那么统计模型又可以怎么分类呢？首先是如上的参数模型（**Parametric Model**），其假设观测数据服从某种真实的分布（参数数量有限，如高斯分布、泊松分布、伯努利分布等等）。而另一种则就是非参数模型（**Nonparametric Model**）了，在这种模型里我们知道数据的分布存在，但不对数据分布进行可有限参数化描述的假设（或者说，数据分布的参数有无限个），下面是一个非参数模型的例子：\n![](7-NP.png)\n\n## 参数估计与置信区间（Parametric Estimation and Confidence Intervals）\n\n在统计学中，基于观测数据计算某个估计值（如平均值、方差或其他形式）的函数，我们称为估计量（**Estimator**）。但随之而来有一个很重要的问题，我们由此得到的估计值与其对应真实值的差异有多大？这就引入了偏差（**Bias**）的概念：\n$$\n\\operatorname{bias}\\left(\\hat{\\theta}_{n}\\right)=\\mathbb{E}\\left[\\hat{\\theta}_{n}\\right]-\\theta\n$$\n其中， $\\mathbb{E}\\left[\\hat{\\theta}_{n}\\right]$ 代表我们的估计值，而 $\\theta$ 代表着真实值。如果偏差为0，那么我们就说我们的估计无偏差（**Unbiased**）。但无偏差估计是否就是最好的？这边还要注意的是，这里的无偏差侧重点在均值上，也就是说，我们的估计值尽管均值与真实值没偏差，但其方差可能很大。\n","tags":["基本数学"]},{"title":"线性代数中的几何 （长文..., TBD)","url":"/2022/02/02/linear-algebra/","content":"\n## 0. 前置知识\n理解 $R^n$ （n维向量空间），增广矩阵（augmented matrix），主元（pivot），自由变量（free\n variable），维度（Dimension），标量（scalar）与向量（vector）的区别，知道向量、矩阵运算规则，大致知道子空间（Subspace）、基（Basis）、标准基（standard basis）、可逆矩阵（invertible matrix）等概念。其实最好就是上过照本宣科的线代课，对基本概念都有一些了解，想要从几何角度来加深对线代的理解，就可以看看这篇博客。<br>\n\n## 1. $R^n$ 中的向量（**Vector**）\n首先从最基本的概念讲起。一个n维向量在几何上有两种解释：一种是它是n维空间里的一个点（point），另一种则是向量（vector），如下图所示。<br>\n![](la-1.png)<br>\n在之后的描述中，除非特别声明，否则默认向量起始于原点。但需要注意的是，这只是为了描述的方便，向量在可以在空间中任意位置，它并不一定要以原点为起点。换句话说，一个向量只由它的长度和方向决定，与其位置无关。<br>\n当然，向量也可以表示两个点之间的距离，如下图所示。<br>\n![](la-2.png)<br>\n\n\n## 2. 向量的运算及其几何表示\n向量的相加减结果可由向量间各个维度数值各自相加减得到，如下图所示。<br>\n![](la-3.png)<br>\n而标量与向量的乘法结果可由标量与向量各个维度数值相乘得到，如下图所示。<br>\n![](la-4.png)<br>\n所以，从几何角度上看，向量间的线性组合，由各个向量各自的缩放（与标量相乘），再将它们相加得到。\n而向量的线性组合又能反映代数上的什么东西呢？假设有这么一个方程（向量方程，即由向量构成的方程，等价于一个线性代数方程）：<br>\n![](la-5.png)<br>\n那么我们所要求的解（ $x$ 与 $y$ ），是不是就是两个向量的缩放因子？换句话说，一个向量方程（线性代数方程）有解即意味着等号右边的向量是等号左边向量们的线性组合，而解就是线性组合中的缩放因子。<br>\n\n\n## 3. 生成空间 （**Span**）\n生成空间是一个很重要的概念。简单来说，对于一组向量，以他们为基能张成（线性组合而成）的空间就称为它们的生成空间，可记为：<br>\n![](la-6.png)<br>\n其中 $x_1$ 到 $x_k$ 为标量（缩放因子），**$v_1$** 到 **$v_k$** 为基向量。<br>\n以在 $R^3$ 中为例，一个向量张成的生成空间为一条一维直线，两个不共线的向量张成的生成空间为一个二维平面，三个不共线且不都在一个平面上的向量张成的生成空间则为一个三维空间，而三个不共线但在一个平面内的向量张成的生成空间则为一个二维平面，如下图所示。<br>\n![](la-7.png)<br>\n结合上面线性组合相关的讨论，我们又可知道由一组基向量以及它们的生成空间中的任意向量组成的向量方程（代表着一个线性代数方程）有解。<br>\n\n## 4. 矩阵方程 （Matrix Equations）\n首先回忆一下，一个矩阵 **$A$** 乘上一个向量 **$x$** ，可以视为矩阵 **$A$** 中各向量与向量 **$x$** 中各元素的线性组合，如下公式所示。<br>\n![](la-8.png)<br>\n其中 $x_1$ 到 $x_k$ 为标量（缩放因子），**$v_1$** 到 **$v_k$** 为向量。所以显然，向量 **$x$** 中元素的数目应与矩阵 **$A$** 中向量的数目相等，即对于 **$Ax = b$** 而言，矩阵 **$A$** 的列数应与 **$x$** 维度一致。而这个 **$Ax = b$** 就被定义为**矩阵方程** （**Matrix Equation**)，其中向量 **$x$** 中各元素大小未知（想想与向量方程的关系？是不是两者是等价且可以相互转换的？）。这也是线性系统的另一种表达方式。<br>\n总结一下，目前为止，结合代数中的内容，我们就有四种方式来思考一个线性系统：\n- 线性代数方程组（代数角度）；\n- 增广矩阵（代数角度）；\n- 向量方程（几何角度）；\n- 矩阵方程（几何角度）。\n\n上面我们解释了矩阵方程与向量线性组合的关系，再回头看我们之前讨论的线性组合与生成空间的关系，是不是发现又可以串起来了？也就是说，当且仅当向量 **$b$** 在矩阵 **$A$** 中各向量张成的生成空间里，\n **$Ax = b$** 有解。其实这就搭起了代数中方程有解与几何中生成空间的桥梁。下面给一个直观的例子。<br>\n假如我们要求 **$Ax = b$** 是否有解？对于：\n $$\n \\mathbf{A}  = \\begin{pmatrix}\n 2 & 1\\\\\n -1 & 0\\\\\n 1 & -1\n\\end{pmatrix},\n\\mathbf{B} = \\begin{pmatrix}\n 0\\\\\n 2\\\\\n2\n\\end{pmatrix}\n$$\n由上面我们的讨论可知，当且仅当向量 **$b$** 在矩阵 **$A$** 中各向量张成的生成空间里，\n **$Ax = b$** 有解。所以我们将 **$A$** 中各向量张成的生成空间（紫色）与  **$b$** （黑色箭头）画出，可见 **$b$** 并不在 **$A$** 中各向量张成的生成空间中，所以方程无解。<br>\n接下来我们考虑 **$Ax = b$** 有解的情况，怎么从几何上去表示其解呢？在回答这个问题之前，我们先考虑一个简单一点的例子：如果 **$b$** 为零向量时解是什么呢？考虑下面的情况：<br>\n假设我们要求 **$Ax = 0$** 的解（这类方程至少有一个平凡解(trivial solution) **$x=0$** ），对于：\n$$\n \\mathbf{A}  = \\begin{pmatrix}\n 1 & -1 & 2\\\\\n -2 & 2 & -4\n\\end{pmatrix}\n$$\n我们用参数化的形式表示它的解，如下所示：<br>\n![](la-9.png)<br>\n显然我们可以将其转换为向量方程的形式：<br>\n![](la-10.png)<br>\n所以显然，方程的解为上述两个向量张成的生成空间：<br>\n![](la-11.png)<br>\n将其可视化如下图所示：\n![](la-12.png)<br>\n所以从几何角度上看，方程 **$Ax = 0$** 的解可以表示为某个/组向量张成的生成空间，注意区分好与上文中对于 **$Ax = b$** 讨论的区别（“解是一个生成空间（解是什么）”与“解在生成空间中（有解的条件）”的区别）。顺带回顾一下变量数目与空间维度的关系：在这个例子中，一共有三个变量（构成 **$x$** ），那么方程的解集必在 $R^3$ 或其子空间，而这个例子中只有两个自由变量（free variable），则解集是在一个二维平面上（自由变量的数目等于解集空间的维度）。<br>\n\n接着回到我们之前的问题：假设 **$Ax = b$** 有解，那么它的解如何在几何上表示？它与 **$b$** 为零向量时有什么不同呢？考虑下面情况：<br>\n$$\n \\mathbf{A}  = \\begin{pmatrix}\n 1 & -1 & 2\\\\\n -2 & 2 & -4\n\\end{pmatrix}，\n\\mathbf{B} = \\begin{pmatrix}\n 1\\\\\n -2\n\\end{pmatrix}\n$$\n类似地，我们可以得到其解为：<br>\n![](la-13.png)<br>\n对比这个结果与上面 **$b$** 为零向量时的结果，显然可以看出，两者相差的只是一个平移( $x_2$ 与 $x_3$ 为0时的特解) **$p$** ，如下图所示：<br>\n![](la-14.png)<br>\n当然，这里举的例子都是方程不是有唯一解的情况，而如果方程只有唯一解，其解在空间上表示就只是一个点了（此处 **$Ax = b$** 与 **$Ax = 0$** 相差的依然只是一个平移）。<br>\n至此，我们在几何角度上又从两个方面上去描述一个矩阵方程（ **$Ax = b$** ），注意两者的区别：\n- **$Ax = b$** 有解的条件（**$b$** 在矩阵 **$A$** 中列向量张成的生成空间中，为 **$x$** 选 **$b$**）；\n- 解集空间（对于确定的 **$b$**，为 **$b$** 选 **$x$**）。\n\n\n## 5. 线性独立（Linear Independence）\n首先回忆一下线性独立的定义：如果一组向量 **$v_1$** 到 **$v_k$** ，对于下面的方程有且仅有一个平凡解（$x_1=x_2=...=x_k=0$），则称它们彼此 **线性独立**。<br>\n![](la-15.png)<br>\n换句话说，就是这组向量中，没有任何一个向量可以由其他向量线性组合得到。下图给出两个 **线性依赖（不独立）** 的例子：<br>\n![](la-16.png)<br>\n也就是说，从几何角度上看，如果一个向量集合中，有一个向量在其他向量张成的生成空间中，那么这组向量就线性依赖（不独立）；而如果其中任何一个向量都不在其他向量张成的生成空间中，那么这组向量就线性独立。再换个角度想，我们将这些向量一个一个加入向量组中，每加入一个向量组张成的生成空间都增大，那么这组向量就是线性独立的。下面再可视化给出一个线性独立的例子：<br>\n![](la-17.png)<br>\n而从代数角度上看，矩阵 **$A$** 中各列向量线性独立的条件是化简后每一列都有一个主元（也就是说对于一个列数目大于行数目的矩阵，它必然有线性依赖的列，因为没法做到化简后每一列都有一个主元）。<br>\n\n## 6. 子空间、列空间、零空间与基（Subspace, Column Space, Null Space and Basis）\n什么是**子空间**（**Subspace**）？简单地说，子空间就是满足下面三个条件的 **$R^n$** 里的一些点构成的子集（**Subset**）：\n- 加法下的封闭性：该子空间中的两个两个向量相加，结果仍在该子空间中；\n- 标量乘法的封闭性：该子空间中的向量乘上一个标量，其结果仍在该子空间中；\n- 非空性：零向量在这个子空间中（子空间要存在，势必要包含了零向量）。\n\n\n所以显然，由上面的前两个性质，某一子空间中的向量的线性组合（它们的生成空间）结果仍在该子空间中；且由第三个性质，子空间必然会经过原点。换句话说，子空间本身就是一个生成空间，它包含了它之中任何向量张成的生成空间。进一步讲，其本身就是其最大的一个子空间。也就说，随着选中的某一子空间中的向量数目变多，这些向量张成的生成空间也会逐步填满这个子空间。下图给出两个子空间的例子（一维与二维）：<br>\n![](la-18.png)<br>\n其中黑色箭头表示用于张成生成空间的向量（基向量，下面会进一步讨论），黑点表示原点。下面再给出一些**非**子空间的例子：<br>\n![](la-19.png)<br>\n其中紫色区域表示定义的“空间”（注意并不是子空间），黑色箭头表示“空间”上的向量，红色箭头表示由“空间”中向量线性组合得到的不在“空间”中的向量。<br>\n另外注意区别空间子集（subset）与子空间（subspace）的区别：子空间是一个需要满足上述三个条件的空间子集，也就是空间子集是一个更大的概念。<br>\n接下来我们讨论两种重要的子空间：**列空间**（由矩阵 **$A$** 中各列向量张成的子空间/生成空间，记作Col( **$A$** )）与**零空间**（对矩阵 **$A$**，满足 **$Ax=0$** 的解构成的子空间/生成空间，记作Null( **$A$** )）。下面如下矩阵 **$A$** 举例：<br>\n$$\n \\mathbf{A}  = \\begin{pmatrix}\n 1 & 1 \\\\\n 1 & 1 \\\\\n 1 & 1\n\\end{pmatrix}\n$$\n它的列空间可以表示为：<br>\n![](la-20.png)<br>\n它的零空间可以表示为：<br>\n![](la-21.png)<br>\n留意列空间是一条在三维（维度大小与矩阵的行数目相同）空间中的一维（等于矩阵化简后主元的数目）线，零空间是一条在二维（维度大小与矩阵的列数目相同）空间中的一维（等于矩阵化简后自由变量的数目）线。这边又隐含了一个重要的定理：矩阵的列空间的维度与零空间的维度之和为矩阵列的数目，在这个例子中为 $1+1=2$ 。<br>\n上面讲完子空间/生成空间，接下来我们讨论子空间/生成空间中的基。为什么需要基的概念，我想最朴素的原因就是数学家们想用最少数量的向量来表征一个生成空间，而基于这个想法，基向量之间就必须相互独立，不然就会有冗余的向量（可以由其他向量线性组合得到）。而对于同一个非零生成空间，它可以有无数组基（比如一个平面，其上任意两个不共线的向量构成它的一组基），但基向量的数目是确定的（由生成空间的维度决定，这其实也是维度的定义）。下图给出了 $R^2$ 的两组基：<br>\n![](la-22.png)<br>\n其中左图是一组标准基。<br>\n那么如何确定列空间的基呢？对于某个矩阵，它的主元（pivot）所在列就构成了它的一组基，如下举例所示，其中RREF（Reduced row echelon form）表示简化列阶梯形矩阵：<br>\n![](la-23.png)<br>\n也就是说，列空间的基向量的数目（维度）等于矩阵主元的数目。<br>\n而如何确定零空间的基呢？将零空间以参数化向量方程表示出来，也可以很轻易得到它的一组基：<br>\n![](la-24.png)<br>\n类似地，零空间的基向量的数目（维度）等于矩阵自由变量的数目。也就是说，我们从基的角度出发，再次印证了上面给出的一个定理：矩阵的列空间的维度与零空间的维度之和为矩阵列的数目（主元与自由变量数目之和）。下面给出几个图进一步说明这个关系：<br>\n![](la-26.png)<br>\n![](la-27.png)<br>\n而其实矩阵的 **秩（rank）** 代表的就是其列空间的维度，零化度（nullity）代表的就是其零空间的维度。也就是说，一个矩阵的秩加上其零化度，等于其列向量的个数（$rank(\\mathbf{A})+nullity(\\mathbf{A})=n$，$n$ 为矩阵 $\\mathbf{A}$ 的列数目）。再结合之前的讨论，换一个说法就是，一个矩阵的主元的个数加上其解集的维度（自由变量的个数），等于其拥有的变量的总个数。其实这个关系也反映了我们在选择 **$Ax = b$** 中的 **$x$** 与 **$b$** 的平衡：当我们拥有的选择 **$x$** 的自由越多，那么我们拥有的选择  **$b$** 的自由就越少，而这个关系被矩阵 **$A$** 中列数目所限定。<br>\n而很自然地，当基确定之后，我们就可以基于基构建一个新的坐标系。在笛卡尔坐标系下，$u_1 ~ u_4$ 的坐标分别为：\n$$\nu_1 = [3, -1, 0], u_2 = [-3/2, 1, -3/2], u_3 = [5/2, -3/2, 2], u_4 = [3/2, 0, -3/2]\n$$\n现在考虑用 **$v_1, v_2$** 来表示它们，如下图所示：<br>\n![](la-25.png)<br>\n我们就可以得到以 **$v_1, v_2$** 为基的新坐标系下$u_1 ~ u_4$ 的坐标：<br>\n$$\nu_{1\\beta} = [1, 1]， u_{2\\beta} = [-1, 1/2], u_{3\\beta} = [3/2, -1/2], u_{4\\beta} = [0, 3/2]\n$$\n由此，相同的几个点在另一个空间中被表示出来，这就完成了一次空间变换。当然，能这么表示的前提是 $u_1 ~ u_4$ 几个点刚好在 **$v_1, V_2$** 的生成空间中。\n\n\n## 7. 矩阵变换（Matrix Transformations）\n对于一个$m$行$n$列的矩阵 **$A$** ，其有 **$b=Ax$** 的关系，我们可以将其视为一个变换（Transformation），将自变量 **$x$** (维度为$n$的向量)，变换为因变量 **$b$** (维度为$m$的向量)。顺便再提一下， **$b$** 在矩阵 **$A$** 的列空间中。下图给出一个例子，其中绿色箭头为 **$x$** ，红色箭头为 **$b$** ，中间的矩阵即为 **$A$** ，紫色直线为矩阵 **$A$** 的列空间，也就是说矩阵 **$A$** 将一个三维空间（定义域，domain）中的向量 **$x$** 与其列向量进行线性组合，将其变换到一个二维平面（取值空间，codomain）上，但在这个二维平面上 **$A$** 的列空间只是一条一维直线（值域，range，其实也就是矩阵 **$A$** 的列空间），上面代表着 **$b$** 能分布的空间。<br>\n![](la-28.png)<br>\n这边再给出几个矩阵背后代表的几何意义的例子，最好自己先思考再看之后的答案以加深理解：<br>\n如果要将三维空间的点( **$x$** )投射（projection）到x-y平面上( **$b$** )，那矩阵 **$A$** 应该是什么样子的呢？<br>\n![](la-29.png)<br>\n那如果要使一个二维平面上的点关于y轴对称呢（Reflection）？<br>\n![](la-30.png)<br>\n而如果要使一个二维平面上的点保持不变呢（Identity）？<br>\n![](la-31.png)<br>\n这也是单位矩阵（Identity Matrix）名字的由来。那如果要将向量进行缩放（Dilation），是不是只需要对单位矩阵乘上一个标量？<br>\n接着试着回答一个稍微难一点的问题：如果要将二维平面上的点顺时针旋转90度呢（Rotation）？<br>\n![](la-32.png)<br>\n留意到这里矩阵 **$A$** 的列向量（[0,1], [-1,0]）是不是刚好是二维笛卡尔坐标系下基向量（[1,0],[0,1]）顺时针旋转90度的结果呢？<br>\n而如果要实现下图所示的在x轴上的错切（shear in the x-direction），矩阵 ( **$A$** )应该是什么样的？<br>\n![](la-33.png)<br>\n可以尝试下面这三个矩阵代表的几何变换分别是什么。<br>\n$$\n \\mathbf{A_1}  = \\begin{pmatrix}\n 1 & 1 \\\\\n 0 & 1\n\\end{pmatrix},\n \\mathbf{A_2}  = \\begin{pmatrix}\n 1 & 0 \\\\\n 1 & 1\n\\end{pmatrix},\n \\mathbf{A_3}  = \\begin{pmatrix}\n 1 & 1 \\\\\n 1 & 1\n\\end{pmatrix}\n$$\n（答案是第一个矩阵代表着上面的变换。）<br>\n对于某个矩阵变换，我们还需要留意，其对应的是一个一对一（one-to-one）的变换（换个角度就是，矩阵方程有唯一解或无解/ **$Ax=0$** 只有一个平凡解/ 矩阵 **$A$** 的列向量都线性独立/ 矩阵 **$A$** 的每一列都有主元/ 矩阵 **$A$** 的列空间（值域）与定义域维度一致），还是一个多对一的变换，注意没有一对多的变换。下面给出几个图解例子：<br>\n首先是一个一对一的矩阵变换例子：<br>\n![](la-34.png)<br>\n接着看一个多对一的矩阵变换的例子：<br>\n![](la-35.png)<br>\n这个图中的矩阵 **$A$** 代表着将三维空间中的点投影到x-y平面上，也就是说，在每一条跟z轴平行的线上的点（ **$x$** ）都对应着同样的 **$b$** 。<br>\n接下来给出一个稍微复杂一点的例子：<br>\n ![](la-36.png)<br>\n怎么理解这个结果？首先这是一个$R^3$到$R^2$的投射，维度的损失带来的就是多对一的投射（在这个例子中，一条直线（由矩阵的零空间维度决定）投射到一个点）；其次，我们之前讨论过， **$Ax=b$** 的解可以视为 **$Ax=0$** 解（零空间）的平移，在这个例子中， **$Ax=0$** 的解为一条直线（ $x=z$ 与 $y=-z$ 两个平面的交界线，图中紫色虚线），而 **$Ax=b$** 的解即为其平移后的直线（紫色实线），绿色向量即为其平移的大小。<br>\n其实也可以看出，如果一个矩阵的列数n多于行数m，这就意味着把一个高维空间的向量投影到一个低维空间（代数角度可以是矩阵没办法每一列都拥有主元，即列向量们线性依赖），这种情况下矩阵的零空间必然不只是零向量，进而 **$Ax=b$** 的解也不只是一个向量，所以此时矩阵就无法实现一对一的投射。<br>\n顺带一提，像最后这个例子，矩阵 **$A$** 的列空间（值域）与其取值空间一致（从代数角度其实就是，每一行都有主元，不然有的维度就被丢掉了），这种变换又叫onto变换（不知道怎么准确翻译...）。类似地，如果一个矩阵的列数n少于行数m，那么其取值空间维度（每一列向量含有的变量数目，即行数m）必然大于列空间（n个向量至多只能张成一个n维空间，$n<m$）维度。<br>\n对于一般的矩阵，上述两种变换，可以同时满足，也可以同时不满足，也可以只满足其中一个。但对于方阵而言，上述两种变换，必然同时满足，或同时不满足。为什么？对方阵而言，是否每一列都有主元（一对一变换成立）就意味着每一行都有主元（onto变换成立）？反之亦然。<br>\n接下来讨论一个问题：矩阵变换与线性变换（Linear Transformations）有什么关系？首先回忆下线性变换的定义，如果一个变换下面两个性质，则称其为线性变换：<br>\n$$\nT(x+y)=T(x)+T(y),\nT(cx)=cT(x)\n$$\n显然，每一个矩阵变换都能满足上述性质，也就是说每一个矩阵变换背后都代表着线性变换；而反之，每一个线性变换也都可以用矩阵变换来表示。换句话说，矩阵变换与线性变换是完全等同的。（顺带一提，留意像 $y=x+1$ 这种两个变量之间虽然是线性关系，但其变换并不是线性变换，不要混淆线性关系与线性变换的概念。其实正是这个表达式中的常数项破坏了其线性变换。）<br>\n也就是说，一个矩阵可以用下面的形式表示，其中$T$代表着某种线性变换，**$e_1, e_2, ..., e_n$** 代表着$R^n$的标准基向量。<br>\n![](la-37.png)<br>\n怎么理解呢？矩阵 **$A$** 的每一列代表着对 $R^n$ 空间中每一个维度进行的线性变换。有点抽象，举个例子好了：<br>\n假如现在要构造一个矩阵，其代表着将$R^3$中的点关于xy平面进行对称映射，再将其投影到yz平面上，那么这个矩阵应该是什么？<br>\n首先确定矩阵的第一列，也就是对$R^3$中的 **$e_1$** 进行操作，其在经过上述操作后，会落到零点，所以：<br>\n![](la-38.png)<br>\n接着确定矩阵的第二列，也就是对$R^3$中的 **$e_2$** 进行操作，其在经过上述操作后，会保持不变，所以：<br>\n![](la-39.png)<br>\n接着确定矩阵最后一列（变换后的空间仍是$R^3$），也就是对$R^3$中的 **$e_3$** 进行操作，其在经过上述操作后，其方向会取反，所以：<br>\n![](la-40.png)<br>\n所以最后确定下来矩阵 **$A$** 应该为：<br>\n![](la-41.png)<br>\n\n## 8. 矩阵乘法（Matrix Multiplication）\n首先强调一下矩阵乘法（ **$AB$** ）中， 矩阵 **$B$** 的行数必须与矩阵 **$A$** 的列数一致。怎么理解呢？矩阵乘法可以理解为前一个矩阵 **$A$** 对后一个矩阵 **$B$** 的各列向量进行空间变换，如下所示：<br>\n![](la-42.png)<br>\n假设 **$A$** 的维度为 $m$ x $n$，那么其代表着一个从 $^n$ 到 $R^m$ 的空间变换，那么必然 **$B$** 中列向量的维度也必须为 $n$，也就是矩阵 **$B$** 的行数必须为 $n$。<br>\n同样重要的是，一个 $m$ x $n$ 的矩阵 **$A$** 乘上一个  $n$ x $p$ 的矩阵 **$B$**，其结果维度为 $m$ x $p$。这个又怎么理解呢？同样还是矩阵乘法可以理解为前一个矩阵 **$A$** 对后一个矩阵 **$B$** 的各列向量进行空间变换，而前者代表着一个从 $^n$ 到 $R^m$ 的空间变换，也就是变换后的列向量维度为 $m$，即 **$AB$** 结果的行数为 $m$；而原来矩阵 **$B$** 有 $p$ 个列向量，对他们分别进行矩阵变换并不会改变他们的数量，也就是说， **$AB$** 结果的列数为 $p$。<br>\n类似地，基于矩阵乘法可以理解为前一个矩阵对后一个矩阵的各列向量进行空间变换，我们也可以理解矩阵乘法不满足交换律（对不同的向量进行不同的变换，他们相等的条件是不是很苛刻？），仅在少数情况下 **$AB$ = $BA$** （但当方阵 **$A$** 与 **$B$** 有 **$AB=I$** 时，直接会有 **$BA=I=AB$**）。<br>\n而因为矩阵可以代表着多对一的变换，由 **$AB=AC$** 并不能得到 **$B=C$**，比如下面的例子：<br>\n![](la-43.png)<br>\n上面是理解矩阵乘法的一个角度，也就是把第二个矩阵当成运算对象。而从另一个角度出发，之前我们提过，一个矩阵代表着一种线性变换，那么显然，两个矩阵相乘其实代表着做完一种线性变换之后接着再做另一种线性变换（链式线性变换）。<br>\n\n## 9. 矩阵的逆（Matrix Inverses）\n首先明确，我们只针对**方阵**去讨论可逆的性质。怎么判断矩阵是否可逆？一种方式是可以通过其行列式（determinant）是否为0来判断，为0时矩阵不可逆（后面再进一步讨论矩阵行列式）。另一种方式是将增广矩阵 **$(A|I_n)$** 变换为 **$(I_n|B)$**，那么矩阵 **$B=A^{-1}$**，即 **$B$** 为 **$A$** 的逆（这里隐含的条件是方阵 **$A$** 必须满秩）。 而矩阵的逆是用来求解线性方程的一种很方便的方式（**$x=A^{-1}b$**）。<br>\n以上都是一些代数角度的讨论，接下来我们进行一些更几何化的补充。前面我们说了，一个矩阵代表着一种线性变换，那么逆矩阵的作用就是“撤销”这个线性变换。这其实也很好理解，**$A^{-1}Ax=I_nx=x$**，首先 **$A$** 对 **$x$** 进行了一个线性变换，然后 **$A^{-1}$** 又撤销了这个变换（做了一个反向变换），结果就是 **$x$** 经过这两个线性变换之后仍是其自身。下面给几个例子：<br>\n比如如果一个矩阵 **$U$** 代表着缩小向量长度为$1/n$，那么其逆矩阵 **$T$** 就代表着放大向量长度至$n$倍：<br>\n![](la-44.png)<br>\n再比如一个矩阵 **$T$** 代表着逆时针旋转45°，那么其逆矩阵 **$U$** 就代表着顺时针旋转45°：<br>\n![](la-45.png)<br>\n那又如何理解矩阵不可逆呢？考虑下面这个例子：矩阵 **$A$** 代表着将三维空间中的点投影到xy平面，即: <br>\n![](la-35.png)<br>\n显然这里的矩阵 **$A$** 并不满秩，也就是说它代表着一个多对一的变换，即在它对矩阵进行线性变换之后，我们就丢掉了在变换之前的信息（我们只能知道哪些点会投影到变换后的点上，但确定不了是哪个点），而上面我们说了，矩阵的逆代表着反向变换，但这个时候矩阵已经不知道应该反到哪个点上了，所以这个矩阵就不可逆了。也就是说，一个方阵只有代表着一对一变换（因为是方阵也代表着onto变换），其才可逆。<br>\n\n结合之前的内容回顾总结一下，一个 $n$ x $n$ 方阵 **$A$** 可逆，其与下列表述等价（它们之间也等价）：\n-  **$A$** 拥有 $n$ 个主元；\n-  **$A$** 的简化列阶梯形矩阵（RREF）是单位矩阵 **$I_n$** ；\n-  **$A$** 的零空间只有{0}；\n-  **$A$** 的列空间是$R^n$；\n-  **$A$** 的列向量线性独立；\n-  **$A$** 的列向量可以张成$R^n$；\n-  **$A$** 的列向量构成了$R^n$的一组基；\n-  **$A$** 的秩是$n$（满秩）；\n-  **$Ax=b$** 对于每个$b$都只有唯一解（且在$R^n$中）；\n-  **$A$** 代表的变换为一对一变换；\n-  **$A$** 代表的变换为onto变换。\n\n也就是说，对一个可逆矩阵，上列表述都成立；而对于一个不可逆矩阵，上列表述都不成立。<br>\n\n顺带一提，要证明矩阵 **$A$** 可逆，只需要证明 **$AB=I_n$** 或 **$BA=I_n$** 中的一个就足够。这里给出一个不完全的证明帮助理解：<br>\n**$AB=I_n$** →  **$A^{-1}AB=A^{-1}I_n$** → **$B=A^{-1}$** → **$BA = A^{-1}A$** → **$BA=I_n$**\n\n## 10. 行列式\n\n\n\n---\n> 参考书籍：Interactive Linear Algebra - by Dan Margalit, Joseph Rabinoff (https://textbooks.math.gatech.edu/ila/index2.html)\n","tags":["基本数学"]},{"title":"《网络科学》 第二章 图论 - Albert-Laszlo Barabasi （TBD）","url":"/2021/11/24/network-science-2/","content":"\n## 1. 七桥问题(The Bridges of Konigsberg)\n\n在真正开始讲解图论的内容之前，我们可以先回答一个问题：图论起源于什么？这可以追溯到1975年的Konigsberg这座当时的大都市。当时的政府想要在几片不相连的土地（下图中的A、B、C和D）上建造7座桥，以方便居民们的生活，如下图所示。<br>\n![](figure-2-1.jpg)<br>\n而当时爱思考的人就提出了这么一个问题：一个人有没有可能经过所有七座桥，但过程中不经过任何一座桥两次呢？针对这个问题，尽管有许多人做了尝试，但都以失败告终。最后是欧拉（对，就是那个大名鼎鼎的数学家）从数学上证明了这样的路径根本不存在，而这也意味着图论的诞生。<br>\n为什么这么说呢？欧拉是这么解构这个问题的。<br>\n\n\n\n上面的问题构成了一个经典的图论问题，用现在图论的语言来说，当时欧拉给出的结论是：<br>\n- 如果一个图中有大于两个结点的度是奇数，那么这样的路径不存在；\n- 如果一个图中没有一个结点的度为奇数，那么至少有一条这样的路径。<br>\n","tags":["网络科学"]},{"title":"《网络科学》 序言 & 第一章 简介 - Albert-Laszlo Barabasi","url":"/2021/11/23/network-science-1/","content":"\n## 序言\n\n在正式介绍书的内容之前，作者首先对如何使用这本书提出了自己的教学建议，也给了一些线上的资源以供参考。\n\n### 教学建议：\n书里首先给出了一些《网络科学》教学的建议，建议包括内容为作业（Homework Problems）、Wiki任务（Wiki Assignment）、社交网络分析（Social Network Analysis）、终期研究项目（Final Research Project）、软件使用（Software）、电影夜（Movie Night）以及客座讲授（Guest Speakers）等。\n课程可分为14周，具体安排如下表所示：<br>\n![](syllabus.jpg)<br>\n\n### 资源：\n[在线电子书及配套资源](http://barabasi.com/NetworkScienceBook \"快点我学习\") ：网站上的资源包括：在线电子书、PPT Slides、数据集等。<br>\n\n   推荐专业书籍：<br>\n![](books1.png)<br>\n![](books2.png)<br>\n\n   推荐科普书籍：<br>\n![](books3.png)<br>\n\n\n## 第一章 简介\n### 1. 相互连接导致的脆弱性 （Vulnerability Due to Interconnectivity）\n我们生活中何处会有网络科学的身影？让我们首先从2003年北美大规模断电(blackout)的故事讲起。<br>\n\n![](2003blackout.png)<br>\n\n这是当时大断电前（左）后（右）的卫星图，仔细看的话，会发现，大断电后，Toronto, Detroit, Cleveland, Columbus, Long Island几个城市/地区在卫星图上看不到了，换句话说，这些城市断电了，这也导致了当时北美几百万人回归无电时代。但这件事是怎么发生的？以后又要怎么避免类似的事情发生呢？<br>\n\n在电力**网络**系统中，一个局部的电力过载（失败）会转移到其他**结点**上，如果这个系统过载结点外的其他部分能够“消化”掉这个过载，那么这个过载就是无所谓的，这个系统就是鲁棒（Robust）的；而如果这个过载（失败）太强，当前过载结点的相邻结点也无法承载，那么相邻结点也会过载，进而造成连锁反应，使得整片的电力网络瘫痪，而这就是上述事件发生的原因。更一般地说，相互连接引起了足够大的非局部性（Non-locality），它允许了如电力等能够在其所在网络上进行传播，不管距离有多远。一句话概之：这反映了网络中**相互连接导致的脆弱性**。<br>\n\n而如何避免这种情况呢？这与**网络结构**有关。而什么样的网络结构会更鲁棒、稳健？怎么定量描述网络结构与其动力过程，及其对失败（failure）的影响？又如何预测失败的发生？此处先不具体回答，具体在之后的章节中我们都会得到答案。<br>\n\n上文所述的连锁反应的失败（Cascading failure）其实在大多复杂系统中都可能存在，如互联网中的网络过载、金融网络连锁崩溃（金融危机）、社交网络中的谣言传播等等，这也可见网络科学其实广泛存在于我们的生活中。<br>\n\n那么，这就引出一个问题了，什么是复杂系统呢？<br>\n\n### 2. 网络是复杂系统的核心（Networks at the Heart of Complex Systems）\n霍金说过：“下个世纪会是复杂性科学的世纪。”<br>\n\n![](Hawking.png)<br>\n\n如上文所述，在我们生活中，随处可见都是非常复杂的系统，如由几百亿个体合作构成的社会、连接着几百亿通讯工具的通讯系统、每个人由几百亿神经元协同运作的大脑、细胞中几千种基因与代谢物质的交互网络等等，这都是复杂系统的例子。简单来说，在复杂系统中，只知道系统各组分的性质，几乎不可能去推测系统的集体行为，总体并不只是个体的堆砌，具体定义可见[维基百科](https://zh.wikipedia.org/wiki/%E5%A4%8D%E6%9D%82%E7%B3%BB%E7%BB%9F \"复杂系统-维基百科\")。由于复杂系统在我们生活中如此常见，理解它们显然变得十分重要，而这也会是21世纪人类将面临的巨大科学难题。<br>\n\n**而在每个复杂系统背后，总会有一个决定了其中各组分如何交互的网络。** 因此，如果我们没有对网络有一个深刻的理解，就更不可能去理解复杂系统的行为了。<br>\n\n![](humanGenes.png)<br>\n\n那当前去发展网络科学时机是否已经成熟？<br>\n\n### 3. 为什么当前网络科学得以发展？（Two Forces Helped the Emergence of Network Science & Societal and Scientific Impacts）\n\n图论早由欧拉发展于1735年，那为什么当时没有进一步将图论发展为网络科学呢？<br>\n\n- 当前网络图谱的涌现（The emergence of network maps）。以建立一个社交网络为例，我需要知道你的朋友，你的朋友的朋友，你的朋友的朋友的朋友，以此类推。这在过去需要花费非常多的精力，是一个几乎不可能的任务。而在现在，有了网络社交的加持，用用爬虫技术得到社交帐户的好友列表，这件事竟也变得易如反掌。同理，有了现代技术的加持，我们还能得到线虫（C. Elegans)全神经元连接网络，引文网络，蛋白质代谢网络等等。而这些在两个世纪前并不可能做到。<br>\n\n但有了建立网络的条件，我们就一定要研究网络科学吗？这就不得不提它下面这个特点。<br>\n\n- 网络的普适性（The unibersality of network characteristics）。网络科学中一个关键的发现是：在不同的学科领域中建立起来的网络，竟然有令人惊讶的相似的架构，遵循着同样的组织原则。换句话说，如果我们有了一套完整的网络科学的数学工具，我们就可以用它来探索不同的系统。<br>\n\n除此之外，它又有什么重要的现实意义吗？\n- 社会影响（Societal impact）。如在网络上建模的技术/商业模型（互联网、搜索引擎、社交网络等）带来的巨大经济效益；建立传染病网络模型对其传播进行预测对人群健康的影响；乃至于运用网络分析找到本拉登居所这种在打击恐怖主义上的应用等等。\n- 科学影响（Scientific impact）。如建立人类大脑连接组、药物设计(要考虑细胞内的连接图)等都离不开网络科学的应用。从2000年到现在，几篇网络科学的经典文章被引用也越来越多，有关网络科学的文章发表数量也越来越多。\n\n> **几篇经典文章：**<br>\n> - 1998: Watts-Strogatz paper in the most cited Nature  publication from 1998; highlighted by ISI as one of the ten most cited papers in physics in the decade after its publication.<br>\n> - 1999: Barabasi and Albert paper  is the most cited  Science paper in 1999;highlighted by ISI as one of the ten most cited papers in physics in the decade after its publication. <br>\n> - 2001: Pastor -Satorras and Vespignani is one of the two most cited papers among the papers published in 2001 by Physical Review Letters.<br>\n> - 2002: Girvan-Newman is the most cited paper in 2002  Proceedings of the National Academy of Sciences.<br>\n\n吹了这么多，也还是可以看出网络科学确实有发展的必要，也是比较有前景的。最后从一个比较大的角度总结一下网络科学的特点，然后就正式开始网络科学的学习。\n\n\n### 4. 网络科学的特点（The characteristics of Network Science）\n- 学科交叉性（Interdisciplinary）。正如上文所述，在不同的学科中都能看到网络科学的影子，网络科学也能在很多领域发挥巨大的作用。\n- 经验性、数据驱动性（Empirical, data-driven）。这一点是网络科学与图论最大的不同，网络科学不只是为了发展抽象的理论数学工具，而是为了洞见真实数据背后的价值（Insight）。\n- 定量与数学性（Quantitative and Mathematical）。网络科学背后最重要的图论（数学）中图的形式与统计物理中对随机性处理的概念框架。当然，在其中我们也可以看到一些控制论、信息论的影子。\n- 计算性（Computational）。一般而言，网络科学家不可避免会遇到“大数据”的问题，因此相关研究也对计算机算力提出了比较高的要求。\n\n\n\n\n\n\n---\n> 参考书籍：Network Science - by Albert-Laszlo Barabasi (http://networksciencebook.com/)\n","tags":["网络科学"]},{"title":"Learning what we know and knowing what we learn Gaussian Process priors for neural data analysis - Kris Jensen & Guillaume Hennequin (TBD)","url":"/2021/11/09/Learning-what-we-know-and-knowing-what-we-learn/","content":"\nTopic: Probabilistic latent variable models\n\n## 1. Latent variable models in neuroscience\n### 1.1 Principle component analysis (PCA)\nsensitive to trial-to-trial noise, so it needs a large amount of trials.\nLinear Assumption between the latent variables and the observations of nerual activity.\n### 1.2 Gassuain process factor analysis - Developed by Byron Yu, et al.\ncommonly used to analyze single trial data. It allows information sharing across time, which can help denoise single trial latent trajectories.\nLinear Assumption between the latent variables and the observations of nerual activity.\n### 1.3 Gausian process latent variable models (LVMs)\nAllow to fit non-linear observations data.\n### 1.4 PfLDS\nRely on the rise of deep learning.\nComine a linear dynamical system with like Kalman Filter, with a readout that is basically a deep network, which can learn by fitting it to a large quantity of neural data.\nlinear dynamical system with a nonlinear readout.\n### 1.5 LFADS\nnonlinear dynamical system with a linear readout.\n\n前三个 non-parametric methods, 后两个dynamical system (parametric) methods\n\nbrige this gap: GPFADS (Gaussian Process Factor Analysis with Dynmaical Systems)\n\n## 2. Where do we begin? Bayesian Inference!\n\njpg.\n\nPrimer on Gaussian Processes\n\nGP1.png\nClearly the answer depends on prior beliefs.\n比如如果知道noise很大，那么可能就是一条横线。\n\nGP4\n用covariance来表示smoothness。\n","tags":["神经数据分析","计算神经科学"]},{"title":"最优控制（Optimal Control) - Maurice Smith (Harvard University) NMA (TBD)","url":"/2021/10/26/Optimal-control/","content":"\n生物要有大脑就是因为需要运动！但大脑怎么计划、协调、执行运动命令呢？最优控制（Optimal Control）是一种可能的解释。但在开始深入讨论之前，我们需要先定下一个数学框架以便讨论。\n\n## 1. 最优控制的数学框架\n\n假设有一个动力系统，它的 **状态** 为 $(s)$ ，这里的状态指的是：用于描述、决定系统行为的变量的集合。\n\n如果这个动力系统中，状态的变化只由状态本身决定，一个通用的描述此动力系统的方程如下：\n\n$$ \\dot{s}(t) = \\frac{\\mathrm{d}s}{\\mathrm{d}t} = D(s(t)) $$\n\n如果简化为线性动力系统，则可以描述为：\n\n$$ \\dot{s}(t) = \\frac{\\mathrm{d}s}{\\mathrm{d}t} = D·s(t) $$\n\n那么这个动力系统即被称为 **自治动力系统（Autonomous Dynamical System)** 。\n\n然而这是非常简单的形式，现实中我们经常遇到的动力系统都会接收外部的输入,记为 $(a)$ (action, 在动力系统上采取的行动)， 那么类似地， **有外部输入 $(a)$ 的动力系统** 的通用描述方程和线性描述方程分别为：\n\n$$ \\dot{s}(t) = \\frac{\\mathrm{d}s}{\\mathrm{d}t} = D(s(t), a(t)) $$\n\n$$ \\dot{s}(t) = \\frac{\\mathrm{d}s}{\\mathrm{d}t} = D·s(t) + B·a(t)$$\n\n\n下面引入一个简单的例子。假设有一只松鼠在一棵树上，它想要吃到另一棵树上的坚果，这时候它很饥饿，所以它想要尽可能快地吃到那个坚果，\n\n\n![](SquirrelJump.jpg)\n","tags":["计算神经科学","运动理论","Neuromatch Academy (NMA)"]},{"title":"基于智能计算的脑机制研究-邬霞","url":"/2021/08/21/基于智能计算的脑机制研究-邬霞/","content":"\n## 脑机制的发展方向：\n### 1. 认识脑\n\n现在的研究认为人脑是以网络的形式相互作用的。结合图论的方法分析大脑功能连接网络的拓扑机构，探索功能区域大的相互作用关系，进而揭示人脑内部的工作原理。\n\n### 2. 保护脑\n\n了解脑发育和衰老过程；神经性、神经性疾病的康复和预防。\n### 3. 创造脑\n\n开发脑型计算机、提升大脑功能（脑机接口）。\n\n---\n\n## 脑机制的研究方法：\n\n解剖学方法、生物化学方法、无创脑影像技术、电生理学方法、分子生物学方法等不同尺度层面的研究方法。（研究技术的发展促进脑机制的研究）\n\n---\n\n## 复杂脑网络的研究主要依赖于测量数据与计算模型：\n1. 数据驱动的脑网络分析（脑影像数据信噪比低、高维小样本的特性，使得脑网络精确构建难）；\n2. 基于模型的脑网络计算（计算模型抽象、脑网络复杂，使得脑网络分析与行为关联刻画难）。\n\n---\n\n## 引入智能计算方法（研究内容）：\n### 1. 脑网络成分识别（哪些脑区构成一个网络）\n- 稀疏模块化可重叠的高斯图模型算法（引入先验知识：稀疏、模块化，找出不同组）\n- 希尔伯特-黄变换，提取时频指标（数据驱动分组）\n\n### 2. 脑连接效应/有效连接分析（上述网络之间如何连接）\n- 基于fMRI的因果网络构建与验证 （AD与正常人群的不同）\n- 构建情感冲突状态下抑郁症患者的脑功能连接 （EEG，动态时间规整，功能连接）\n- 基于EEG源信号的有向连接网络构建 （先溯源，再分频段建立连接）\n\n### 3. 脑认知行为预测（挖掘有效生物标记物）\n- 正常认知行为——自恋人格的个体化预测 （算各种网络指标，之后相关分析：Exploratory Correlation Analysis, Pearson Correlation Analysis）\n- 正常认知行为——情绪预测 （挖掘影响个体之间对不公平忍受度的网络标志物）\n- 正常认知行为——认知负荷评估 （不同认知负荷下网络标志物不同）\n- 脑疾病辅助预测——阿尔兹海默症、抑郁症等\n\n### 4. 脑状态解码研究\n- 高密度EEG数据的情感计算\n- 基于大脑时空共变特性的脑状态评估 （fMRI, 考虑时间信息，方法：Deep Sparse Recurrent Auto-encoder）\n\n\n\n\n---\n>记录自：2021.8.21 基于智能计算的脑机制研究-邬霞-北京师范大学（多模态认知计算与脑机智能）\n","tags":["讲座记录"]},{"title":"Markdown的基本语法（英文稿）","url":"/2019/03/11/Basic-grammar-of-Markdown/","content":"\n# 1. Header:\n  The symbol \"#\" indicates the level of a header, which is shown as follows:\n  # first-level header\n  >\"# first-level header\": first-level header\n\n  ## second-level header\n  >\"## second-level header\": second-level header\n\n  ##### fifth-level Header\n  >\"##### fifth-level header\": fifth-level header\n\n  And so on.\n\n# 2. Typeface:\n- Bold: \\**The text to be bold**\n\n  Result: **The text to be bold**\n- Italic: \\*The test to be italic*\n\n  Result: *The text to be italic*\n- Bold and Italic: \\*\\*\\*The test to be bold and italic*\\**\n\n  Result: ***The test to be bold and italic***\n\n- Strikethrough: \\~~The test to be strikethrough~~\n\n  Result: ~~The test to be trikethough~~\n\n- Newline: two ' ' in the gap between the 2 texts. Or < br> (No ' ').\n\n  Result:\n  **Tip:**\n    If you want to share your library by copying the files, you need to copy both the .data file and the .enl file. They need to work together.\n\n    <br>\n\n- Be placed in the middle: < center>markdown< /center> (No ' ').\n<center>markdown</center>\n\n# 3. Quotation:\n  The symbol \">\" indicates the level of a quotation just as what \"#\" does.\n\n  Grammar: > the quotation\n\n    >This is the first-level quotation\n\n  And so on.\n\n# 4. Split Line:\n  Symbol: more than 3 \"-\" or \"\\*\", the results are shown as follows:\n\n---\n***\n\n# 5.Figure:\n  Grammar: \\!\\[the caption(shown below the figure)](the address of the figure (can be online or offline)\"the title of the figure (shown when the mouse is over the figure)\")\n\n  ![Online](https://pic.36krcnd.com/201801/25121602/9hk589zocc2etptd!heading \"BCI1\")\n\n  ![Offline](https://raw.githubusercontent.com/1997Jonas/1997Jonas.github.io/master/images/BCI2.jpg \"BCI2\")\n\n  However, the offline way is not recommended. (Offline way cannot be use in deployment of a website)\n\n  Place the figure in the middle:\n  < div align=center> (No the first ' ').\n\n  \\!\\[  ](   )\n\n  < div align=left> (No the first ' ')\n\n  Result:\n\n  <div align=center>\n\n  ![](https://latex.codecogs.com/gif.latex?X%5Ccdot%20%5Cfrac%7BX%7D%7BY%7D)\n\n<div align=left>\n\n  set the figure's size:\n < img **width=200** src=\"the link of the figure.png\" >\n\n  Result:\n\n![](https://raw.githubusercontent.com/1997Jonas/1997Jonas.github.io/master/images/ERDERS6.png)\n\n\n  <img width=200 src=\"https://raw.githubusercontent.com/1997Jonas/1997Jonas.github.io/master/images/ERDERS6.png\" >\n\n\n# 6. Hyperlink:\n  Grammar: \\[Hyperlink's name](Address \"title\")\n\n  [Emoji's website](https://www.webfx.com/tools/emoji-cheat-sheet/ \"Emoji's website\")\n\n\n\n# 7. List:\n- Unordered: symbol ‘-’ or '+' or '\\*', the result is just like this line.\n\n\n1. Ordered: 1. or 2. or 3. and so on, the result is just like this line.\n\n\n- nesting: three ' '(blank) between the indexes.\n\n# 8. Table:\nGrammar:(No blank line!)\n\nheader 1|header 2|deader 3\n\n---|:--:|---:\n\ncontent 11|content 21|content 31\n\ncontent 12|content 22|content 32\n\nThe result is as follows:\n\n\n\n  header 1|header 2|deader 3\n  ---|:--:|---:\n  content 11|content 21|content 31\n  content 12|content 22|content 32\n\n# 9. Code:\nGrammar:\n- 'One-line code'\n\n  `create database hero;`\n- ''' A passage of code'''\n\n  ```\n      function fun(){\n           echo \"Hello world!\";\n      }\n      fun();\n  ```\n\n# 10. Formula:\n  Use the website: [LateXFormula](https://www.codecogs.com/latex/eqneditor.php)\n  Create the formula and then Choose 'URL Encoded' option below.\n  Copy the URL and then paste it in Markdown.\n\n  Example: ![](https://latex.codecogs.com/gif.latex?X%5Ccdot%20%5Cfrac%7BX%7D%7BY%7D)\n  <br/>\n","tags":["科研基本技能"]}]