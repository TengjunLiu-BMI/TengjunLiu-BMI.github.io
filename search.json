[{"title":"大型语言模型的提示工程（Prompt Engineering for LLM）","url":"/2023/05/04/PromptEngineering/","content":"\n你听说过人工智能可以像人一样聊天吗？不，我不是在跟你开玩笑。这就是大型语言模型（Large langauge model, LLM）所能实现的事情！它们是一种新型的人工智能技术，可以通过学习大量的文本数据，来理解人类语言的复杂性和变化性。最近，一种名为ChatGPT的大型语言模型正引领着一场新的范式革命。ChatGPT不仅可以像人一样回答你的问题，还能够进行有趣的闲聊，甚至写作文本。它的出现将彻底改变我们与人工智能的互动方式。在本文中，我们会介绍如何有效地对大型语言模型进行提问（Prompt Engineering, 提示工程），以获得你想要的信息。<br>\n\n## 一、提问原则（Principles）\n进行有效的提问，我们要先明确几个原则：<br>\n1. 写出清晰明确的指导（Write clear and specific instructions）。<br>\n- 使用分隔符（Use delimiters），如\"\"\"（triple quote）、'''（triple backticks）、---（triple dashes）等；<br>\n```\n例子：Summarize the text delimited by triple backticks into a single sentence. ```{text}```\n```\n- 要求结构化输出（Ask for a structured output），如JSON, HTML；<br>\n```\n例子：Generate a list of three made-up book titles along with their authors and genres. Provide them in JSON format with the following keys: book_id, title, author, genre. <br>\n```\n- 验证条件是否满足（Check whether conditions are satisfied）；<br>\n```\n例子：You will be provided with text delimited by triple quotes. If it contains a sequence of instructions, re-write those instructions in the following format:\nStep 1 - ...\nStep 2 - …\n…\nStep N - …\nIf the text does not contain a sequence of instructions, then simply write \"No steps provided.\"\n\"\"\"{text_1}\"\"\"\n```\n- 小样本提示（Few-shot prompting）。<br>\n```\n例子：Your task is to answer in a consistent style.\n<child>: Teach me about patience.\n<grandparent>: The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\n<child>: Teach me about resilience.\n```\n2. 给模型足够的时间去“思考”（Give the model time to “think”）。<br>\n- 将负责的任务分解成模型好理解的任务（Specify the steps required to complete a task）；\n```\n例子：prompt_1 = f\"\"\"\nPerform the following actions:\n1 - Summarize the following text delimited by triple backticks with 1 sentence.\n2 - Translate the summary into French.\n3 - List each name in the French summary.\n4 - Output a json object that contains the following keys: french_summary, num_names.\n\nSeparate your answers with line breaks.\n\nText:\n\"\"\"{text}\"\"\"\n```\n- 给模型具体的指导（Instruct the model to work out its own solution before rushing to a conclusion）；\n```\nprompt = f\"\"\"\nYour task is to determine if the student's solution \\\nis correct or not.\nTo solve the problem do the following:\n- First, work out your own solution to the problem.\n- Then compare your solution to the student's solution \\\nand evaluate if the student's solution is correct or not.\nDon't decide if the student's solution is correct until\nyou have done the problem yourself.\n\nUse the following format:\nQuestion:\n\"\"\"question here\"\"\"\nStudent's solution:\n\"\"\"student's solution here\"\"\"\nActual solution:\n\"\"\"steps to work out the solution and your solution here\"\"\"\nIs the student's solution the same as actual solution just calculated:\n\"\"\"yes or no\"\"\"\nStudent grade:\n\"\"\"correct or incorrect\"\"\"\n\nQuestion:\n\"\"\"\nI'm building a solar power installation and I need help \\\nworking out the financials.\n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost \\\nme a flat $100k per year, and an additional $10 / square \\\nfoot\nWhat is the total cost for the first year of operations \\\nas a function of the number of square feet.\n\"\"\"\nStudent's solution:\n\"\"\"\nLet x be the size of the installation in square feet.\nCosts:\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n\"\"\"\nActual solution:\n\"\"\"\n```\n\n## 二、提示的迭代开发（Iterative Development）\n就算我们有了上述的一些提出prompt的技巧/原则，我们也很难保证我们提出的prompt就是最好的，甚至依然有可能是有问题的。通过如下图所示的迭代开发流程，我们可以提出更好的prompt。<br>\n![](IterativePromptDevelopment.png)<br>\n例如:<br>\n- 对生成答案的长度不满意：**use at most** 50 words/sentences/characters.<br>\n- 对关注的点不满意：be technical in nature and **focus on** the materials the product is constructed from.<br>\n\n## 三、总结/提取提示（Summarizing/Extracting Prompt）\n对文本的总结和对信息的提取是LLM模型很重要的应用场景。下面分别是简单总结和提取信息的例子：\n```\nYour task is to generate a short summary of a product review from an ecommerce site to give feedback to the Shipping deparmtment.\n\nSummarize the review below, delimited by triple backticks, in at most 30 words, and focusing on any aspects that mention shipping and delivery of the product.\n\nReview: \"\"\"{prod_review}\"\"\"\n```\n```\nYour task is to extract relevant information from a product review from an ecommerce site to give feedback to the Shipping department.\n\nFrom the review below, delimited by triple quotes extract the information relevant to shipping and delivery. Limit to 30 words.\n\nReview: \"\"\"{prod_review}\"\"\"\n```\n\n## 四、推理（Inference Prompt）\n对文本进行总结/信息提取之后，我们自然也会感兴趣文本背后蕴含的信息，于是我们也可以让LLM模型对所得到的文本进行推理。例如，我们可以问LLM模型分析的文本的情感（Sentiment）是积极的还是消极的：<br>\n```\nWhat is the sentiment of the following product review,\nwhich is delimited with triple backticks?\n\nGive your answer as a single word, either \"positive\" or \"negative\".\n\nReview text: '''{lamp_review}'''\n```\n或者也可以让LLM模型列出文本背后的情感：<br>\n```\nIdentify a list of emotions that the writer of the following review is expressing. Include no more than five items in the list. Format your answer as a list of lower-case words separated by commas.\n\nReview text: '''{lamp_review}'''\n```\n其他的推理还可以是文本相关的主题等等。\n\n## 五、变换（Transforming Prompt）\n除了总结推理，LLM模型另一个重要的应用：变换。变换包含很多种形式：翻译、语气、语法矫正等。可以用到的prompt比如：<br>\n翻译：\n```\nTranslate the following text to Spanish in both the formal and informal forms:\n'Would you like to order a pillow?'\n```\n语气变换：\n```\nTranslate the following from slang to a business letter:\n'Dude, This is Joe, check out this spec on this standing lamp.'\n```\n语法矫正：\n```\n\"Proofread and correct the following text. If you don't find and errors, just say \"No errors found\".\n```\n\n## 六、其他（Others）\n除了这些应用，LLM模型还可以用于文本拓展（Expanding，如续写故事、协助头脑风暴等）或者构建聊天机器人（Chatbot，按照一定的设定回答问题）等。LLM的潜力很大程度上依赖于使用者的想象力。\n\n## 七、局限性（Limitations）\n此外，很重要的一点是我们也要清楚地意识到LLM模型也是有局限性的：幻觉（Hallucinations）。也就是说，它会产生一些听起来好像是对的但其实是错的回答。沿着上面我们使用LLM模型的技巧，我们可以使用以下方法尽量减少幻觉的产生：先自己找到相关的信息，再让LLM模型基于这些信息回答问题。\n\n\n\n\n---\n> 参考课程：ChatGPT Prompt Engineering for Developers - by\nIsa Fulford, Andrew Ng (https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\n","tags":["科研基本技能"]},{"title":"工程微分方程（Differential Equations For Engineers）","url":"/2023/05/04/DifferentialEquationsForEngineers/","content":"\n## 0. 引言\n在正式介绍微分方程之前，我想很重要的一件事情是建立起学习动机：为什么要学？一个比较现实的答案就是物理学定律/工程过程通常都是依靠微分方程进行描述的，那么对于一个理工科的学生/工作者，对微分方程有一定的了解也就成了一个必修项。一个比喻就是，就像是你不得不去阅读德语写成的材料，那么你就必须先学会德语。更具体地，解决现实问题的框架可以理解为：<br>\n![](intro_framework_DE.jpg)<br>\n当你面对一个现实问题，首先要基于一些基本假设，对问题进行抽象，形成一个用一组**微分方程**描述的数学模型，接着运用数学方法对其进行求解，再用得到的结果对现实问题进行解释。在这本书中，我们会主要关注于数学分析上。<br>\n\n下面举四个微分方程中的简单又常用的例子及其通解，可以将其通解代入原式进行验证：\n$$\n\\frac{d y}{d x}=k y, k>0  \\Longrightarrow y(x)=C e^{k x}\n$$\n$$\n\\frac{d y}{d x}=-k y, k>0 \\Longrightarrow y(x)=C e^{-k x}\n$$\n$$\n\\frac{d^{2} y}{d x^{2}}=-k^{2} y, k>0 \\Longrightarrow y(x)=C_{1} \\cos (k x)+C_{2} \\sin (k x)\n$$\nPS: 对于一个二阶方程，通解中会有两个待定的常数（$C_1, C_2$）。\n$$\n\\frac{d^{2} y}{d x^{2}}=k^{2} y, k>0 \\Longrightarrow y(x)=C_{1} e^{kx}+C_{2} e^{-kx}\n$$\n\n接着简单介绍一下微分方程的分类方式：\n1. **常微分方程**（Ordinary differential eqations, **ODE**, 左式）与**偏微分方程**（Partial differential equations, **PDE**, 右式）。\n$$\n\\frac{dy}{dt}=ky \\mid  \\dfrac{\\partial^2u}{\\partial t^2}=\\dfrac{\\partial^2u}{\\partial x^2}+\\dfrac{\\partial^2u}{\\partial y^2}.\n$$\nODE指微分对象只有一个变量，而PDE指偏微分对象有多个变量。<br>\n2. 方程**阶数**（**order**）。\n\n\n---\n> 参考书籍：Notes on Diffy Qs: Differential Equations for Engineers - by Jiří Lebl (https://www.jirka.org/diffyqs/html/frontmatter-1.html)\n","tags":["基本数学"]},{"title":"统计力学基础入门（Introduction to Basic Statistical Mechanics）","url":"/2022/09/16/Basic-Statistical-Mechanics/","content":"\n统计力学/热动力学并不是一个很新的概念，可以说是前现代（pre-modern）物理，但是在我们的生活中随处可见。统计力学的一个核心思想是：宏观系统由很多微观粒子（如分子）组成，我们对这样的系统的行为基于最大概率进行预测。所以概率论是其很重要的一个基础。在进行统计力学的学习之前，需要先确保自己对概率论足够熟悉。\n\n我们先回顾热力学的内容当作热身。首先，一个热力学中很重要的定律就是能量守恒定律（热力学第一定律），对一个封闭系统其可以由下面公式简单描述：\n$$\n\\frac{dE}{dt} = 0\n$$\n其中$E$为系统的能量，t为时间。其另一种表述为：\n$$\n\\text{d}E = \\text{d}W + \\text{d}Q\n$$\n其中$\\text{d}W$表示运动做功带来的能量变化，$\\text{d}Q$表示热传递带来的内能的变化。他们都依赖于变化路径，而不只是依赖于状态。\n\n另外一个很重要的概念——熵（entropy），它描述了一个系统的混乱程度。从统计力学的角度，其可以由下面的公式定义：\n$$\nS=\\ln \\Omega, \\Omega \\in N^+\n$$\n$\\Omega$则为该宏观系统中所包含的微观状态的数量（取值为正整数）。这个公式也被称为玻尔兹曼原理，它描述了系统中的微观特性（$\\Omega$）与其热力学特性（$S$）的关系。另外这里的熵与香农信息论中定义的熵相差了一个玻尔兹曼常数（$S_{Shannon}=k_BS$）。<br>\n\n所以什么是微观状态的数量$\\Omega$呢？它指的是一个宏观系统中的微观粒子可能出现的排列关系（出现概率不为0）。假设现在有两个均匀的不断在被抛的硬币（类比为微观粒子）构成一个宏观系统，那么这个宏观系统就会有4种微观状态（HH、HT、TH、TT，H为head表示正面，T为tail表示反面）。但如果其中第一个硬币并不均匀，使得其只能抛出正面，那么这个宏观系统就只会有两个微观状态（HH、HT）。所以可以看出微观状态的数量与这个特定系统的性质有关。而同样考虑上述的硬币不均匀的例子，假设我们并不知道它的这个性质，那么我们就不知道TH、TT两种微观状态出现的概率为0，也无法判断出其仅有两个微观状态。可见，微观状态的数量也与我们对宏观系统的了解程度有关。<br>\n\n当然，上述的统计力学的熵的定义也可以由其下述更普遍的定义推导得到，这里不赘述，感兴趣的话可以参考[视频](https://youtu.be/D1RzvXDXyqA)（1:12:56 to 1:38:09）。\n$$\nS = - \\sum_i p(i)\\log p(i)\n$$\n其中$i$为微观状态，$p(i)$为其出现的概率。这里的概率就体现了熵的大小与我们对宏观系统了解程度有关。另外有个事实：上述的概率分布依赖于宏观系统的能量，能量越高，p(i)的分布越广，熵越大（能量和熵的大小是单调正相关的）。根据这个公式，我们也可以得到能量与熵的关系：<br>\n$$\nS(E)=- \\sum_i p(i,E)\\log p(i,E)\n$$\n而$p(i)$如何得到？我们可以把一个系统分割成足够多的$N$个子系统，$n_i$表示其中占有状态$i$的子系统的数目（occupation number），则有：\n$$\np(i) = \\frac{n_i}{N}\n$$\n\n\n对于熵，我们又有热力学第二定律：熵随着时间一直在增加或至少保持不变，即：<br>\n$$\ndS\\ge0\n$$\n基于热二定律我们可以有一个推论：当一个系统是隔热的（adiabatic）/在缓慢变化，那么其熵变为0。另外，热二定律与混沌理论也有一定的关系：事物随时间会分叉/变得更加混沌，熵就也变得更大（讨论见[视频](https://youtu.be/sg15UClUY48)（1:11:00 to end））。<br>\n\n而我们日常生活中经常提到的温度（temperature），其对宏观系统的描述并没有能量、熵那么根本。温度其实对能量数量的一个描述，他们的关系如下：\n$$\n\\bar{E} = \\frac{3}{2}kT_k\n$$\n其中$T_k$为温度（单位为开尔文$K$），$k$为玻尔兹曼常数（$k=1.4\\times 10^{-23}J/K$）。简化起见，后面我们定义$T=k_BT_k$。更根本地说，温度由一个系统的能量（即内能）和熵定义如下：\n$$\nT = \\frac {d\\bar{E}}{dS}\n$$\n\n在比热力学第一、第二定律更根本的，是热力学第零定律（热平衡定律）：若两个热力学系统均与第三个系统处于热平衡状态，那么这两个系统也必定相互处于热平衡。而对于不处于热平衡状态的两个系统，能量必定从温度高的系统流向温度低的系统，直到在一定时间后达到热平衡（系统的温度相同）。三个定律的关系的推导可以参见[视频](https://youtu.be/EmM1jOV1uSY)（15:00 to 24:30）。<br>\n\n\n我们再回到对于微观状态排列方式的讨论。假设现在有3个子系统，要将他们分配到3个状态中，每个状态有1个子系统，则这样的排列方式数目有：\n$$\n\\text{number of arrangement} = \\frac{N!}{\\prod_i n_i!}=\\frac{N!}{n_1!n_2!n_3!} = \\frac{3!}{1!1!1!}=6\n$$\n上述的公式告诉我们，重排列$N$个子系统到$i$状态中的方式的数目。这个数目越大，则意味着某种排列的出现概率越大。也就是说，在两个限制条件下（$\\sum_i n_i = N$ & $\\sum_i n_iE_i=E_{total}$），我们想找到一种$N$个子系统在$i$中最可能的分布方式（也就是$n_i$的大小）。但如果$N$变得非常大，这时我们就需要借助Stirling's Approximation（$N!\\approx N^Ne^{-N}$，证明可参见[视频](https://youtu.be/EmM1jOV1uSY)（56:00 to 1:04:30））。基于这个估计，取对数，我们有（证明可参见[视频](https://youtu.be/EmM1jOV1uSY)（1:10:00 to 1:16:30））：\n$$\n\\log{(\\text{number of arrangement)}} = -N\\sum_ip_i\\log{p_i}\n$$\n也就是说，$N$足够大，在限制条件（$\\sum_i p_i = 1$ & $\\sum_i p_iE_i=E_{total}/N$）下，求上述排列方式数目的最大值，就是求$p_i$取值是多少时，熵取得最大值。<br>\n\n而为了求$p_i$，我们又需要拉格朗日乘数（Lagrange multipliers，证明、直观理解及例子参见[视频](https://youtu.be/EmM1jOV1uSY)（1:27:15 to end）），这是一个在有限制的优化问题中非常重要的算法。比如，要求$f(x, y)$在$g(x,y)=0$时的局部极值时，我们可以引入新变量拉格朗日乘数$\\lambda$ ，这时我们只需要求下列拉格朗日函数的局部极值：\n$$\n\\mathcal{L}(x, y, \\lambda)=f(x, y)-\\lambda \\cdot g(x, y)\n$$\n或者更一般地，\n$$\n\\mathcal{L}\\left(x_{1}, \\ldots, x_{n}, \\lambda_{1}, \\ldots, \\lambda_{k}\\right)=f\\left(x_{1}, \\ldots, x_{n}\\right)-\\sum_{i=1}^{k} \\lambda_{i} g_{i}\\left(x_{1}, \\ldots, x_{n}\\right)\n$$\n直观上说，$\\lambda$是为了让新组成的函数能够在限制条件下取得最小值。而对于求最大值的问题，我们只需要对原始方程取个反再求最小值。<br>\n\n回到我们具体的问题：\n$$\n\\max -N\\sum_ip_i\\log{p_i}, \\text{ s.t. }\n\\left\\{\n\\begin{array}{l}\n\\sum_i p_i = 1 \\\\\n\\sum_i p_iE_i=\\frac{E_{total}}{N}=\\bar{E}\\\\\n\\end{array}\n\\right.\n$$\n基于拉格朗日乘数，我们可以构造一个新的函数：\n$$\nF(p) = N\\sum_ip_i\\log{p_i}+\\alpha (\\sum_i p_i-1)+\\beta(\\sum_i p_iE_i-\\bar{E})\n$$\n其中，为了将求最大值问题转变为求最小值问题，第一项的负号变为正号，$\\alpha$与$\\beta$为两个拉格朗日乘数。为了求上式的最小值，我们令上式的导数为0，对每一个$p_i$有：\n$$\n\\frac{\\partial F}{\\partial p_i} = \\log p_i +1 + \\alpha + \\beta E_i = 0\n$$\n接着我们可以得到：\n$$\np_i = z^{-1}e^{-\\beta E_i}\n$$\n这个公式描述了玻尔兹曼分布（Boltzman Distribution·），其中$z=e^{1+\\alpha}$为一个与$E_i$无关的参数。代入第一个限制条件，我们会发现其依赖于$\\beta$：\n$$\nz = \\sum_i e^{-\\beta E_i} \\\\\n$$\n这个公式被称为配分函数（partition function）。<br>\n\n留意到，$\\beta$越大，$p_i$衰减得越快，系统中微观状态更集中分布于$E_i$比较小的状态，则$\\bar{E}$越小，反之亦然。将上式代入到我们的限制条件与熵的定义式中，再经过一些技巧性变换（参见[视频](https://youtu.be/rhFkYjaM5kE)（18:30 to 45:00）），我们得到两个很有趣的公式：\n$$\n\\bar{E} = - \\frac{ \\partial \\log z}{\\partial \\beta} \\\\\nS = \\beta \\bar{E} + \\log z\n$$\n于是我们有：\n$$\ndS = \\beta d\\bar{E} + \\bar{E} d\\beta + \\frac{\\partial \\log z}{\\partial \\beta} d\\beta = \\beta d\\bar{E} + \\bar{E} d\\beta -\\bar{E} d\\beta = \\beta d\\bar{E}\n$$\n回顾温度的定义，我们有：\n$$\n\\beta = \\frac{dS}{d\\bar{E}}=\\frac{1}{T}\n$$\n可以发现，$\\beta$有其物理含义，即逆温度（inverse temperature）。对这些公式的实际物理问题（理想气体）应用参见[视频](https://youtu.be/rhFkYjaM5kE)（57:00 to end）。<br>\n\n由上面的公式，我们又能得到：\n$$\n\\bar E-TS = - T\\log z\n$$\n其中式子左边被称为亥姆霍兹自由能$A$（Helmholtz free energy）：\n $$\nA = \\bar{E}-TS\n $$\n\n之后我们介绍一个定理（证明参见[视频](https://youtu.be/2BJYXuZZK3c)（26:00 to 36:40））：\n$$\n\\left.\\frac{\\partial \\bar E}{\\partial V}\\right|_{S}=\\left.\\frac{\\partial \\bar E}{\\partial V}\\right|_{T}-\\left.\\left.\\frac{\\partial \\bar E}{\\partial S}\\right|_{V} \\frac{\\partial S}{\\partial V}\\right|_{T}\n$$\n这个定理能告诉我们什么？首先当我们要对一个系统进行观测的时候，维持熵不变很难观测得到，我们可以将其转换为更好观测的温度和体积。另外考虑一个绝热系统（熵不变）的压力（pressure，如气压）为$p=\\left.\\frac{\\partial \\bar E}{\\partial V}\\right|_{S}$，且$T=\\frac{d\\bar E}{dS}$，再基于上面三个公式，我们可以得到一个非常基本的表达式：\n$$\n p=-\\left.\\frac{\\partial(\\bar E-T S)}{\\partial V}\\right|_{T} = -\\left.\\frac{\\partial A}{\\partial V}\\right|_{T}=\\left.T \\frac{\\partial \\log z}{\\partial V}\\right|_{T}\n$$\n这个公式适用于任何绝热系统。特别地，对于理想气体，我们可以得到理想气体公式（证明参见[视频](https://youtu.be/2BJYXuZZK3c)（1:04:00 to 1:08:00））：\n$$\npV=NT\n$$\n留意到这里的$T$吸收了玻尔兹曼常数$k_b$。<br>\n\n接下来我们回到更微观的情况，假如我们考虑粒子间的势能（仅考虑一对粒子间的相互作用，其他更高阶的相互作用概率相对低至可忽略不计），我们有：\n$$\n\\log z = \\log z_{0}-\\frac{B N^{2}}{2 V} u_{0}\n$$\n其中$z_0$对应着内能/温度，$u_0$是每对粒子在系统里的平均势能。进一步地，我们有：\n$$\nE=-\\frac{\\partial \\log z}{\\partial \\beta}=\\left[\\frac{3}{2} \\cdot T+\\frac{\\rho}{2} u_{0}\\right] N \\\\\np = \\left. T \\frac{\\partial \\log z} {\\partial V} \\right|_{T} = \\rho T+\\frac{1}{2} \\rho^{2} u_{0}\n$$\n具体的推导见[视频](https://youtu.be/bW30Rj6w8VI)（0:16:45 to 1:20:00）。\n\n接下来我们讨论简谐振荡子（harmonic oscillator）运动，如下图所示：\n![](HO.png)<br>\n其中弹簧的弹力系数为$k$，砝码的质量为$m$，那么这个振荡子的能量是多少？\n$$\nE = \\frac{p^2}{2m} + \\frac{kx^2}{2}\n$$\n其中$p$为砝码的动量（momentum）。接着按照之前的思路，我们将能量大小代入配分函数：\n$$\nz = \\int e^{-\\beta\\frac{p^2}{2m}}e^{-\\beta\\frac{kx^2}{2}} \\text{d}p\\text{d}x = \\int e^{-\\beta\\frac{p^2}{2m}}\\text{d}p \\int e^{-\\beta\\frac{kx^2}{2}} \\text{d}x\n$$\n由换元法，我们可以得到（具体的推导见[视频](https://youtu.be/sg15UClUY48)（23:00 to 28:00））：\n$$\nz = 2\\pi \\sqrt{\\frac{m}{k}} \\frac{1}{\\beta}= 2\\pi \\sqrt{\\frac{m}{k}} T\n$$\n所以为了求能量，我们对$z$取对数，有：\n$$\nE = -\\frac{\\log z}{\\beta}=T\n$$\n类似的思路也可以用于求解量子简谐振荡子的能量（具体的推导见[视频](https://youtu.be/sg15UClUY48)（39:00 to 1:11:00））。\n\n接着我们讨论理想化/理论上的一维磁体（magnet）。假设磁体由$N$个仅能指向上或下两个方向（$\\sigma_i=\\pm1$表示）且彼此不相互影响的小磁体构成，且他们排成一列。每个小磁体有一个与磁场强度$H$有关的磁矩$\\mu$，则这个小磁体的能量为：\n$$\nE_i = \\sigma_i\\mu H\n$$\n故整个磁体的能量为:\n$$\nE = \\sum_i^NE_i=(n-m)\\mu H\n$$\n其中$n$为指向上的小磁体的数量，$m$为指向下的小磁体的数量（$n+m=N$）。所以这$N$个小磁体有多少种排列方式使得有$n$个指向上、$m$个指向下？或者说，这个系统可以有几种状态？\n$$\n\\text{\\# state} = \\frac{N!}{n!m!}\n$$\n接着我们求解配分函数，根据二项式展开（bionomial expension）：\n$$\nz = \\sum^{\\text{\\# state}}_{n,m}e^{-\\beta \\mu H(n-m)} = \\sum_n^N \\frac{N!}{n!(N-n)!} (e^{-\\beta\\mu H})^n (e^{\\beta \\mu H})^{N-n} = (e^{-\\beta\\mu H}+e^{\\beta\\mu H})^N = 2^N\\cosh {(\\beta\\mu H)}^N\n$$\n另外根据（平均）磁化强度$M$的定义（$M=(n-m)/N$），我们有$\\bar E=NM\\mu H$，所以知道了平均能量$\\bar E$，我们就能求解$M$。为了求解$\\bar E$，我们对$z$取对数有：\n$$\n\\bar E = - \\frac{\\partial \\log z}{\\partial \\beta} = -N\\mu H\\frac{\\sinh {(\\beta \\mu H)}}{\\cosh {(\\beta \\mu H)}}\n$$\n所以我们有：\n$$\nM = - \\frac{\\sinh {(\\beta \\mu H)}}{\\cosh {(\\beta \\mu H)}}=-\\tanh {(\\beta \\mu H)}\n$$\n回忆一下双曲正切函数的形状：\n![](tanh.png)<br>\n所以当温度$T$很低的时候，$\\beta$很大，这时候$M=-1$；而当温度$T$很高的时候，$\\beta$趋近于0，这时候$M=0$。留意到这个系统里没有$M$没有突变（也即后文描述的相变）。\n稍微做一点推广，假设相邻的单元（如上文的小磁体，spin）会相互影响，我们就得到了非常出名的一维Ising模型。当相邻的两个单元指向同一个方向，那么其能量为负（也就是系统的偏好朝向——系统总是偏好处于低能状态）；而当相邻的两个单元指向相反方向，那么其能量为正，所以对于整个系统有：\n$$\nE = -j\\sum_i^{N-1} \\sigma_i\\sigma_{i+1}\n$$\n其中$j$为一个描述能量大小的常量（如上一个例子的$\\mu H$）。留意到这个系统中存在对称性：相邻的单元的指向同时取反，能量大小不会改变。如果我们定义一个描述独立的相邻单元相互作用的变量$\\mu_i=\\sigma_i\\sigma_{i+1}$，我们得到一个与上面的例子类似的能量表达式：\n$$\nE =  -j\\sum_i^{N-1} \\mu_i\n$$\n接着类似地，我们可以得到配分函数与平均的相互作用大小$\\bar \\mu$：\n$$\nz=(2\\cosh {(\\beta j)})^{N-1} \\\\\n\\bar \\mu = \\tanh {\\beta j}\n$$\n这里的$\\bar \\mu$也可以理解为两个相邻单元的指向相同的概率。所以假设我们在一个信息传递的游戏中，如果我们要求一列单元中的第$i$个准确将信息传递到第$i+d$个单元的概率即为（要求这个过程中传递的信息没有发生变化）：\n$$\np = ( \\tanh {\\beta j})^{N-1}\n$$\n同样，留意到这个系统里没有$M$没有突变（也即后文描述的相变）。<br>\n\n接下来，我们进一步拓展，分析$D$维的Ising模型，这时候各单元排列在一个$D$维空间中，这时空间中每个单元会有$2D$个相邻单元。为了分析这个问题，我们需要用到平均场估计（mean field approximation）。即这时，对于系统中第$i$个单元有：\n$$\nE_i = -j \\sigma_i\\sum_{\\text{neighbor}} \\sigma = -j \\sigma_i \\times 2D \\bar \\sigma =  -2D j \\bar \\sigma \\sigma_i\n$$\n接着我们求解$\\bar{\\sigma_i}$，基于平均场估计对于一个自洽的系统（self-consistent）：\n$$\n\\bar{\\sigma_i} = \\bar{\\sigma} = \\tanh {((2Dj\\beta)\\bar {\\sigma})}\n$$\n为了更直观求解这个方程，我们进行变量替换，$y=(2Dj\\beta)\\bar{\\sigma}$，又$\\beta=1/T$，我们有：\n$$\n\\frac{T}{2Dj}y=\\tanh y\n$$\n接着画出等号两边的函数，有：\n![](ytanhy.png)<br>\n可以看出，当温度很高，线性函数的斜率很大（斜率大于1），这时候上述方程只有$\\bar \\sigma = 0$一个解，这也跟直观印象吻合，因为温度很高的时候，各个单元随机朝向，最终平均下来即为0；而当温度比较低（斜率小于1），这时候上述方程除了$\\bar \\sigma = 0$还有其他解，这另外的解代表着磁化现象。$T_{c}=2Dj$为系统的临界点（critical point），高于这个温度，系统中各单元倾向于均匀分布（$\\bar \\sigma=0$），而低于这个温度，系统可以出现一定的偏向（$\\bar \\sigma \\ne0$）。<br>\n之后，我们考虑对系统施加一个外部细微场，这时每个单元会被施加一个细微的干扰，这个干扰的大小仅依赖于其自身状态，即：\n$$\nE_i = -2D j \\bar \\sigma \\sigma_i + B\\sigma_i = (-2D j \\bar \\sigma + B)\\sigma_i\n$$\n类似地，我们会得到：\n$$\n\\frac{T}{2Dj}y=\\tanh {(y+ B/T)}\n$$\n考虑到$B/T$为正，上图中的黑色曲线会向左平移（平移量也会取决于温度的大小），那么此时$\\bar \\sigma=0$的解就消失了，仅剩下$\\bar \\sigma >0$的解。而无论$B$多小，上述结论都成立，于是突变/相变（phase transition）又出现了。\n\n---\n> 参考课程：Series of Statistical Mechanics Lectures - by Leonard Susskind, Stanford University (https://youtu.be/D1RzvXDXyqA)\n","tags":["计算神经科学","理论物理"]},{"title":"学习的方法论（Learning How to Learn）","url":"/2022/03/04/learningHowToLearn/","content":"\n\n\n## 思考的方式：专注与分散（Focused versus Diffuse Thinking）\n\n首先设想一下，如果你在想一个问题，但始终得不到答案，你会怎么办？对没有脑子的僵尸（zombie）来说，它可以不断地撞墙，但我们比它们聪明那么多，应该怎么做才能学得更好而且不焦虑呢？这里我们就要引入思考的两种方式：**专注模式（Focused Mode）** 和 **分散模式（Diffused Mode）**。<br>\n\n专注地思考似乎很好理解，但分散地思考就感觉不太好理解了。我们可以将大脑类比为一个弹珠机，弹珠机的每个点代表着大脑的某个区域。<br>\n\n对于**专注模式**而言，你已经**熟悉**了某个/种问题如何解决，这种解决方式就可以用弹珠机的特定几个点的碰撞模式来表示，当你的想法（Thought）在这几个点碰撞之后，boom！你的问题就解决了！也就是说，对于一个你熟悉的问题，你知道你的想法在的大脑中应该走怎样的路径/模式（或者说你的思考的起始点应该在哪），你的问题就能解决，于是你会选择专注到这个路径/模式上，这就是思考的专注模式。比如你正在解你很轻松就能解出来的薛定谔方程（嗯？），又或者你在进行着你熟悉的文学批评。专注模式更利于我们解决问题。<br>\n\n而对于**分散模式**而言，你遇到了某个你从没遇到过的问题，所以首先你并不知道怎么解决，这个时候你的想法需要在你大脑的弹珠机里不断乱撞，最终才能找到一个出路。也就是说，对于一个你**陌生**的问题，你并不知道问题的解决方式是什么，你只能在大脑中不断探索，无法专注到某个路径/模式上（或者说你并不知道你的思考的起始点应该在哪），这就是思考的分散模式。分散模式更利于我们进行创造性思考。<br>\n\n![](FocusDiffsedThinking.png)\n\n所以，这样的两种思考方式对我们来说有什么启示意义？上面说了，分散模式更利于创造性思考，而专注模式利于问题的解决。也就是说，我们要学会在两种思考模式之间进行转换，特别是对于对创新性有高要求的科研工作者而言。<br>\n\n## 拖延、记忆与睡眠（Procrastination, Memory and Sleep）\n每个人或多或少都有点**拖延症**（应该吧），但是拖延是怎么发生的呢？说来其实也简单，首先我们遇到了某件事让我们不开心/焦虑，然后我们的大脑会驱使我们去做开心的事情以获得快乐。<br>\n\n![](Procrastination.png)\n\n那么怎么克服拖延症呢？著名的 **番茄工作法（Pomodoro）** 其实就是一个很有效的方式！具体操作如下：\n1. 定时25分钟，大部分人都可以保持25分钟的专注时间；\n2. 关掉所有会对你有打扰的源，以帮助你更加专注；\n3. 专注、专注再专注；\n4. 25分钟结束之后，一定要给自己一点奖赏！（促进多巴胺分泌）可以是一杯咖啡、一次闲聊、几分钟的上网甚至只是一次拉伸（休息一段时间）。\n\n另外一个克服拖延症的方法是给自己**积极的心理暗示**（比如“不能再浪费时间了！”），且为了防止拖延，应该**避免把注意力集中在结果**上，而应该是在**执行过程**中（比如告诉自己，不是“不要写完这个作业”，而是“我要花20分钟写作业上”，因为结果往往是造成你不开心/焦虑的来源）。<br>\n\n而关于记忆，我们常说的就是**熟能生巧**（Practice makes permanent）。所以要记住一个概念，练习很重要（这也是让工作记忆（working memory）转变为长期记忆（long term memory）的关键，此外练习也应该优先考虑难题）。但是，填鸭式的学习并不提倡，有效的学习应该是**间隔重复（Spaced Repetition）**的，这也是为什么番茄工作法要每25分钟休息一次的原因。做个比喻，在专注状态下，我们的大脑在用水泥讲我们大脑中的砖块（神经元）进行连接，而休息时间，我们的大脑更像是在会水泥墙砖进行修整（或者说让水泥干燥以更好支撑砖块），我们一次不能用太多水泥，而且如果没有时间进行修整的话，我们的大脑最终记忆/学习的结果可能会如下右图所示。<br>\n![](BuildingStrongNeuralStructure.png)\n\n说到记忆，另一个不得不提的事实就是，图像对记忆非常重要（我们对图像的记忆能力远大于文字）！也就是说，将要记忆的信息**图像化**，是一个重要的记忆技巧。另外要将要记忆的内容进行**有意义地分组**，也很重要（比如要记住五线谱上四个间对应的几个音（分别是F、A、C、E），那我们就可以用face这个单词来记忆）。以上两点也是**记忆宫殿**背后的机理，其很适合被用于记忆一个列表的内容，而且有趣的是，用了记忆宫殿，记忆的过程又会变成一种创造力训练。<br>\n\n对于记忆做个总结，其实要将短期记忆转变为长期记忆，有两个条件：首先，它应该是**令人印象深刻**的（足够夸张，比如一只在飞的牛）；另一个则是，内容需要我们的**重复和回顾**（重复间隔可以逐渐变长，anki在这方面有内置算法帮助实现）。实践上来说，手写（手写也能帮助记忆）索引卡（一面写着索引，另一面写着内容，同时也可以画出一些令人难忘的图像），且必要时大声朗读（建立听觉联系），也是一个有效的重复方式。<br>\n\n也许睡眠看起来是一件浪费时间的事情，但其实，在我们睡觉的时候，大脑中一天积累的有毒物质会被清理，所以睡眠不足也会让人思维能力下降，甚至产生头痛、抑郁症等后果。实际上，**睡眠**在记忆和学习中也是很重要的一部分：在睡眠的时候，大脑会将学习和思考过的想法概念进行整理，清除一些不太重要的内容，并增强重要的内容（在大脑中重复（Rehearsal）学习到的内容以增强记忆）。所以一个好的方式是：在睡前学习自己想学的内容，并不断告诉自己想梦到自己学的内容（相信潜意识的能力！），这样在睡眠中加强学到内容的记忆的概率会变大。<br>\n\n\n\n## 组块（Chunkings）\n\n首先什么是 **组块（Chunking）** ？设想你感知这个世界的一小部分（比如学习特定内容）在你脑子里就是一小块拼图，那么组块就是把这些碎片化的小拼图给拼到一起，成为一个更大的相互嵌套、有序连接的大拼图，而大拼图之间又可以组成更大的拼图。下次当你需要回忆起相关的内容，你就可以想起对应的整个拼图。这背后的神经科学其实就是：**组块就是大脑中的神经元网络，他们共同发放，接着就连在了一起（Firing together, wiring together）**。再联系到人的工作记忆容量只有4左右，我们将学到的东西组块之后，当我们对其调用时就只会占用一个工作记忆的槽位，这也变相增加了我们的记忆容量。<br>\n![](chunks-0.png)\n\n那么怎么形成我们大脑中的组块呢？可以参照以下步骤：\n1. 首先，对你想要组块化的信息**全神贯注**（不要被外部信息干扰，如回复信息）。我们可以把大脑连接组块的机制想象成一只有四个触手的章鱼（人类的工作记忆容量大致只有4！），所以当其中的某个触手被其他事情占用时，你大脑里的章鱼就无法触及到新知识的各个方面；\n![](chunks.png)\n\n2. 其次，你对建立组块的对象要有**基本的理解**。理解是一种强力胶，会帮助你找到不同碎片的联系，以建立更大的组块，且知道如何使用它们（所以记得做题、亲自操作来帮助你加强理解！）；\n![](chunks-2.png)\n\n3. 第三，要了解组块的**背景知识**。这样你就不仅知道如何使用组块，还明白应该在什么时候用它（Top-down的big piture）。比如在阅读时，可以先过一遍文章/书籍的标题/节标题，及其中的图，会帮助你大致了解其内容。换句话说，其实就是事先了解你要建立连接的组块之间可能的联系。\n![](chunks-3.png)\n\n\n## 回顾（Recall）\n\n2011年有一篇发表在 *Science* 上的论文表明，在学习某个材料之后，拿走材料，尽力回想自己学到的东西，实在想不起来再重新学习材料，学习效果会远远好于繁杂的反复阅读、或者绘制据说很有效果的思维导图。<br>\n\n> Karpicke, J. D., & Blunt, J. R. (2011). Retrieval practice produces more learning than elaborative studying with concept mapping. Science, 331(6018), 772-775. doi: 10.1126/science.1199327\n\n在回顾的时候，我们加深了对材料的理解，这也有助于我们形成知识组块。但同时需要强调的是，思维导图并不是一种无效的学习方法，而是应该在对知识组块有个清晰的理解之后（打好地基），才能起到更好的效果。<br>\n\n此外，尽管回顾不起来一些内容需要我们重新阅读材料，但也别忘了前面提到的间隔重复练习才更有效果！所以，在两次阅读之间，要有足够的间隔时间。<br>\n\n再联系到阅读文献，每读一部分内容，用自己的话在旁边总结这部分讲了什么，效果会远远好过单纯的（自欺欺人的）对整篇文章进行高亮、划线。<br>\n\n总结一下：**回顾** 是很重要的学习的过程！千万不要让自己掉进以为书里的或者谷歌上的内容就印在自己脑海里的 **学习的错觉（Illusions of Competence）**！<br>\n\n此外，在不同于学习发生的环境中进行回顾，也可以帮助我们学得更加牢固。（避免了环境对潜意识的提示作用。）<br>\n\n## 过度识记与交叉学习（Overlearning and Interleaving）\n\n**过度识记** 有其自身的意义，它能够帮助我们使得 **行为自动化**（如演奏钢琴），自动性（Automaticity）在紧张的时候确实很有用。但要警惕在单一学习阶段的重复性过度识记，这可能是对宝贵的学习时间的浪费，容易产生学习的错觉（觉得一切都很简单），也会更容易让你 **陷入思维定势（Einstellung）**。<br>\n\n而**交叉学习**是减弱思维定势的一种好的方式，也就是说，我们可以尝试在学习中，在需要不同技术和策略的问题以及情形中来回切换。但要注意的是，交叉学习也需要发生在对学习的内容有基本的掌握之后。交叉学习会让大脑**更具有灵活性和创造性**。当你在一个学科的内容内交叉学习，你会开始在这个学科内发展创造力；而当你在多个学科的内容之间交叉学习，你会更容易找到这些学科之间的联系，这也能进一步提高你的创造力。<br>\n\n要做出真正有创造性的工作，一定要避开思维定势，正如Thomas S. Kuhn在 *The Structure of Scientific Revolutions* 里所说，科学中的大部分范式转变，都是由年轻人或者之前学习其他学科的人所提出。加油吧！争取做这个年轻人！<br>\n\n\n\n## 习惯（Habbits）\n我们每个人大脑中都仿佛有一个僵尸（zombie），不加思考地做着某些事情，它驱动着我们的习惯。那么怎么驾驭这个“僵尸”让它为我们服务而不是耽误我们干事呢？养成习惯有4个要素，我们分别对他们进行阐述：\n1. **信号（the cue）**：信号可以是地点（location）、时间（time）、你的感觉（how you feel，对别人或者刚刚发生的事）和你的反应（reactions，对别人或者刚刚发生的事）。那么破坏掉触发坏习惯的信号，就是一个好的改掉坏习惯的方式（比如关掉手机或者断掉互联网），而在干正事之前做一点小事作为自己的开始信号，以后执行这个动作之后就会更容易就进入状态（比如喝一杯咖啡）；\n2. **惯式（the routine)**：做好计划（plan），比如在看书的时候都呆在某个椅子上，将坐在这个椅子与看书强绑定在一起，这样的惯式会也会帮助你养成好习惯；\n3. **奖赏（the reward）**：记住，习惯的力量之所以强大，最重要的就是因为习惯制造了神经系统的欲望。所以首先要调查一下自己喜欢的是什么？比如说要改掉玩游戏的习惯，那么你要知道什么样的奖励对你来说会比游戏更诱人，并将它附加在你想养成的行为之后。只有当大脑开始期待那个奖励，旧习惯才能得到重置；\n4. **信念（the belief）**：首先你要相信你自己能做到，否则当事情变得困难的时候（很经常的情况），你就会觉得自己做不到，想着要回到之前的舒适区（旧习惯）之中。可以与志同道合的朋友组成小组，相互鼓励，帮助自己坚持信念。\n\n\n## 待办事项（To-do list）\n对大多数人来说，学习需要在不同的每天繁杂的任务中达到平衡。一个好的方式就是在每周写下 **本周关键任务列表（Weekly list of key tasks）**。每一天**睡前**，又写下接下来一天的每日任务（因为这样可以召集你大脑中的僵尸（潜意识）去帮助你完成任务）。如果你不把任务写下来，那么它们就会占用你宝贵的工作记忆的槽位，你大可以将其空出来干更有用的事情。<br>\n\n同时我们在安排任务的时候，应该做好**生活和学习的交叉**（比如在学习之间安排一次体育锻炼），这样也可以避免久坐成疾，且让每件事更有趣。每天也要定下一个时间点（比如下午5点），在这时间点之后就是你的休息时间（休息之后才能有足够的精力进行第二天的工作）。<br>\n\n此外，也要在计划本中对完成的和未完成的任务**做好批注**，这样你才能更了解自己的时间应该怎么安排，以避免任务完成不了带来的挫败感。<br>\n\n## 其他（Others）\n\nquote from Dr. Terrence Sejnowski, **充足宽广的空间/锻炼** 可能会让大脑产生新的神经元（海马体上），这也能帮助我们学习记忆！所以，没事就多跑步吧！而且，多 **跟有创造力的人交流**，也可以让你更有创造力！所以，也多跟人交流吧！<br>\n\nquote from Dr. Norman Fortenberry in MIT，学习中的**团队合作**很重要（但注意别让学习小组变成社交小组）！讨论会发现你们彼此理解的谬误。另外要重视**休息**的重要性。<br>\n\n**比喻与类比**对于我们理解概念也很重要，它们能帮助我们从形象的角度上**理解** 抽象的东西（比如将电流比作水流，这样的比喻可以让我们记住很久，因为它可以将要学习的东西与我们脑海中已经存在的神经结构建立联系）。另一方面，比喻和类比也可以帮我们保有 **创造力**，能将我们从思维定势中解救出来。<br>\n\n现代神经科学之父卡哈尔（Cajal），曾经固执而叛逆，小时候曾进过监狱，最后也获得了诺贝尔奖。他觉得自己成功的关键之一正是因为自己不太聪明，**敢于承认错误并修正**，此外还有**坚韧不拔的毅力和自信**。<br>\n\n应对**考试**，相比于从简单的题目开始，一个更好的技巧是：**从难题开始，但2分钟觉得没有思路马上跳过（基本上都是这样），回到简单的题目**，这会让难题在你脑海中留下印象，这会开始让分散模式开始作用于难题（别忘了创造力怎么来的），通常也会让你更快进入状态。如果你觉得太紧张了，一个有效的方式就是把注意力集中在你的呼吸上（**冥想**），然后缓慢腹式深呼吸一段时间。在答案检查上，选用一个与做题**相反的顺序**，可以给大脑新鲜感，这也会让你更容易揪出错误。\n\n\n---\n> 参考网课：Learning How to Learn: Powerful mental tools to help you master tough subjects, by Deep Teaching Solutions, Coursera\n","tags":["科研基本技能"]},{"title":"统计学基础（Fundamentals of Statistics）","url":"/2022/02/21/FundamentalsStatistics/","content":"\n## 什么是统计？（What's Statistics?）\n\n现在是一个数据驱动的时代，所以深刻地理解数据分析核心的统计学显得尤为重要。</br>\n\n先明确一个问题，统计与数据科学的关系是什么？首先，很多数据科学中的机器学习算法本质上是一些统计原则的有效实现（后文会有具体介绍）。其次，从统计的角度上看：数据来自于一个**随机过程**。所以数据科学的目的是要从有限的数据中理解这个过程如何工作，并基于此做出对未来的预测或理解数据背后揭示的机理。换句话说，在数据科学中，我们需要进行统计建模。</br>\n\n简单来说，统计建模就是：复杂模型 = 简单模型 + 随机噪声。也就是说，一个好的统计模型包含两个部分，一个是简单的（可行的）过程模型，另一个是噪声的分布模型。</br>\n\n而为了理解上述统计学中的随机性（randomness），我们又离不开概率（**Probability**）的支持。那么概率与统计的关系/区别是什么？当随机过程是确定的（比如摇骰子时每一面出现的可能性是1/6），那么我们面对的就是一个概率问题；当随机过程中的参数需要被估计（由数据估计随机过程参数），那么我们面对的就是一个统计问题。如下图所示：</br>\n![](1.png)\n学好统计学离不开对概率的深刻理解，下面先复习相关的部分概率论知识。\n\n## 概率论复习（Recap of Probability）\n\n首先我们要明确，随机变量之间相互独立，且满足相同分布（**Independent Identical Distribution, I.I.D.**）是很多统计学内容的基础（如大数定理、中心极限定理等等），这个前提条件之后不再赘述。\n\n统计学中很常见的操作就是求期望。用数据的期望来表征数据分布真实的平均的依据是什么？我们需要用到大数定理（**Laws of large numbers, LLN**）。\n![LLN](2.png)\n但这个定理并不能告诉我们我们的期望离数据真实的平均有多近（ How large is the deviation? ），这时候我们就需要用到中心极限定理（**Central Limit Theorem, CLT**）。\n![CLT](3.png)\n也就是说，红框中的左式极大概率的分布会在[-3, 3]之内（由标准正态分布决定，常数3可以依据对精度的要求自行决定），即：\n$$\n|bar(X_n)-\\mu| <= 3\\sigma/sqrt(n), with high Probability.\n$$\n所以依据中心极限定理，我们就能知道对于特定的样本大小 $n$ （$n$ 要足够大，常见如 $n>=30$），我们得到的期望偏离真正的平均的程度。</br>\n\n而当 $n$ 不够大，CLT无法适用时，我们则需要用到霍夫丁不等式（**Hoeffding's inequality**）（这也是机器学习研究中很重要的理论基础）。使用霍夫丁不等式的前提条件是样本属于某一确定范围，即 $X \\in [a, b]$ 。\n![Hoeffding's inequality](4.png)\n也就是说，在不对样本数做要求情况下，我们可以知道满足某一特定精度要求 $\\varepsilon$ 的（分布于某一确定范围内的）样本出现的概率。\n此外要注意的是，相对于CLT，Hoeffding's inequality会显得更加保守，也没有那么精确。举例来说，相同样本下，CLT估计出的偏差小于某一值的概率（如5%）要小于Hoeffding's inequality估计的结果（如35%）。这也是为什么多数情况下大家不用其来进行统计陈述的原因。\n\n回到中心极限定理CLT，它告诉我们的是当数据量足够大时，各种分布的数据的期望最终都会趋近于在真实平均值附近的一个高斯分布，这就显得高斯分布极其重要。\n![Gaussian Density](5-GD.png)\n从概率角度看，高斯概率密度是一个钟形（bell shape）分布，且其下方与x轴包含的面积必然为1（包含了所有可能性），所以当高斯概率密度分布的方差变大，则其最大值必然变小（如上图中的红线之于蓝线）。而当样本取值趋向于负无穷或正无穷，高斯分布的概率会趋向于0（但不等于0，所以不做近似的话是不能应用Hoeffding's inequality的）。而且高斯概率分布的累积分布函数（Cumulative Distribution Function， CDF）是无法解析表示的，其计算需要依赖计算器计算方法。</br>\n上面是高斯分布的简要描述，它还有什么有用的特性呢？假设$X~\\mathnormal{N}(\\mu, \\sigma^2)$，有：\n- 仿射变换（Affine Transformation）下的不变性。即对于任意实数$a, b$，有：\n$$\na·X+b ~ \\mathnormal{N}(a·\\mu+b,a^2·\\sigma^2)\n$$\n也就是说，对于一个高斯分布进行仿射变换，其结果仍然服从高斯分布。\n- 标准化/归一化/Z-score。即：\n$$\nZ = (X-\\mu)/\\sigma ~ \\mathnormal{N}(0,1)\n$$\n这一性质可方便我们进行查表（一般只有标准高斯分布的数值表）。\n- 对称性。由上面的图可以显然得到。\n\n概率的内容就先复习这么多，具体可以参见另外对概率论进行比较详尽讨论的文章。接下来我们开始进入正题：统计。\n\n## 推测基础（Foundation of Inference）\n\n首先明确统计推测的最终目标是确定观测数据的真实分布，具体而言，包含三个陈述：首先是估计（**Estimation**），其次是确定置信区间（**Confidence Intervals**），最后是假设检验（**Hypothesis Testing**）。\n为了完成上面三个统计推测的陈述，首先我们要进行一次统计建模。那么首先，怎么描述一个统计模型？如下所示：\n![](6-SM.png)\n其中， $E$ 表示观测数据所在的样本空间（也是观测数据背后真实分布所有可能产生的结果）， $\\left(\\mathbb{P}_{\\theta}\\right)_{\\theta \\in \\Theta}$ 是一组基于 $E$ 的概率分布，而 $\\Theta$ 则是统计模型的参数集。为了不过于抽象，下面举一个伯努利分布的实例（0-1分布）如下：\n$$\n\\left(\\{0,1\\},(\\operatorname{Ber}(p))_{p \\in(0.2,0.4)}\\right)\n$$\n在这个实例中，伯努利分布的参数$p$被限定在 $(0.2,0.4)$ 之间。</br>\n\n上面其实只是统计模型的一种类型，那么统计模型又可以怎么分类呢？首先是如上的参数模型（**Parametric Model**），其假设观测数据服从某种真实的分布（参数数量有限，如高斯分布、泊松分布、伯努利分布等等）。而另一种则就是非参数模型（**Nonparametric Model**）了，在这种模型里我们知道数据的分布存在，但不对数据分布进行可有限参数化描述的假设（或者说，数据分布的参数有无限个），下面是一个非参数模型的例子：\n![](7-NP.png)\n\n## 参数估计与置信区间（Parametric Estimation and Confidence Intervals）\n\n在统计学中，基于观测数据计算某个估计值（如平均值、方差或其他形式）的函数，我们称为估计量（**Estimator**）。但随之而来有一个很重要的问题，我们由此得到的估计值与其对应真实值的差异有多大？这就引入了偏差（**Bias**）的概念：\n$$\n\\operatorname{bias}\\left(\\hat{\\theta}_{n}\\right)=\\mathbb{E}\\left[\\hat{\\theta}_{n}\\right]-\\theta\n$$\n其中， $\\mathbb{E}\\left[\\hat{\\theta}_{n}\\right]$ 代表我们的估计值，而 $\\theta$ 代表着真实值。如果偏差为0，那么我们就说我们的估计无偏差（**Unbiased**）。但无偏差估计是否就是最好的？这边还要注意的是，这里的无偏差侧重点在均值上，也就是说，我们的估计值尽管均值与真实值没偏差，但其方差可能很大。\n","tags":["基本数学"]},{"title":"线性代数中的几何 (Geometry in Linear Algebra)","url":"/2022/02/02/linear-algebra/","content":"\n## 0. 前置知识\n理解 $R^n$ （n维向量空间），增广矩阵（augmented matrix），主元（pivot），自由变量（free\n variable），维度（Dimension），标量（scalar）与向量（vector）的区别，知道向量、矩阵运算规则，大致知道子空间（Subspace）、基（Basis）、标准基（standard basis）、可逆矩阵（invertible matrix）等概念。其实最好就是上过照本宣科的线代课，对基本概念都有一些了解，想要从几何角度来加深对线代的理解，就可以看看这篇博客。<br>\n\n## 1. $R^n$ 中的向量（**Vector**）\n首先从最基本的概念讲起。一个n维向量在几何上有两种解释：一种是它是n维空间里的一个点（point），另一种则是向量（vector），如下图所示。<br>\n![](la-1.png)<br>\n在之后的描述中，除非特别声明，否则默认向量起始于原点。但需要注意的是，这只是为了描述的方便，向量在可以在空间中任意位置，它并不一定要以原点为起点。换句话说，一个向量只由它的长度和方向决定，与其位置无关。<br>\n当然，向量也可以表示两个点之间的距离，如下图所示。<br>\n![](la-2.png)<br>\n\n\n## 2. 向量的运算及其几何表示\n向量的相加减结果可由向量间各个维度数值各自相加减得到，如下图所示。<br>\n![](la-3.png)<br>\n而标量与向量的乘法结果可由标量与向量各个维度数值相乘得到，如下图所示。<br>\n![](la-4.png)<br>\n所以，从几何角度上看，向量间的线性组合，由各个向量各自的缩放（与标量相乘），再将它们相加得到。\n而向量的线性组合又能反映代数上的什么东西呢？假设有这么一个方程（向量方程，即由向量构成的方程，等价于一个线性代数方程）：<br>\n![](la-5.png)<br>\n那么我们所要求的解（ $x$ 与 $y$ ），是不是就是两个向量的缩放因子？换句话说，一个向量方程（线性代数方程）有解即意味着等号右边的向量是等号左边向量们的线性组合，而解就是线性组合中的缩放因子。<br>\n\n\n## 3. 生成空间 （**Span**）\n生成空间是一个很重要的概念。简单来说，对于一组向量，以他们为基能张成（线性组合而成）的空间就称为它们的生成空间，可记为：<br>\n![](la-6.png)<br>\n其中 $x_1$ 到 $x_k$ 为标量（缩放因子），**$v_1$** 到 **$v_k$** 为基向量。<br>\n以在 $R^3$ 中为例，一个向量张成的生成空间为一条一维直线，两个不共线的向量张成的生成空间为一个二维平面，三个不共线且不都在一个平面上的向量张成的生成空间则为一个三维空间，而三个不共线但在一个平面内的向量张成的生成空间则为一个二维平面，如下图所示。<br>\n![](la-7.png)<br>\n结合上面线性组合相关的讨论，我们又可知道由一组基向量以及它们的生成空间中的任意向量组成的向量方程（代表着一个线性代数方程）有解。<br>\n\n## 4. 矩阵方程 （Matrix Equations）\n首先回忆一下，一个矩阵 **$A$** 乘上一个向量 **$x$** ，可以视为矩阵 **$A$** 中各向量与向量 **$x$** 中各元素的线性组合，如下公式所示。<br>\n![](la-8.png)<br>\n其中 $x_1$ 到 $x_n$ 为标量（缩放因子），**$v_1$** 到 **$v_n$** 为向量。所以显然，向量 **$x$** 中元素的数目应与矩阵 **$A$** 中向量的数目相等，即对于 **$Ax = b$** 而言，矩阵 **$A$** 的列数应与 **$x$** 维度一致。而这个 **$Ax = b$** 就被定义为**矩阵方程** （**Matrix Equation**)，其中向量 **$x$** 中各元素大小未知（想想与向量方程的关系？是不是两者是等价且可以相互转换的？）。这也是线性系统的另一种表达方式。<br>\n总结一下，目前为止，结合代数中的内容，我们就有四种方式来思考一个线性系统：<br>\n1. 线性代数方程组（代数角度）；<br>\n2. 增广矩阵（代数角度）；<br>\n3. 向量方程（几何角度）；<br>\n4. 矩阵方程（几何角度）。<br>\n\n上面我们解释了矩阵方程与向量线性组合的关系，再回头看我们之前讨论的线性组合与生成空间的关系，是不是发现又可以串起来了？也就是说，当且仅当向量 **$b$** 在矩阵 **$A$** 中各向量张成的生成空间里，\n **$Ax = b$** 有解。其实这就搭起了代数中方程有解与几何中生成空间的桥梁。下面给一个直观的例子。<br>\n假如我们要求 **$Ax = b$** 是否有解？对于：\n $$\n \\mathbf{A}  = \\begin{pmatrix}\n 2 & 1\\\\\n -1 & 0\\\\\n 1 & -1\n\\end{pmatrix},\n\\mathbf{b} = \\begin{pmatrix}\n 0\\\\\n 2\\\\\n2\n\\end{pmatrix}\n$$\n由上面我们的讨论可知，当且仅当向量 **$b$** 在矩阵 **$A$** 中各向量张成的生成空间里，\n **$Ax = b$** 有解。所以在下图中我们将 **$A$** 中各向量张成的生成空间（紫色）与  **$b$** （黑色箭头）画出，可见 **$b$** 并不在 **$A$** 中各向量张成的生成空间中，所以方程无解。<br>\n ![](la-46.png)<br>\n接下来我们考虑 **$Ax = b$** 有解的情况，怎么从几何上去表示其解呢？在回答这个问题之前，我们先考虑一个简单一点的例子：如果 **$b$** 为零向量时解是什么呢？考虑下面的情况：<br>\n假设我们要求 **$Ax = 0$** 的解（这类方程至少有一个平凡解(trivial solution) **$x=0$** ），对于：\n$$\n \\mathbf{A}  = \\begin{pmatrix}\n 1 & -1 & 2\\\\\n -2 & 2 & -4\n\\end{pmatrix}\n$$\n我们用参数化的形式表示它的解，如下所示：<br>\n![](la-9.png)<br>\n显然我们可以将其转换为向量方程的形式：<br>\n![](la-10.png)<br>\n所以显然，方程的解为上述两个向量张成的生成空间：<br>\n![](la-11.png)<br>\n将其可视化如下图所示：\n![](la-12.png)<br>\n所以从几何角度上看，方程 **$Ax = 0$** 的解可以表示为某个/组向量张成的生成空间，注意区分好与上文中对于 **$Ax = b$** 讨论的区别（“解是一个生成空间（解是什么）”与“解在生成空间中（有解的条件）”的区别）。顺带回顾一下变量数目与空间维度的关系：在这个例子中，一共有三个变量（构成 **$x$** ），那么方程的解集必在 $R^3$ 或其子空间，而这个例子中只有两个自由变量（free variable），则解集是在一个二维平面上（自由变量的数目等于解集空间的维度）。<br>\n\n接着回到我们之前的问题：假设 **$Ax = b$** 有解，那么它的解如何在几何上表示？它与 **$b$** 为零向量时有什么不同呢？考虑下面情况：<br>\n$$\n \\mathbf{A}  = \\begin{pmatrix}\n 1 & -1 & 2\\\\\n -2 & 2 & -4\n\\end{pmatrix}，\n\\mathbf{B} = \\begin{pmatrix}\n 1\\\\\n -2\n\\end{pmatrix}\n$$\n类似地，我们可以得到其解为：<br>\n![](la-13.png)<br>\n对比这个结果与上面 **$b$** 为零向量时的结果，显然可以看出，两者相差的只是一个平移( $x_2$ 与 $x_3$ 为0时的特解) **$p$** ，如下图所示：<br>\n![](la-14.png)<br>\n当然，这里举的例子都是方程不是有唯一解的情况，而如果方程只有唯一解，其解在空间上表示就只是一个点了（此时 **$Ax = b$** 与 **$Ax = 0$** 相差的依然只是一个平移）。<br>\n至此，我们在几何角度上又从两个方面去描述一个矩阵方程（ **$Ax = b$** ），注意两者的区别：<br>\n1. **$Ax = b$** 有解的条件（**$b$** 在矩阵 **$A$** 中列向量张成的生成空间中，为 **$x$** 选 **$b$**）；<br>\n2. 其解集空间（对于确定的 **$b$**，为 **$b$** 选 **$x$**）。\n\n\n## 5. 线性独立（Linear Independence）\n首先回忆一下线性独立的定义：如果一组向量 **$v_1$** 到 **$v_k$** ，对于下面的方程有且仅有一个平凡解（$x_1=x_2=...=x_k=0$），则称它们彼此 **线性独立**。<br>\n![](la-15.png)<br>\n换句话说，就是这组向量中，没有任何一个向量可以由其他向量线性组合得到。下图给出两个 **线性依赖（不独立）** 的例子：<br>\n![](la-16.png)<br>\n也就是说，从几何角度上看，如果一个向量集合中，有一个向量在其他向量张成的生成空间中，那么这组向量就线性依赖（不独立）；而如果其中任何一个向量都不在其他向量张成的生成空间中，那么这组向量就线性独立。再换个角度想，我们将这些向量一个一个加入向量组中，每加入一个向量组张成的生成空间都增大，那么这组向量就是线性独立的。下面再可视化给出一个线性独立的例子：<br>\n![](la-17.png)<br>\n而从代数角度上看，矩阵 **$A$** 中各列向量线性独立的条件是化简后每一列都有一个主元（也就是说对于一个列数目大于行数目的矩阵，它必然有线性依赖的列，因为没法做到化简后每一列都有一个主元）。<br>\n\n## 6. 子空间、列空间、零空间与基（Subspace, Column Space, Null Space and Basis）\n什么是**子空间**（**Subspace**）？简单地说，子空间就是满足下面三个条件的 **$R^n$** 里的一些点构成的子集（**Subset**）：<br>\n1. 加法下的封闭性：该子空间中的两个向量相加，结果仍在该子空间中；<br>\n2. 标量乘法的封闭性：该子空间中的向量乘上一个标量，其结果仍在该子空间中；<br>\n3. 非空性：零向量在这个子空间中（子空间要存在，势必要包含了零向量）。<br>\n\n\n所以显然，由上面的前两个性质，某一子空间中的向量的线性组合（它们的生成空间）结果仍在该子空间中；且由第三个性质，子空间必然会经过原点。换句话说，子空间本身就是一个生成空间，它包含了它之中任何向量张成的生成空间。进一步讲，其本身就是其最大的一个子空间。也就说，随着选中的某一子空间中的向量数目变多，这些向量张成的生成空间也会逐步填满这个子空间。下图给出两个子空间的例子（一维与二维）：<br>\n![](la-18.png)<br>\n其中黑色箭头表示用于张成生成空间的向量（基向量，下面会进一步讨论），黑点表示原点。下面再给出一些**非**子空间的例子：<br>\n![](la-19.png)<br>\n其中紫色区域表示定义的“空间”（注意并不是子空间），黑色箭头表示“空间”上的向量，红色箭头表示由“空间”中向量线性组合得到的不在“空间”中的向量。<br>\n另外注意区别空间子集（subset）与子空间（subspace）的区别：子空间是一个需要满足上述三个条件的空间子集，也就是空间子集是一个更大的概念。<br>\n接下来我们讨论两种重要的子空间：**列空间**（由矩阵 **$A$** 中各列向量张成的子空间/生成空间，记作Col( **$A$** )）与**零空间**（对矩阵 **$A$**，满足 **$Ax=0$** 的解构成的子空间/生成空间，记作Null( **$A$** )）。下面如下矩阵 **$A$** 举例：<br>\n$$\n \\mathbf{A}  = \\begin{pmatrix}\n 1 & 1 \\\\\n 1 & 1 \\\\\n 1 & 1\n\\end{pmatrix}\n$$\n它的列空间可以表示为：<br>\n![](la-20.png)<br>\n它的零空间可以表示为：<br>\n![](la-21.png)<br>\n留意列空间是一条在三维（维度大小与矩阵的行数目相同）空间中的一维（等于矩阵化简后主元的数目）线，零空间是一条在二维（维度大小与矩阵的列数目相同）空间中的一维（等于矩阵化简后自由变量的数目）线。这边又隐含了一个重要的定理：矩阵的列空间的维度与零空间的维度之和为矩阵列的数目，在这个例子中为 $1+1=2$ 。<br>\n上面讲完子空间/生成空间，接下来我们讨论子空间/生成空间中的基。为什么需要基的概念，我想最朴素的原因就是数学家们想用最少数量的向量来表征一个生成空间，而基于这个想法，基向量之间就必须相互独立，不然就会有冗余的向量（可以由其他向量线性组合得到）。而对于同一个非零生成空间，它可以有无数组基（比如一个平面，其上任意两个不共线的向量构成它的一组基），但基向量的数目是确定的（由生成空间的维度决定，这其实也是维度的定义）。下图给出了 $R^2$ 的两组基：<br>\n![](la-22.png)<br>\n其中左图是一组标准基。<br>\n那么如何确定列空间的基呢？对于某个矩阵，它的主元（pivot）所在列就构成了它的一组基，如下举例所示，其中RREF（Reduced row echelon form）表示简化列阶梯形矩阵：<br>\n![](la-23.png)<br>\n也就是说，列空间的基向量的数目（维度）等于矩阵主元的数目。<br>\n而如何确定零空间的基呢？将零空间以参数化向量方程表示出来，也可以很轻易得到它的一组基：<br>\n![](la-24.png)<br>\n类似地，零空间的基向量的数目（维度）等于矩阵自由变量的数目。也就是说，我们从基的角度出发，再次印证了上面给出的一个定理：矩阵的列空间的维度与零空间的维度之和为矩阵列的数目（主元与自由变量数目之和）。下面给出几个图进一步说明这个关系：<br>\n![](la-26.png)<br>\n![](la-27.png)<br>\n而其实矩阵的 **秩（rank）** 代表的就是其列空间的维度，零化度（nullity）代表的就是其零空间的维度。也就是说，一个矩阵的秩加上其零化度，等于其列向量的个数（$rank(\\mathbf{A})+nullity(\\mathbf{A})=n$，$n$ 为矩阵 $\\mathbf{A}$ 的列数目）。再结合之前的讨论，换一个说法就是，一个矩阵的主元的个数加上其解集的维度（自由变量的个数），等于其拥有的变量的总个数。其实这个关系也反映了我们在选择 **$Ax = b$** 中的 **$x$** 与 **$b$** 的平衡：当我们拥有的选择 **$x$** 的自由越多，那么我们拥有的选择  **$b$** 的自由就越少，而这个关系被矩阵 **$A$** 中列数目所限定。<br>\n而很自然地，当基确定之后，我们就可以基于基构建一个新的坐标系。在笛卡尔坐标系下，$u_1$ ~ $u_4$ 的坐标分别为：\n$$\nu_1 = [3, -1, 0], u_2 = [-3/2, 1, -3/2], u_3 = [5/2, -3/2, 2], u_4 = [3/2, 0, -3/2]\n$$\n现在考虑用 **$v_1, v_2$** 来表示它们，如下图所示：<br>\n![](la-25.png)<br>\n我们就可以得到以 **$v_1, v_2$** 为基的新坐标系下$u_1$ ~ $u_4$  的坐标：<br>\n$$\nu_{1\\beta} = [1, 1]， u_{2\\beta} = [-1, 1/2], u_{3\\beta} = [3/2, -1/2], u_{4\\beta} = [0, 3/2]\n$$\n由此，相同的几个点在另一个空间中被表示出来，这就完成了一次空间变换。当然，能这么表示的前提是 $u_1$ ~ $u_4$  几个点刚好在 **$v_1, v_2$** 的生成空间中。\n\n\n## 7. 矩阵变换（Matrix Transformations）\n对于一个$m$行$n$列的矩阵 **$A$** ，其有 **$b=Ax$** 的关系，我们可以将其视为一个变换（Transformation），将自变量 **$x$** (维度为$n$的向量)，变换为因变量 **$b$** (维度为$m$的向量)。顺便再提一下， **$b$** 在矩阵 **$A$** 的列空间中。下图给出一个例子，其中绿色箭头为 **$x$** ，红色箭头为 **$b$** ，中间的矩阵即为 **$A$** ，紫色直线为矩阵 **$A$** 的列空间，也就是说矩阵 **$A$** 将一个三维空间（定义域，domain）中的向量 **$x$** 与其列向量进行线性组合，将其变换到一个二维平面（取值空间，codomain）上，但在这个二维平面上 **$A$** 的列空间只是一条一维直线（值域，range，其实也就是矩阵 **$A$** 的列空间），上面代表着 **$b$** 能分布的空间。<br>\n![](la-28.png)<br>\n这边再给出几个矩阵背后代表的几何意义的例子，最好自己先思考再看之后的答案以加深理解：<br>\n如果要将三维空间的点( **$x$** )投射（projection）到x-y平面上( **$b$** )，那矩阵 **$A$** 应该是什么样子的呢？<br>\n![](la-29.png)<br>\n那如果要使一个二维平面上的点关于y轴对称呢（Reflection）？<br>\n![](la-30.png)<br>\n而如果要使一个二维平面上的点保持不变呢（Identity）？<br>\n![](la-31.png)<br>\n这也是单位矩阵（Identity Matrix）名字的由来。那如果要将向量进行缩放（Dilation），是不是只需要对单位矩阵乘上一个标量？<br>\n接着试着回答一个稍微难一点的问题：如果要将二维平面上的点逆时针旋转90度呢（Rotation）？<br>\n![](la-32.png)<br>\n留意到这里矩阵 **$A$** 的列向量（[0,1], [-1,0]）是不是刚好是二维笛卡尔坐标系下基向量（[1,0],[0,1]）逆时针旋转90度的结果呢？<br>\n而如果要实现下图所示的在x轴上的错切（shear in the x-direction），矩阵 ( **$A$** )应该是什么样的？<br>\n![](la-33.png)<br>\n可以尝试下面这三个矩阵代表的几何变换分别是什么。<br>\n$$\n \\mathbf{A_1}  = \\begin{pmatrix}\n 1 & 1 \\\\\n 0 & 1\n\\end{pmatrix},\n \\mathbf{A_2}  = \\begin{pmatrix}\n 1 & 0 \\\\\n 1 & 1\n\\end{pmatrix},\n \\mathbf{A_3}  = \\begin{pmatrix}\n 1 & 1 \\\\\n 1 & 1\n\\end{pmatrix}\n$$\n（答案是第一个矩阵代表着上面的变换。）<br>\n对于某个矩阵变换，我们还需要留意，其对应的是一个一对一（one-to-one）的变换（换个角度就是，矩阵方程有唯一解或无解/ **$Ax=0$** 只有一个平凡解/ 矩阵 **$A$** 的列向量都线性独立/ 矩阵 **$A$** 的每一列都有主元/ 矩阵 **$A$** 的列空间（值域）与定义域维度一致），还是一个多对一的变换，注意没有一对多的变换。下面给出几个图解例子：<br>\n首先是一个一对一的矩阵变换例子：<br>\n![](la-34.png)<br>\n接着看一个多对一的矩阵变换的例子：<br>\n![](la-35.png)<br>\n这个图中的矩阵 **$A$** 代表着将三维空间中的点投影到x-y平面上，也就是说，在每一条跟z轴平行的线上的点（ **$x$** ）都对应着同样的 **$b$** 。<br>\n接下来给出一个稍微复杂一点的例子：<br>\n ![](la-36.png)<br>\n怎么理解这个结果？首先这是一个$R^3$到$R^2$的投射，维度的损失带来的就是多对一的投射（在这个例子中，一条直线（由矩阵的零空间维度决定）投射到一个点）；其次，我们之前讨论过， **$Ax=b$** 的解可以视为 **$Ax=0$** 解（零空间）的平移，在这个例子中， **$Ax=0$** 的解为一条直线（ $x=z$ 与 $y=-z$ 两个平面的交界线，图中紫色虚线），而 **$Ax=b$** 的解即为其平移后的直线（紫色实线），绿色向量即为其平移的大小。<br>\n其实也可以看出，如果一个矩阵的列数n多于行数m，这就意味着把一个高维空间的向量投影到一个低维空间（代数角度可以是矩阵没办法每一列都拥有主元，即列向量们线性依赖），这种情况下矩阵的零空间必然不只是零向量，进而 **$Ax=b$** 的解也不只是一个向量，所以此时矩阵就无法实现一对一的投射。<br>\n顺带一提，像最后这个例子，矩阵 **$A$** 的列空间（值域）与其取值空间一致（从代数角度其实就是，每一行都有主元，不然有的维度就被丢掉了），这种变换又叫onto变换（不知道怎么准确翻译...）。类似地，如果一个矩阵的列数n少于行数m，那么其取值空间维度（每一列向量含有的变量数目，即行数m）必然大于列空间（n个向量至多只能张成一个n维空间，$n<m$）维度。<br>\n对于一般的矩阵，上述两种变换，可以同时满足，也可以同时不满足，也可以只满足其中一个。但对于方阵而言，上述两种变换，必然同时满足，或同时不满足。为什么？对方阵而言，是否每一列都有主元（一对一变换成立）就意味着每一行都有主元（onto变换成立）？反之亦然。<br>\n接下来讨论一个问题：矩阵变换与线性变换（Linear Transformations）有什么关系？首先回忆下线性变换的定义，如果一个变换下面两个性质，则称其为线性变换：<br>\n$$\nT(x+y)=T(x)+T(y),\nT(cx)=cT(x)\n$$\n显然，每一个矩阵变换都能满足上述性质，也就是说每一个矩阵变换背后都代表着线性变换；而反之，每一个线性变换也都可以用矩阵变换来表示。换句话说，矩阵变换与线性变换是完全等同的。（顺带一提，留意像 $y=x+1$ 这种两个变量之间虽然是线性关系，但其变换并不是线性变换，不要混淆线性关系与线性变换的概念。其实正是这个表达式中的常数项破坏了其线性变换。）<br>\n也就是说，一个矩阵可以用下面的形式表示，其中$T$代表着某种线性变换，**$e_1, e_2, ..., e_n$** 代表着$R^n$的标准基向量。<br>\n![](la-37.png)<br>\n怎么理解呢？矩阵 **$A$** 的每一列代表着对 $R^n$ 空间中每一个维度进行的线性变换。有点抽象，举个例子好了：<br>\n假如现在要构造一个矩阵，其代表着将$R^3$中的点关于xy平面进行对称映射，再将其投影到yz平面上，那么这个矩阵应该是什么？<br>\n首先确定矩阵的第一列，也就是对$R^3$中的 **$e_1$** 进行操作，其在经过上述操作后，会落到零点，所以：<br>\n![](la-38.png)<br>\n接着确定矩阵的第二列，也就是对$R^3$中的 **$e_2$** 进行操作，其在经过上述操作后，会保持不变，所以：<br>\n![](la-39.png)<br>\n接着确定矩阵最后一列（变换后的空间仍是$R^3$），也就是对$R^3$中的 **$e_3$** 进行操作，其在经过上述操作后，其方向会取反，所以：<br>\n![](la-40.png)<br>\n所以最后确定下来矩阵 **$A$** 应该为：<br>\n![](la-41.png)<br>\n\n## 8. 矩阵乘法（Matrix Multiplication）\n首先强调一下矩阵乘法（ **$AB$** ）中， 矩阵 **$B$** 的行数必须与矩阵 **$A$** 的列数一致。怎么理解呢？矩阵乘法可以理解为前一个矩阵 **$A$** 对后一个矩阵 **$B$** 的各列向量进行空间变换，如下所示：<br>\n![](la-42.png)<br>\n假设 **$A$** 的维度为 $m$ x $n$，那么其代表着一个从 $R^n$ 到 $R^m$ 的空间变换，那么必然 **$B$** 中列向量的维度也必须为 $n$，也就是矩阵 **$B$** 的行数必须为 $n$。<br>\n同样重要的是，一个 $m$ x $n$ 的矩阵 **$A$** 乘上一个  $n$ x $p$ 的矩阵 **$B$**，其结果维度为 $m$ x $p$。这个又怎么理解呢？同样还是矩阵乘法可以理解为前一个矩阵 **$A$** 对后一个矩阵 **$B$** 的各列向量进行空间变换，而前者代表着一个从 $R^n$ 到 $R^m$ 的空间变换，也就是变换后的列向量维度为 $m$，即 **$AB$** 结果的行数为 $m$；而原来矩阵 **$B$** 有 $p$ 个列向量，对他们分别进行矩阵变换并不会改变他们的数量，也就是说， **$AB$** 结果的列数为 $p$。<br>\n类似地，基于矩阵乘法可以理解为前一个矩阵对后一个矩阵的各列向量进行空间变换，我们也可以理解矩阵乘法不满足交换律（对不同的向量进行不同的变换，他们相等的条件是不是很苛刻？），仅在少数情况下 **$AB$ = $BA$** （但当方阵 **$A$** 与 **$B$** 有 **$AB=I$** 时，直接会有 **$BA=I=AB$**）。<br>\n而因为矩阵可以代表着多对一的变换，由 **$AB=AC$** 并不能得到 **$B=C$**，比如下面的例子：<br>\n![](la-43.png)<br>\n上面是理解矩阵乘法的一个角度，也就是把第二个矩阵当成运算对象。而从另一个角度出发，之前我们提过，一个矩阵代表着一种线性变换，那么显然，两个矩阵相乘其实代表着做完一种线性变换之后接着再做另一种线性变换（链式线性变换）。<br>\n\n## 9. 矩阵的逆（Matrix Inverses）\n首先明确，我们只针对**方阵**去讨论可逆的性质（此章节的矩阵都特指方阵）。怎么判断矩阵是否可逆？一种方式是可以通过其行列式（determinant）是否为0来判断，为0时矩阵不可逆（后面再进一步讨论矩阵行列式）。另一种方式是将增广矩阵 **$(A|I_n)$** 变换为 **$(I_n|B)$**，那么矩阵 **$B=A^{-1}$**，即 **$B$** 为 **$A$** 的逆（这里隐含的条件是方阵 **$A$** 必须满秩）。 而矩阵的逆是用来求解线性方程的一种很方便的方式（**$x=A^{-1}b$**）。<br>\n以上都是一些代数角度的讨论，接下来我们进行一些更几何化的补充。前面我们说了，一个矩阵代表着一种线性变换，那么逆矩阵的作用就是“撤销”这个线性变换。这其实也很好理解，**$A^{-1}Ax=I_nx=x$**，首先 **$A$** 对 **$x$** 进行了一个线性变换，然后 **$A^{-1}$** 又撤销了这个变换（做了一个反向变换），结果就是 **$x$** 经过这两个线性变换之后仍是其自身。下面给几个例子：<br>\n比如如果一个矩阵 **$U$** 代表着缩小向量长度为$1/n$，那么其逆矩阵 **$T$** 就代表着放大向量长度至$n$倍：<br>\n![](la-44.png)<br>\n再比如一个矩阵 **$T$** 代表着逆时针旋转45°，那么其逆矩阵 **$U$** 就代表着顺时针旋转45°：<br>\n![](la-45.png)<br>\n那又如何理解矩阵不可逆呢？考虑下面这个例子：矩阵 **$A$** 代表着将三维空间中的点投影到xy平面，即: <br>\n![](la-35.png)<br>\n显然这里的矩阵 **$A$** 并不满秩，也就是说它代表着一个多对一的变换，即在它对矩阵进行线性变换之后，我们就丢掉了在变换之前的信息（我们只能知道哪些点会投影到变换后的点上，但确定不了是哪个点），而上面我们说了，矩阵的逆代表着反向变换，但这个时候矩阵已经不知道应该反到哪个点上了，所以这个矩阵就不可逆了。也就是说，一个方阵只有代表着一对一变换（因为是方阵也代表着onto变换），其才可逆。<br>\n\n结合之前的内容回顾总结一下，一个 $n$ x $n$ 方阵 **$A$** 可逆，其与下列表述等价（它们之间也等价）：<br>\n1.  **$A$** 拥有 $n$ 个主元；<br>\n2.  **$A$** 的简化列阶梯形矩阵（RREF）是单位矩阵 **$I_n$** ；<br>\n3.  **$A$** 的零空间只有{0}；<br>\n4.  **$A$** 的列空间是$R^n$；<br>\n5.  **$A$** 的列向量线性独立；<br>\n6.  **$A$** 的列向量可以张成$R^n$；<br>\n7.  **$A$** 的列向量构成了$R^n$的一组基；<br>\n8.  **$A$** 的秩是$n$（满秩）；<br>\n9.  **$Ax=b$** 对于每个$b$都只有唯一解（且在$R^n$中）；<br>\n10.  **$A$** 代表的变换为一对一变换；<br>\n11.  **$A$** 代表的变换为onto变换。<br>\n\n也就是说，对一个可逆矩阵，上列表述都成立；而对于一个不可逆矩阵，上列表述都不成立。<br>\n\n顺带一提，要证明矩阵 **$A$** 可逆，只需要证明 **$AB=I_n$** 或 **$BA=I_n$** 中的一个就足够。这里给出一个不完全的证明帮助理解：<br>\n       **$AB=I_n$** →  **$A^{-1}AB=A^{-1}I_n$** → **$B=A^{-1}$** → **$BA = A^{-1}A$** → **$BA=I_n$**\n\n## 10. 行列式（Determinant）\n行列式是许多中文教材中非常吓人的一个概念，看起来定义很复杂，很多人又不知道它到底有什么作用，也就失去了学好它的欲望。但通过下面的讨论，我们应该会发现行列式其实是一个很重要也很有用的工具。<br>\n\n首先我们还是给出行列式很吓人的定义。行列式是一个将方阵转换为实数的函数（det:{square matrices} → $R$），其满足下面四个性质：<br>\n1. 对方阵 **$A$** 进行行替换（row replacement）操作并不会改变其行列式 det(**$A$**) 大小；<br>\n2. 对方阵 **$A$** 的某一行进行$c$倍缩放（scaling），其行列式也会缩放相应的倍数$c$；<br>\n3. 对方阵 **$A$** 某两行进行交换（swapping），其行列式会取相反数；<br>\n4. 单位矩阵 **$I_n$** 的行列式等于1。<br>\n\n> 引理：满足上述四个性质的将方阵转换为实数的函数有且仅有一个。进一步说，对于一个方阵，你都会得到一个唯一数值大小的行列式。\n\n\n为什么要给出这么一个复杂的定义，我们可以从两个角度进行理解：代数角度上方便我们对某方阵 **$A$** 的行列式det(**$A$**)进行计算；几何上则更深刻告诉我们这么定义的动机。类似地，在本章节下方的讨论中，矩阵都特指方阵。<br>\n\n代数角度上，由于我们知道单位矩阵的行列式等于1，且满秩的方阵进行一系列的行变换都可以变成单位矩阵，而每一步的行变换对方阵的行列式的影响我们都知道，基于此我们可以很轻松得到一个满秩方阵的行列式。下面举一个例子：<br>\n![](la-47.png)<br>\n首先我们知道最后一步的单位矩阵的行列式等于1，而其是由第四步的矩阵进行行替换操作（$R_1=R_1-4R_2$）得到的，而行替换操作前后的矩阵的行列式保持不变，所以此步操作之前的矩阵的行列式也等于1；而第三步是一个缩放操作（$R_2=R_2÷-7$）得到的，而对矩阵进行缩放同时也会导致其行列式大小的缩放，所以这一步操作之前的矩阵的行列式为$1×(-7)=-7$；以此类推（再考虑行交换操作会使矩阵的行列式取反），我们可以得到原始方阵的行列式为7。<br>\n类似地，基于上述运算，我们能得到一个二维方阵的行列式的计算表达式如下：\n$$\n\\operatorname{det}\\left(\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right)=a d-b c\n$$\n而对于一个阶梯形式方阵，其行列式就是其对角线上元素的乘积（对上/下三角方阵、对角阵都成立）。进一步的，只要一个方阵其某一行或某一列都为0，那么其行列式就为0。更进一步，非满秩的方阵其行列式为0。<br>\n\n记得在教科书中，我们学的行列式的计算方法是一种迭代的运算方式：拉普拉斯展开 / 代数余子式展开（Laplace expansion / cofactor expansion）。在这种计算方式下，我们可以按行展开：<br>\n$$\n\\operatorname{det}(A)=\\sum_{j=1}^{n} a_{i j} C_{i j}=a_{i 1} C_{i 1}+a_{i 2} C_{i 2}+\\cdots+a_{i n} C_{i n}\n$$\n也可以按列展开：<br>\n$$\n\\operatorname{det}(A)=\\sum_{i=1}^{n} a_{i j} C_{i j}=a_{1 j} C_{1 j}+a_{2 j} C_{2 j}+\\cdots+a_{n j} C_{n j}\n$$\n其中，$a_{ij}$是矩阵 **$A$** 第$i$行第$j$列的元素，而$C_{ij}$是矩阵 **$A$** 第$i$行第$j$列的代数余子式（cofactor），其定义如下：<br>\n$$\nC_{i j}=(-1)^{i+j} \\operatorname{det}\\left(A_{i j}\\right)\n$$\n其中$A_{i j}$是矩阵 **$A$** 删去第$i$行第$j$列后剩下的元素组成的维度为 $(n-1) × (n-1)$ 的矩阵。<br>\n顺带一提的是，上述第一种运算的计算复杂度也相对较低，不管对人还是对机器而言（$O(n^3)$），而代数余子式展开计算的计算复杂度为$O(n !) \\approx O\\left(n^{n} \\sqrt{n}\\right)$。那数学家为什么还要开发代数余子式展开这么一套工具，它在什么时候有用呢？第一种情况，如果一个矩阵的某行/列中有很多的零，那么其对应位置的代数余子式就没有计算的意义了，在这种情况下计算可以很大程度被简化；另一种情况，如果一个矩阵里有未知大小的元素，这个时候用上述第一种运算是不太合适的，因为我们并不确定这个未知元素在矩阵约简成阶梯矩阵后是否会是主元。<br>\n\n而在几何角度上，矩阵的行列式与“容积”（volume）大小有着密不可分的关系。行列式计算着平行六面体（paralellepiped）的容积 ，其由 **$R^n$** 中的$n$个向量 $v_1, v_2, ..., v_n$确定，当$n=2$时，其退化为一个平行四边形。下面举几个例子：<br>\n首先二维与三维空间中的标准正交基组成的平行四边形/平行六面体如下图所示：<br>\n![](la-49.png)<br>\n而对于二维与三维空间中的线性独立的任意二/三个向量组成的平行四边形/平行六面体举例如下图所示：<br>\n![](la-50.png)<br>\n那么如果上述组成平行四边形/平行六面体的向量线性依赖呢？这时几何图形的“容积”为0，如下图所示：<br>\n![](la-51.png)<br>\n这些向量线性依赖，则意味着它们组成的矩阵非满秩，而我们上面已经讨论过，非满秩的矩阵的行列式为0，所以这边我们就得到一个很重要的观察：几个向量组成的平行六面体（包含二维空间下的平行四边形，下面不再赘述）的容积为0，当且仅当它们组成的矩阵（作为行向量或列向量）的行列式为0。更进一步地，其实上述平行六面体（记为P）的容积与上述矩阵的行列式（记为 **$A$**）绝对值一直相等：<br>\n$$\n|\\operatorname{det}(A)|=\\operatorname{vol}(P)\n$$\n直觉上怎么理解呢？回顾行列式的定义，我们有：<br>\n1. 对方阵 **$A$** 进行行替换（row replacement）操作并不会改变 |det(**$A$**)| 大小；<br>\n2. 对方阵 **$A$** 的某一行进行$c$倍缩放（scaling），其行列式的绝对值也会缩放相应的倍数|$c$|；<br>\n3. 对方阵 **$A$** 某两行进行交换（swapping），其行列式绝对值保持不变；<br>\n4. 单位矩阵 **$I_n$** 的行列式等于1。<br>\n而平行六面体的容积如何计算？底（base）×高（height），如下图所示：<br>\n![](la-52.png)<br>\n首先，我们考虑对方阵 **$A$** 进行行替换（row replacement）操作对其对应的平行六面体容积的影响：<br>\n![](la-53.png)<br>\n可以发现，行替换操作并不会改变平行六面体的底或高，进而其容积也不会改变。也就是说，一组向量的行替换操作既不会改变其组成的平行六面体的容积，也不会改变其组成的方阵 **$A$** 行列式绝对值的大小。<br>\n其次，我们考虑对方阵 **$A$** 的某一行进行$c$倍缩放（scaling）操作对其对应的平行六面体容积的影响：<br>\n![](la-54.png)<br>\n可以发现，缩放操作会使平行六面体的高也进行相应的缩放，进而其容积也进行相应的缩放。也就是说，对方阵 **$A$** 的某一行进行$c$倍缩放（scaling），其行列式的绝对值和对应的平行六面体的容积也会缩放相应的倍数|$c$|。<br>\n再者，我们考虑对方阵 **$A$** 进行行交换（swapping）操作对其对应的平行六面体容积的影响：<br>\n![](la-55.png)<br>\n显然，除了对应方阵行列式的绝对值，行交换操作也不会改变平行六面体容积的大小。<br>\n最后，同样显然的是，单位矩阵内的向量组成的平行六面体的容积也为1。<br>\n\n现在我们已经知道了一个方阵的行列式绝对值等于其内在向量组成的平行六面体的容积，那进一步我们有没有可能摆脱掉这个烦人的绝对值呢？答案当然是有的，只要我们引入有符号的容积（signed volume，可正可负的容积，就像在微积分中那样有负的面积）。那这个容积的正负性与对应方阵行列式的正负性关系是什么？<br>\n1. 对于一个二维方阵，行列式的正负性告诉我们组成它的向量的方向关系。如果行列式为正，则意味着第二个向量在第一个向量的逆时针方向上（角度小于180°，下同）；反之，则在顺时针方向上，如下图所示：<br>\n![](la-56.png)<br>\n2. 对于一个三维方阵，行列式的正负性则由右手定则（right-hand rule）决定。如果你把食指（index finger）指向第一个向量，中指（middle finger）指向第二个向量，则如果你的拇指（thumb）大致可以指向第三个向量，那么行列式为正；而如果你的拇指大致指向第三个向量的反方向，那么行列式为负，如下图所示：<br>\n![](la-57.png)<br>\n3. 我们已经看到行列式的正负性与有符号容积的正负性有很大的关系，所以在更高维度中，有符号容积的正负性经常由行列式的正负性定义。\n\n回顾我们之前讨论过的，一个矩阵本身代表着一个线性变换。那么其行列式对这个变换有什么意义呢？这里有一个定理：**$A$** 是一个$n × n$的方阵，而$T$代表着其对应的线性变换（$T(x)=\\mathbf{A}x$，$\\mathbf{R^n} → \\mathbf{R^n}$），而S是$\\mathbf{R^n}$中的一个区域，则有（证明参见参考书籍4.3）：<br>\n$$\n\\operatorname{vol}(T(S))=|\\operatorname{det}(A)| \\cdot \\operatorname{vol}(S)\n$$\n这边举一个有趣的例子：用行列式来求解椭圆的面积。我们要求解的椭圆方程为：<br>\n$$\n\\left(\\frac{2 x-y}{2}\\right)^{2}+\\left(\\frac{y+3 x}{3}\\right)^{2}=1\n$$\n它与单位圆（$X^2+Y^2=1$）存在下面的变换关系：<br>\n$$\n\\begin{array}{l}\nX=\\frac{2 x-y}{2} \\\\\nY=\\frac{y+3 x}{3}\n\\end{array}\n$$\n也就是说，我们可以定义一个线性变换$T$，在经过这个线性变换之后，原本在椭圆上的点会变换到单位圆上：<br>\n$$\nT\\left(\\begin{array}{l}\nx \\\\\ny\n\\end{array}\\right)=\\left(\\begin{array}{l}\n(2 x-y) / 2 \\\\\n(y+3 x) / 3\n\\end{array}\\right)\n$$\n也就是如下图所示：<br>\n![](la-58.png)<br>\n而单位圆的面积和线性变换矩阵的行列式我们都易求，再根据上面的公式，我们可以轻易求得椭圆的面积：<br>\n$$\n\\pi=\\operatorname{vol}(C)=\\operatorname{vol}(T(E))=|\\operatorname{det}(A)| \\cdot \\operatorname{vol}(E)=\\frac{5}{6} \\operatorname{vol}(E)\n$$\n所以椭圆的面积为$6 \\pi/5$。\n\n上面讲完了怎么定义并理解行列式这个概念，那么它到底有什么用呢？<br>\n首先考虑行列式与方阵可逆性的关系我们有，当且仅当一个方阵的行列式不为零时，其可逆。通过在章节9中的分析我们知道，一个方阵不可逆，那么其不满秩，也就是其约简到阶梯式后至少会有一行全为0，那么其行列式就为0。将之前对矩阵可逆条件的讨论结合到一起，我们有下述表述都等价 (**$A$** 为方阵)：<br>\n1.  **$A$** 拥有 $n$ 个主元；<br>\n2.  **$A$** 的简化列阶梯形矩阵（RREF）是单位矩阵 **$I_n$** ；<br>\n3.  **$A$** 的零空间只有{0}；<br>\n4.  **$A$** 的列空间是$R^n$；<br>\n5.  **$A$** 的列向量线性独立；<br>\n6.  **$A$** 的列向量可以张成$R^n$；<br>\n7.  **$A$** 的列向量构成了$R^n$的一组基；<br>\n8.  **$A$** 的秩是$n$（满秩）；<br>\n9.  **$Ax=b$** 对于每个$b$都只有唯一解（且在$R^n$中）；<br>\n10.  **$A$** 代表的变换为一对一变换；<br>\n11.  **$A$** 代表的变换为onto变换；<br>\n12.  **$A$** 可逆；<br>\n13.  **$A$** 的行列式不为零。<br>\n\n再者方阵行/列向量线性独立性与其行列式的关系我们有，当一个方阵的行或列向量线性依赖，那么其行列式为0。如果其行向量线性依赖，则意味着我们可以通过行操作得到一个全为零的行，而有全为0的行的方阵的行列式为0，进而又可以得到原始方阵行列式为0；而如果其列向量线性依赖，则由上述等价条件可以直接得到其行列式为0。比如下列的几个方阵的行列式都为0：<br>\n![](la-48.png)<br>\n\n同时行列式也满足可乘性，即：\n$$\n\\operatorname{det}(A B)=\\operatorname{det}(A) \\operatorname{det}(B)\n$$\n其中 **$A$** 与 **$B$** 都为方阵。这个性质可以由上述的行列式唯一性引理证明，详细证明可参见参考书籍。而由这个性质，我们又可以得到，对于方阵 **$A$**， 在$n≥1$条件下有：<br>\n$$\n\\operatorname{det}\\left(A^{n}\\right)=\\operatorname{det}(A)^{n}\n$$\n而如果方阵 **$A$** 可逆，上式在$n≤0$时也成立，特别地，我们有：<br>\n$$\n\\operatorname{det}\\left(A^{-1}\\right)=\\frac{1}{\\operatorname{det}(A)}\n$$\n\n此外，一个方阵 **$A$** 转置后的行列式保持不变：<br>\n$$\n\\operatorname{det}(A)=\\operatorname{det}\\left(A^{T}\\right)\n$$\n由这个性质，我们又可以引申到：<br>\n1. 对方阵 **$A$** 进行列替换（column replacement）操作并不会改变其行列式 det(**$A$**) 大小；<br>\n2. 对方阵 **$A$** 的某一列进行$c$倍缩放（scaling），其行列式也会缩放相应的倍数$c$；<br>\n3. 对方阵 **$A$** 某两列进行交换（swapping），其行列式会取相反数。<br>\n\n这就意味着，我们要求解一个方阵的行列式时，行列的操作是等价的。<br>\n\n## 10. 特征值与特征向量（Eigenvalues and Eigenvectors）\n特征值与特征向量在解决许多现实问题的应用中都扮演了举足轻重的角色，但怎么理解它们？它们为什么重要？他们又有什么用？这是我们接下来尝试解答的问题。<br>\n\n首先让我们回忆一下特征值与特征向量的定义(非常重要！)。对于下式：\n$$\nAv=\\lambda v\n$$\n其中$A$为**方阵**（只对方阵考虑特征值与特征向量，下述矩阵表述均指方阵），$v$为向量，$\\lambda$为标量。如果其有非平凡（非零）解$v$，那么我们称$\\lambda$是特征向量$v$对应的特征值。特征值与特征向量就是在描述矩阵$A$的特征。一个矩阵可以有多个特征值/特征向量，但每个特征值与特征向量一一对应。如果接受$v$为零向量作为特征向量，那么其将有无数个对应的特征值，这并不合适；但特征值为0是可能的，比如下面这个例子：\n$$\nA v=\\left(\\begin{array}{ll}\n1 & 3 \\\\\n2 & 6\n\\end{array}\\right)\\left(\\begin{array}{c}\n-3 \\\\\n1\n\\end{array}\\right)=\\left(\\begin{array}{l}\n0 \\\\\n0\n\\end{array}\\right)=0 v\n$$\n\n几何上来看，上面的定义告诉我们，$Av$、$\\lambda v$与原点共线。也就是说，矩阵$A$对其特征向量$v$进行线性变换后，得到$\\lambda v$，$v$与$\\lambda v$仅仅相差一个缩放因子$\\lambda$（标量，即其对应特征值），如下图所示：<br>\n\n![](la-59.png)<br>\n\n图中$v$为矩阵$A$的特征向量，而$w$不是。<br>\n\n此外，矩阵的特征向量有一个很重要的性质：对应不同特征值的特征向量线性独立。一个证明思路是反证法。具体可参见参考书籍5.1。\n\n上面我们讨论了怎么判断一个给定的向量$v$是否是方阵$A$的特征向量，接着反过来，我们讨论怎么判断一个给定的数值$\\lambda$是否是方阵$A$的特征值，且是的话求其对应的特征向量。对于特征值、特征向量的定义式，我们有：\n$$\n A v=\\lambda v \\Longleftrightarrow \\left(A-\\lambda I_{n}\\right) v=0\n$$\n也就是说，如果上式存在非平凡解，或者说零空间Null($A-\\lambda I_n$)中存在非零向量，那么这些非零解就是矩阵$A$的特征向量；而如果上式不存在非平凡解，则$\\lambda$不是$A$的特征值。上述的零空间（Null($A-\\lambda I_n$)）即为方阵$A$的 **$\\lambda$-特征空间（Eigenspace）**，注意其中也包含了零向量，但零向量不是矩阵$A$的特征向量。进一步我们有推论：一个$n\\times n$矩阵$A$至多拥有$n$个特征值，因为在上述特征空间中至多有$n$个线性独立的非零向量（即特征向量）。但这里也要注意的是，一个特征向量仅对应一个特征值，但一个特征值对应多个与原点共线的特征向量（比如，$v_1$是矩阵$A$的特征向量，则$cv_1$也是矩阵$A$的特征向量，且它们对应的特征值相同），因为在特征空间中一个非零子空间（即特征向量所在的子空间）是无限大的。另外，与之前对零空间的讨论联系起来，一个$A$的特征值对应的特征向量的数量为方程$(A-\\lambda I_{n}) v=0$化简为阶梯式后的自由变量的数量。下面举一个实例：<br>\n$$\nA_1=\n\\left[\\begin{array}{ccc}\n3.50 & 0.00 & 3.00 \\\\\n-1.50 & 2.00 & -3.00 \\\\\n-1.50 & 0.00 & -1.00\n\\end{array}\\right]\n$$\n它有两个特征值：$\\lambda_1=0.5$与$\\lambda_2=2$。$\\lambda_1$-特征空间为下图中的绿色一维空间（一个特征向量），而$\\lambda_2$-特征空间为下图中的紫色二维空间（两个特征向量）。<br>\n![](la-60.png)<br>\n\n而如果考虑0-特征空间，其即为Null($A$)，这时当且仅当Null($A$)不只是{0}，0是矩阵$A$的特征值，也就是说需要矩阵$A$不可逆。回顾之前的可逆矩阵定理，现在我们又可以添加一条等价地表述（14）：\n1.  **$A$** 拥有 $n$ 个主元；<br>\n2.  **$A$** 的简化列阶梯形矩阵（RREF）是单位矩阵 **$I_n$** ；<br>\n3.  **$A$** 的零空间只有{0}；<br>\n4.  **$A$** 的列空间是$R^n$；<br>\n5.  **$A$** 的列向量线性独立；<br>\n6.  **$A$** 的列向量可以张成$R^n$；<br>\n7.  **$A$** 的列向量构成了$R^n$的一组基；<br>\n8.  **$A$** 的秩是$n$（满秩）；<br>\n9.  **$Ax=b$** 对于每个$b$都只有唯一解（且在$R^n$中）；<br>\n10.  **$A$** 代表的变换为一对一变换；<br>\n11.  **$A$** 代表的变换为onto变换；<br>\n12.  **$A$** 可逆；<br>\n13.  **$A$** 的行列式不为零；<br>\n14.  0不是矩阵 **$A$** 的特征值。<br>\n\n\n接下来，我们要考虑的就是怎么直接求解一个矩阵的特征值了，这里我们要用到的一个工具是**特征多项式（Characteristic Polynomial）**。对于$n \\times n$方阵$A$，其特征多项式定义为：\n$$\nf(\\lambda)=\\operatorname{det}\\left(A-\\lambda I_{n}\\right)\n$$\n举个例子，对于矩阵$A=\\left(\\begin{array}{ccc}\n0 & 6 & 8 \\\\\n\\frac{1}{2} & 0 & 0 \\\\\n0 & \\frac{1}{2} & 0\n\\end{array}\\right)$，我们求其特征多项式：\n$$\n\\begin{aligned}\nf(\\lambda)=\\operatorname{det}\\left(A-\\lambda I_3\\right) &=\\operatorname{det}\\left(\\begin{array}{ccc}\n-\\lambda & 6 & 8 \\\\\n\\frac{1}{2} & -\\lambda & 0 \\\\\n0 & \\frac{1}{2} & -\\lambda\n\\end{array}\\right) \\\\\n&=8\\left(\\frac{1}{4}-0 \\cdot-\\lambda\\right)-\\lambda\\left(\\lambda^2-6 \\cdot \\frac{1}{2}\\right) \\\\\n&=-\\lambda^3+3 \\lambda+2 .\n\\end{aligned}\n$$\n可以看出，特征多项式是一个以特征值为自变量的多项式。它有什么用呢？这边我们有一个很漂亮的定理：特征多项式的根就是对应矩阵的特征值（具体证明可参见参考书籍5.2）。也就是说：\n$$\n\\lambda_0 \\text { is an eigenvalue of } A \\Longleftrightarrow f(\\lambda_0)=0\n$$\n对于上面的矩阵$A$，我们就可以按照这个定理求得其特征值：\n$$\nf(\\lambda)=-\\lambda^3+3 \\lambda+2=-(\\lambda-2)(\\lambda+1)^2\n$$\n所以矩阵$A$的特征值为$2,-1$。接着我们依据之前的求特定特征值对应特征向量的方法，可以得到其特征向量：\n$$\nA-2 I_3=\\left(\\begin{array}{ccc}\n-2 & 6 & 8 \\\\\n\\frac{1}{2} & -2 & 0 \\\\\n0 & \\frac{1}{2} & -2\n\\end{array}\\right) \\quad \\stackrel{\\mathrm{RREF}}{\\longrightarrow}\\left(\\begin{array}{ccc}\n1 & 0 & -16 \\\\\n0 & 1 & -4 \\\\\n0 & 0 & 0\n\\end{array}\\right)\n$$\n即：\n$$\n\\left\\{\\begin{array}{l}\nx=16 z \\\\\ny=4 z \\\\\nz=z\n\\end{array} \\longrightarrow\\left(\\begin{array}{l}\nx \\\\\ny \\\\\nz\n\\end{array}\\right)=z\\left(\\begin{array}{c}\n16 \\\\\n4 \\\\\n1\n\\end{array}\\right) .\\right.\n$$\n所以$A$的2-特征空间为：\n$$\n\\operatorname{Span}\\left\\{\\left(\\begin{array}{c}\n16 \\\\\n4 \\\\\n1\n\\end{array}\\right)\\right\\}\n$$\n类似地，我们可以得到$A$的-1-特征空间为：\n$$\n\\operatorname{Span}\\left\\{\\left(\\begin{array}{c}\n4 \\\\\n-2 \\\\\n1\n\\end{array}\\right)\\right\\}\n$$\n至此，我们已经有能力去求某个矩阵的特征值与特征向量了。<br>\n\n接下来，我们考虑一种特殊情况——对角阵。这时候求解特征值非常简单，一个对角阵的特征值即为其对角线上的元素。比如对于矩阵$A=\\left(\\begin{array}{cccc}\n1 & 7 & 2 & 4 \\\\\n0 & 1 & 3 & 11 \\\\\n0 & 0 & \\pi & 101 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right)$，其特征值即为$1, \\pi, 0$。<br>\n\n在一开始介绍特征值特征向量的时候我们说它很重要，那它到底有什么用？接下来我们要尝试回答这个问题。首先我们先介绍 **相似矩阵（Similar Matrices）** 的概念：如果存在一个$n \\times n$ **可逆**矩阵$C$使得：\n$$\nA=CBC^{-1}\n$$\n那么就称$n \\times n$矩阵$A$、$B$为相似矩阵。相似矩阵具有自反性（Reflexivity，矩阵与其自身相似）、对称性（Symmetry，矩阵$A$与$B$相似即矩阵$B$与$A$相似）与传递性（Transitivity，矩阵$A$与$B$相似，矩阵$B$与$C$相似，即矩阵$A$与$C$相似）。<br>\n\n为何相似矩阵重要？在下面的讨论中，我们可以看到，从几何上看，相似矩阵在**不同的坐标系**下执行**相同的操作**。为什么我们要求定义中的$n \\times n$矩阵$C$可逆？回顾我们之前的矩阵可逆定理，我们有$C$中各列可作为基张成$R^n$空间。也就是说我们有：\n$$\nC[x]_{\\mathcal{B}} = I_n x = x \\quad \\text { and } \\quad C^{-1} x=[x]_{\\mathcal{B}}\n$$\n其中$x$表示一个向量在标准正交基（也就是乘上单位矩阵$I_n$）下的坐标，而$[x]_{\\mathcal{B}}$则表示以$C$中各列为基的空间（$\\mathcal{B}$-坐标系）中该向量的坐标。换句话说，$C$将$\\mathcal{B}$-坐标系下的坐标变换为了标准正交坐标系下的坐标，而$C^{-1}$把标准正交坐标系下的坐标变换为了$\\mathcal{B}$-坐标系下的坐标。基于这样的理解，对于相似矩阵$A$、$B$：\n$$\ny = Ax = CBC^{-1}x = C(B(C^{-1}x))\n$$\n我们先计算$C^{-1}x$，也就是将标准正交坐标系下的坐标变换为了$\\mathcal{B}$-坐标系下的坐标；接着对运算结果乘上$B$，也就是在$\\mathcal{B}$-坐标系下进行矩阵运算；最后再在结果上乘上$C$，将运算结果坐标转换为标准正交坐标系下的坐标。如图所示：<br>\n![](la-61.png)<br>\n总结而言，$B$在$\\mathcal{B}$-坐标系下进行与$A$在标准正交坐标系下相同的矩阵变换。下面给一个具体的简单例子。对于：\n$$\nA=\\left(\\begin{array}{cc}\n1 / 2 & 3 / 2 \\\\\n3 / 2 & 1 / 2\n\\end{array}\\right) \\quad B=\\left(\\begin{array}{cc}\n2 & 0 \\\\\n0 & -1\n\\end{array}\\right) \\quad C=\\left(\\begin{array}{cc}\n1 & 1 \\\\\n1 & -1\n\\end{array}\\right)\n$$\n我们有$A = CBC^{-1}$。首先矩阵$B$是一个对角阵，其执行的操作很简单，对第一个维度方向的坐标放大两倍，对第二个维度方向的坐标取反，如图所示:<br>\n![](la-62.png)<br>\n下图中我们以标准正交坐标系下$x=\\left(\\begin{array}{c}\n0 \\\\\n-2\n\\end{array}\\right)$为例（右图绿色），先对其乘上$C^{-1}$得到$\\mathcal{B}$下的坐标$[x]_{\\mathcal{B}}=\\left(\\begin{array}{c}\n-1 \\\\\n1\n\\end{array}\\right)$（左图绿色），接着对 $[x]_\\mathcal{B}$ 执行矩阵变换，即横坐标放大两倍，纵坐标取反，得到 $B[x]_\\mathcal{B}$（左图红色），最后再对其乘上$C$，得到标准正交坐标系下的$Ax$（右图红色）。<br>\n\n而对于相似矩阵，它们的特征值相同、迹（trace）相同、行列式相同（证明可见参考书籍5.3）。一个方阵的迹是其对角线上各元素的累计和：\n$$\n\\operatorname{Tr}\\left(\\begin{array}{ccccc}\na_{11} & a_{12} & \\cdots & a_{1, n-1} & a_{1 n} \\\\\na_{21} & a_{22} & \\cdots & a_{2, n-1} & a_{2 n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\na_{n-1,1} & a_{n-1,2} & \\cdots & a_{n-1, n-1} & a_{n-1, n} \\\\\na_{n 1} & a_{n 2} & \\cdots & a_{n, n-1} & a_{n n}\n\\end{array}\\right)=a_{11}+a_{22}+\\cdots+a_{n n} .\n$$\n而相似矩阵的特征向量之间相差着一个矩阵变换：对于$A=CBC^{-1}$，如果$v$是$A$的一个特征向量则$C^{-1}v$是$B$的一个特征向量，如果$v$是$B$的一个特征向量则$Cv$为$A$的一个特征向量。或者说，对于$A=CBC^{-1}$，$C^{-1}$把矩阵$A$的$\\lambda$-特征空间变换到$B$的$\\lambda$-特征空间；而$C$把矩阵$B$的$\\lambda$-特征空间变换到$A$的$\\lambda$-特征空间。\n\n从上面的例子中，我们可以看到，如果一个矩阵与一个对角阵相似，那么我们就可以比较轻易理解它的空间变换行为。对于这样有趣的矩阵，我们给他一个名字：**可对角化矩阵（diagonalizable matrix）**。很自然地，接下来我们就想问，怎么判断一个矩阵是否可对角化？这里我们就引入对角化定理（Diagonalization Theorem）（证明见参考书籍5.4）：当且仅当$n \\times n$矩阵$A$有$n$个线性独立特征向量时，其可对角化，即对于$A=CDC^{-1}$有：\n$$\nC=\\left(\\begin{array}{cccc}\n\\mid & \\mid & & \\mid \\\\\nv_{1} & v_{2} & \\cdots & v_{n} \\\\\n\\mid & \\mid & & \\mid\n\\end{array}\\right) \\quad D=\\left(\\begin{array}{cccc}\n\\lambda_{1} & 0 & \\cdots & 0 \\\\\n0 & \\lambda_{2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_{n}\n\\end{array}\\right)\n$$\n其中$v_i$为$A$线性独立的特征向量，而$\\lambda_i$为各特征向量对应的特征值，$i \\in [1, n]$。特别地，对于一个$n \\times n$的矩阵，如果它有$n$个不相等的特征值，则其可对角化；但矩阵可对角化，其不一定有$n$个不相等的特征值。留意到对角化并不唯一，只要对应好特征向量与特征值的位置关系：\n$$\n\\begin{aligned}\nA &=\\left(\\begin{array}{ccc}\n\\mid & \\mid & \\mid \\\\\nv_{1} & v_{2} & v_{3} \\\\\n\\mid & \\mid & \\mid\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n\\lambda_{1} & 0 & 0 \\\\\n0 & \\lambda_{2} & 0 \\\\\n0 & 0 & \\lambda_{3}\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n\\mid & \\mid & \\mid \\\\\nv_{1} & v_{2} & v_{3} \\\\\n\\mid & \\mid & \\mid\n\\end{array}\\right)^{-1} \\\\\n&=\\left(\\begin{array}{ccc}\n\\mid & \\mid & \\mid \\\\\nv_{3} & v_{2} & v_{1} \\\\\n\\mid & \\mid & \\mid\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n\\lambda_{3} & 0 & 0 \\\\\n0 & \\lambda_{2} & 0 \\\\\n0 & 0 & \\lambda_{1}\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n\\mid & \\mid & \\mid \\\\\nv_{3} & v_{2} & v_{1} \\\\\n\\mid & \\mid & \\mid\n\\end{array}\\right)^{-1}\n\\end{aligned}\n$$\n或者单纯缩放特征向量，也可以得到另外的对角化方式：\n$$\n\\begin{aligned}\nA &=\\left(\\begin{array}{ccc}\n\\mid & \\mid & \\mid \\\\\nv_{1} & v_{2} & v_{3} \\\\\n\\mid & \\mid & \\mid\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n\\lambda_{1} & 0 & 0 \\\\\n0 & \\lambda_{2} & 0 \\\\\n0 & 0 & \\lambda_{3}\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n\\mid & \\mid & \\mid \\\\\nv_{1} & v_{2} & v_{3} \\\\\n\\mid & \\mid & \\mid\n\\end{array}\\right)^{-1} \\\\\n&=\\left(\\begin{array}{ccc}\n\\mid & \\mid & \\mid \\\\\nc v_{1} & v_{2} & v_{3} \\\\\n\\mid & \\mid & \\mid\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n\\lambda_{1} & 0 & 0 \\\\\n0 & \\lambda_{2} & 0 \\\\\n0 & 0 & \\lambda_{3}\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n\\mid & \\mid & \\mid \\\\\nc v_{1} & v_{2} & v_{3} \\\\\n\\mid & \\mid & \\mid\n\\end{array}\\right)^{-1}\n\\end{aligned}\n$$\n举一个具体的例子，比如对于$A=\\left(\\begin{array}{ll}\n1 / 2 & 3 / 2 \\\\\n3 / 2 & 1 / 2\n\\end{array}\\right)$，我们可以求得其特征值分别为-1、2，对应的特征值为[-1, 1]与[1, 1]，所以我们有：\n$$\nA=C D C^{-1} \\quad \\text { for } \\quad C=\\left(\\begin{array}{cc}\n-1 & 1 \\\\\n1 & 1\n\\end{array}\\right) \\quad D=\\left(\\begin{array}{cc}\n-1 & 0 \\\\\n0 & 2\n\\end{array}\\right)\n$$\n或者\n$$\nA=C^{\\prime} D^{\\prime}\\left(C^{\\prime}\\right)^{-1} \\quad \\text { for } \\quad C^{\\prime}=\\left(\\begin{array}{cc}\n1 & -1 \\\\\n1 & 1\n\\end{array}\\right) \\quad D^{\\prime}=\\left(\\begin{array}{cc}\n2 & 0 \\\\\n0 & -1\n\\end{array}\\right)\n$$\n或者\n$$\nA=C^{\\prime\\prime} D^{\\prime\\prime}\\left(C^{\\prime\\prime}\\right)^{-1} \\quad \\text { for } \\quad C^{\\prime}=\\left(\\begin{array}{cc}\n2 & -1 \\\\\n2 & 1\n\\end{array}\\right) \\quad D^{\\prime}=\\left(\\begin{array}{cc}\n2 & 0 \\\\\n0 & -1\n\\end{array}\\right)\n$$\n另外，留意到当$A$不可逆时，对角阵$D$上会有零。如对于$A=\\left(\\begin{array}{rr}\n2 / 3 & -4 / 3 \\\\\n-2 / 3 & 4 / 3\n\\end{array}\\right) $，我们有：\n$$\nA=C D C^{-1} \\quad \\text { for } \\quad C=\\left(\\begin{array}{cc}\n2 & 1 \\\\\n1 & -1\n\\end{array}\\right) \\quad D=\\left(\\begin{array}{ll}\n0 & 0 \\\\\n0 & 2\n\\end{array}\\right)\n$$\n下面再给出一个矩阵不可对角化的例子。对于$A=\\left(\\begin{array}{lll}\n1 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 2\n\\end{array}\\right)$，其特征值为1、2，其1-特征空间为：\n$$\n\\left(A-I_{3}\\right) v=0 \\Longleftrightarrow\\left(\\begin{array}{lll}\n0 & 1 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 2\n\\end{array}\\right)\\left(\\begin{array}{l}\nx \\\\\ny \\\\\nz\n\\end{array}\\right)=0 \\Longleftrightarrow y=z=0\n$$\n即x坐标轴。其2-特征空间为：\n$$\n\\left(A-2 I_{3}\\right) v=0 \\Longleftrightarrow\\left(\\begin{array}{ccc}\n-1 & 1 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right)\\left(\\begin{array}{l}\nx \\\\\ny \\\\\nz\n\\end{array}\\right)=0 \\Longleftrightarrow x=y=0\n$$\n即z-坐标轴。所以该矩阵线性独立的特征向量只有两个（$<n=3$），所以矩阵不可对角化。另外，矩阵是否可对角化与矩阵是否可逆没有关系。<br>\n\n为了更好理解可对角化矩阵，接下来我们再从几何角度对其进行讨论。上面说过，从几何上看，相似矩阵在不同的坐标系下执行相同的操作。而一个对角矩阵执行的操作其实很简单：对各个坐标轴执行缩放操作。综上，也就是说，对于一个可对角化矩阵（特化的相似矩阵），它在另一个坐标系下执行着对新的坐标轴执行缩放的操作；更具体地，对于一个可对角化的$n \\times n$矩阵$A$，$v_1, v_2, ..., v_n$是其线性独立的$n$个特征向量，那么$A$执行的线性变换的操作即为：在$v_i$向量方向上执行$\\lambda_i$倍的缩放。于是，特征向量（方向）与特征值（大小）描述着一个矩阵执行的线性变换的重要特征。下面以两组不同特征值为例（$A = CDC^{-1}$）：<br>\n1. 对于$A=\\frac{1}{10}\\left(\\begin{array}{cc}\n11 & 6 \\\\\n9 & 14\n\\end{array}\\right)$，我们有$C=\\left(\\begin{array}{cc}\n2 / 3 & -1 \\\\\n1 & 1\n\\end{array}\\right)、 \\quad D=\\left(\\begin{array}{cc}\n2 & 0 \\\\\n0 & 1 / 2\n\\end{array}\\right)$，也就是说，当$A$乘上一个向量，会使其在$\\left(\\begin{array}{cc}\n2 / 3  \\\\\n1\n\\end{array}\\right)$代表的方向上的坐标放大2倍，使其在$\\left(\\begin{array}{cc}\n-1  \\\\\n1\n\\end{array}\\right)$代表的方向上的坐标缩小为1/2，如图所示，其中蓝色曲线表示$A$执行空间变换的大致方向：<br>\n![](la-63.png)<br>\n2. 对于$A=\\frac{1}{5}\\left(\\begin{array}{cc}\n13 & -2 \\\\\n-3 & 12\n\\end{array}\\right)$，我们有$C=\\left(\\begin{array}{cc}\n2 / 3 & -1 \\\\\n1 & 1\n\\end{array}\\right)、 \\quad D=\\left(\\begin{array}{ll}\n2 & 0 \\\\\n0 & 3\n\\end{array}\\right)$，也就是说，当$A$乘上一个向量，会使其在$\\left(\\begin{array}{cc}\n2 / 3  \\\\\n1\n\\end{array}\\right)$代表的方向上的坐标放大2倍，使其在$\\left(\\begin{array}{cc}\n-1  \\\\\n1\n\\end{array}\\right)$代表的方向上的坐标放大3倍，如图所示，其中蓝色曲线表示$A$执行空间变换的大致方向：<br>\n![](la-64.png)<br>\n以此类推，我们可以以特征向量和特征值去描述一个矩阵的线性空间变换行为。<br>\n\n\n在上面的讨论中，我们只允许实数特征值的出现，在这种情况下，我们会看到矩阵在执行的都是缩放操作。而如果我们允许虚数特征值的存在，情况又会是怎样的呢？下面我们对此进行讨论。<br>\n首先我们先介绍如何求解得到复特征值和复特征向量。类似地，我们求解一个矩阵的特征行列式。对于一个$n \\times n$的矩阵，其特征行列式也为$n$阶，则其对应的复特征值会有$n$个（虚部可能为0）。根据求得的复特征值，我们同样求解Null($A-\\lambda  I_n$)，即可得到复特征向量。下面以矩阵$A=\\left(\\begin{array}{cc}\n1 & -1 \\\\\n1 & 1\n\\end{array}\\right)$为例，我们先求得其特征行列式为：\n$$\nf(\\lambda)=\\lambda^2-2\\lambda+2\n$$\n于是我们求解得到：\n$$\n\\lambda=\\frac{2 \\pm \\sqrt{4-8}}{2}=1 \\pm i\n$$\n我们先对第一个特征值$\\lambda_1=1+i$求其相应特征向量：\n$$\nA-(1+i) I_{2}=\\left(\\begin{array}{cc}\n1-(1+i) & -1 \\\\\n1 & 1-(1+i)\n\\end{array}\\right)=\\left(\\begin{array}{cc}\n-i & -1 \\\\\n1 & -i\n\\end{array}\\right)\n$$\n再对其进行行列操作：\n$$\n\\left(\\begin{array}{cc}\n-i & -1 \\\\\n1 & -i\n\\end{array}\\right) \\stackrel{R_{2}=R_{2}-i R_{1}}{\\longrightarrow}\\left(\\begin{array}{cc}\n-i & -1 \\\\\n0 & 0\n\\end{array}\\right) \\stackrel{R_{1}=R_{1} \\div-i}{\\longrightarrow}\\left(\\begin{array}{cc}\n1 & -i \\\\\n0 & 0\n\\end{array}\\right)\n$$\n于是我们得到$\\left(\\begin{array}{cc}\nx  \\\\\ny\n\\end{array}\\right) = y\\left(\\begin{array}{cc}\ni  \\\\\n1\n\\end{array}\\right)$，即$\\lambda_1$对应的特征向量$v_1 = \\left(\\begin{array}{cc}\ni  \\\\\n1\n\\end{array}\\right)$。同理对于$\\lambda_2 = 1-i$，我们可以得到$v_2 = \\left(\\begin{array}{cc}\n-i  \\\\\n1\n\\end{array}\\right)$。<br>\n另外，因为我们只考虑实数矩阵，那么特征行列式里的系数只会是实数，而对于系数为实数的多项式，其含虚数的复数解总是共轭出现的，如上面例子中的$\\lambda_1=\\bar{\\lambda_2}$、$v_1=\\bar{v_2}$。<br>\n\n接下来我们讨论复特征值和复特征向量的几何意义，先从一种简单但又很重要的矩阵开始——旋转缩放矩阵（rotation-scaling matrices）讲起。其定义为一个有如下形式的$2 \\times 2$矩阵，其中$a$与$b$都是实数且不都等于0：\n$$\n\\left(\\begin{array}{cc}\na & -b \\\\\nb & a\n\\end{array}\\right)\n$$\n对于这样一个矩阵，其可以视为一个旋转矩阵$R$（空间变换操作为将向量逆时针旋转角度$\\theta$）与一个缩放矩阵$S$的乘积：\n$$\nR = \\left(\\begin{array}{cc}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{array}\\right) \\quad \\text {,} \\quad S=\\left(\\begin{array}{ll}\nr & 0 \\\\\n0 & r\n\\end{array}\\right) \\text {. }\n$$\n其中缩放系数$r$：\n$$\nr=\\sqrt{\\operatorname{det}(A)}=\\sqrt{a^{2}+b^{2}}\n$$\n也就是说，从几何上看，一个旋转缩放矩阵乘上一个向量，会对其进行一个角度$\\theta$的旋转并对其缩放$r$倍。这样一个矩阵的特征值为$\\lambda=a\\pm bi$。下面还是以$A=\\left(\\begin{array}{cc}\n1 & -1 \\\\\n1 & 1\n\\end{array}\\right)$为例，我们可视化它的作用。首先$r=\\sqrt{\\operatorname{det}(A)}=\\sqrt{2}$，接着$\\cos \\theta = a/r = 1/\\sqrt{2}, \\sin \\theta = b/r = 1/\\sqrt{2}$，即$\\theta = 45 \\degree$。所以矩阵$A$执行的空间变换：对向量进行$\\sqrt{2}$倍的缩放并逆时针旋转$45\\degree$，如下图所示：<br>\n![](la-65.png)<br>\n但要注意的是，角度不能直接用$\\arctan(·)$求解，因为其输出的角度只在第一四象限之间，如果旋转后的向量在第二三象限的话，我们会得到错误的角度。如对于矩阵$\\left(\\begin{array}{cc}\n-\\sqrt{3} & -1 \\\\\n1 & -\\sqrt{3}\n\\end{array}\\right)$旋转的角度为$150\\degree$，但如果直接使用$\\arctan(·)$求解我们会得到$-30\\degree$。<br>\n![](la-66.png)<br>\n\n有复特征值和复特征向量的矩阵与旋转缩放矩阵又有什么关系呢？这就要引入旋转缩放定理：对于一个$2 \\times 2$的实数矩阵$A$，其一个非实数复特征向量为$\\lambda$，对应的特征向量为$v$，则对于：\n$$\nC=\\left(\\begin{array}{cc}\n\\mid & \\mid \\\\\n\\operatorname{Re}(v) & \\operatorname{Im}(v) \\\\\n\\mid & \\mid\n\\end{array}\\right) \\quad \\text { and } \\quad B=\\left(\\begin{array}{cc}\n\\operatorname{Re}(\\lambda) & \\operatorname{Im}(\\lambda) \\\\\n-\\operatorname{Im}(\\lambda) & \\operatorname{Re}(\\lambda)\n\\end{array}\\right)\n$$\n有$A=CBC^{-1}$（证明参见参考书籍5.5）。可以发现，$B$为一个旋转缩放矩阵。特别地，其对应的缩放系数为特征值的模$|\\lambda|$。上式中Re与Im分别表示实部与虚部，即：\n$$\n\\operatorname{Re}(a+b i)=a \\quad \\operatorname{Im}(a+b i)=b \\quad \\operatorname{Re}\\left(\\begin{array}{c}\nx+y i \\\\\nz+w i\n\\end{array}\\right)=\\left(\\begin{array}{l}\nx \\\\\nz\n\\end{array}\\right) \\quad \\operatorname{Im}\\left(\\begin{array}{l}\nx+y i \\\\\nz+w i\n\\end{array}\\right)=\\left(\\begin{array}{l}\ny \\\\\nw\n\\end{array}\\right)\n$$\n下面以矩阵$A=\\left(\\begin{array}{cc}\n2 & -1 \\\\\n2 & 0\n\\end{array}\\right)$为例，介绍其表示的空间变换。首先我们可以求得其特征值$\\lambda = 1\\pm i$，取$\\lambda = 1-i$求其特征向量为$v=\\left(\\begin{array}{c}\n1 \\\\\n1+i\n\\end{array}\\right)$。接着按照上面的定理我们对$A$进行分解，得到：\n$$\n\\begin{aligned}\nC &=\\left(\\operatorname{Re}\\left(\\begin{array}{c}\n1 \\\\\n1+i\n\\end{array}\\right) \\quad \\operatorname{Im}\\left(\\begin{array}{c}\n1 \\\\\n1+i\n\\end{array}\\right)\\right)=\\left(\\begin{array}{ll}\n1 & 0 \\\\\n1 & 1\n\\end{array}\\right) \\\\\nB &=\\left(\\begin{array}{cc}\n\\operatorname{Re}(\\lambda) & \\operatorname{Im}(\\lambda) \\\\\n-\\operatorname{Im}(\\lambda) & \\operatorname{Re}(\\lambda)\n\\end{array}\\right)=\\left(\\begin{array}{cc}\n1 & -1 \\\\\n1 & 1\n\\end{array}\\right)\n\\end{aligned}\n$$\n由$B$可以知道，矩阵$A$也是在进行一个与上面例子相同的旋转缩放操作，只不过是在$C$列向量对应坐标系下进行。更具体地，当$A$乘上一个向量，先将其转到一个新的坐标系下（$C^{-1}$），接着对其旋转缩放（$B$），之后再将其转回原来的坐标系下（$C$），如下图所示：<br>\n![](la-67.png)<br>\n再以空间点为例子，将其可视化如下：\n![](la-68.png)<br>\n图中橘色线表示每乘上一个$A$，空间点的变换方向。所以类似地，我们也可以讨论不同$\\lambda$对应空间变换的意义。当矩阵$A$的$|\\lambda|>1$，不断乘上$A$会使得向量向外伸张，如下图所示：<br>\n![](la-69.png)<br>\n而如果矩阵$A$的$|\\lambda|=1$，不断乘上$A$会使得向量在一个椭圆环上旋转，如下图所示：<br>\n![](la-70.png)<br>\n而如果矩阵$A$的$|\\lambda|<1$，不断乘上$A$会使得向量向原点收缩，如下图所示：<br>\n![](la-71.png)<br>\n\n以上的讨论我们都基于二维矩阵，而如果矩阵的维度更大的情况会如何？这边就要引入区块对角化定理（block diagonalization theorem）。对于一个$n \\times n$矩阵$A$，如果其特征向量的个数与特征值个数一致，那么其可以分解为$A=CBC^{-1}$，其中$B$是区块对角化矩阵，实特征值对应其对角线上一个数，一组共轭复特征值（选其中一个）对应其对角线上及附近的区块，$C$为它们各自对应的特征向量。以三维矩阵为例，假设其有一组复特征值（其中一个$\\lambda_1$）与一个实特征值$\\lambda_2$，那么其对应的$C$和$B$分别为：<br>\n$$\nC=\\left(\\begin{array}{ccc}\n\\mid & \\mid & \\mid \\\\\n\\operatorname{Re}\\left(v_{1}\\right) & \\operatorname{Im}\\left(v_{1}\\right) & v_{2} \\\\\n\\mid & \\mid & \\mid\n\\end{array}\\right) \\quad B=\\left(\\begin{array}{ccc}\n\\operatorname{Re}\\left(\\lambda_{1}\\right) & \\operatorname{Im}\\left(\\lambda_{1}\\right) & 0 \\\\\n-\\operatorname{Im}\\left(\\lambda_{1}\\right) & \\operatorname{Re}\\left(\\lambda_{1}\\right) & 0 \\\\\n0 & 0 & \\lambda_{2}\n\\end{array}\\right)\n$$\n举一个实例，如对于矩阵$A=\\frac{1}{29}\\left(\\begin{array}{ccc}\n33 & -23 & 9 \\\\\n22 & 33 & -23 \\\\\n19 & 14 & 50\n\\end{array}\\right)$，其特征值为$\\lambda_1 = 1-i, \\bar{\\lambda_1} = 1+i, \\lambda_2=2$，根据特征值又可以求得$v_{1}=\\left(\\begin{array}{c}\n-7-i \\\\\n2-9 i \\\\\n5\n\\end{array}\\right), v_{2}=\\left(\\begin{array}{c}\n2 \\\\\n-1 \\\\\n3\n\\end{array}\\right)$。所以$A=CBC^{-1}$，其中：\n$$\n\\begin{array}{l}\nC=\\left(\\begin{array}{ccc}\n\\mid & \\mid & \\mid \\\\\n\\operatorname{Re}\\left(v_{1}\\right) & \\operatorname{Im}\\left(v_{1}\\right) & v_{2} \\\\\n\\mid & \\mid & \\mid\n\\end{array}\\right)=\\left(\\begin{array}{ccc}\n-7 & -1 & 2 \\\\\n2 & -9 & -1 \\\\\n5 & 0 & 3\n\\end{array}\\right) \\\\\nB=\\left(\\begin{array}{ccc}\n\\operatorname{Re}\\left(\\lambda_{1}\\right) & \\operatorname{Im}\\left(\\lambda_{1}\\right) & 0 \\\\\n-\\operatorname{Im}\\left(\\lambda_{1}\\right) & \\operatorname{Re}\\left(\\lambda_{1}\\right) & 0 \\\\\n0 & 0 & 2\n\\end{array}\\right)=\\left(\\begin{array}{ccc}\n1 & -1 & 0 \\\\\n1 & 1 & 0 \\\\\n0 & 0 & 2\n\\end{array}\\right) .\n\\end{array}\n$$\n矩阵$B$表示的意义是在标准坐标系xy轴方向上对向量旋转$45\\degree$并缩放$\\sqrt{2}$倍，并对z轴上缩放$2$倍。矩阵$A$执行的是相同的操作，只是在$C$对应的坐标系下完成。<br>\n\n## 11. 正交性（Orthogonality）\n\n正交性也是线性代数中很重要的一个概念。我们为什么要在最后提到它呢？它有什么用呢？下面我们将对此进行讨论。<br>\n\n说到正交，我们很容易就联想到我们高中就学过的点积运算，对两个向量$x, y$而言：\n$$\nx \\cdot y=\\left(\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\n\\vdots \\\\\nx_{n}\n\\end{array}\\right) \\cdot\\left(\\begin{array}{c}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{n}\n\\end{array}\\right)=x_{1} y_{1}+x_{2} y_{2}+\\cdots+x_{n} y_{n} .\n$$\n当向量$x, y$满足下面条件，我们有$x, y$正交：\n$$\nx·y = \\|x\\|\\|y\\|\\cos \\alpha = 0\n$$\n其中$\\alpha为向量$x, y$之间的夹角，$$\\|·\\|$表示向量的长度：\n$$\n\\|x\\|=\\sqrt{x \\cdot x}=\\sqrt{x_{1}^{2}+x_{2}^{2}+\\cdots+x_{n}^{2}}\n$$\n长度为1的向量被称为单位向量。另外两点之间的距离即它们组成向量的长度：\n$$\n\\operatorname{dist}(x, y)=\\|y-x\\|\n$$\n\n在知道了两个向量的正交条件之后，进一步地，我们讨论对于一个子空间的正交——**正交补空间（Orthogonal Complement）**。对于$R^n$的子空间$W$而言，其正交补空间为：\n$$\nW^{\\perp}=\\left\\{v \\text { in } \\mathbf{R}^{n} \\mid v \\cdot w=0 \\text { for all } w \\text { in } W\\right\\}\n$$\n也就是说，$W^{\\perp}$是正交于$W$中所有向量的所有$R^n$中的向量的集合。例如，在$R^2$里一条直线的正交补空间是与它垂直的一条直线，在$R^3$中一条直线的正交补空间是与它垂直的一个平面，在$R^3$中一个平面的正交补空间是与它垂直的一条直线。<br>\n![](la-72.png)<br>\n从上图我们也可以看出，$(W^{\\perp})^{\\perp}=W, \\operatorname{dim}(W)+\\operatorname{dim}\\left(W^{\\perp}\\right)=n $。在明确正交补空间的概念之后，接下来我们介绍如何计算它。首先设定$W$是矩阵$A$的列空间（$W=\\text{Col}(A)$），则有（证明参见参考书籍6.2）：\n$$\nW^{\\perp}=\\text{Nul}(A^T)\n$$\n举个例子，比如对于$v_1, v_2$张成的空间（我们可以将它们拼接成一个矩阵，即可以将它们理解为一个矩阵的列空间）：\n$$\nv_{1}=\\left(\\begin{array}{l}\n1 \\\\\n7 \\\\\n2\n\\end{array}\\right) \\quad v_{2}=\\left(\\begin{array}{c}\n-2 \\\\\n3 \\\\\n1\n\\end{array}\\right)\n$$\n其正交补空间为：\n$$\n\\text{Nul} \\left(\\begin{array}{l}\n-v_{1}^{T}- \\\\\n-v_{2}^{T}-\n\\end{array}\\right)=\\text{Nul} \\left(\\begin{array}{ccc}\n1 & 7 & 2 \\\\\n-2 & 3 & 1\n\\end{array}\\right)\n$$\n即求解下列方程组：\n$$\n\\left\\{\\begin{aligned}\nx_{1}+7 x_{2}+2 x_{3} &=0 \\\\\n-2 x_{1}+3 x_{2}+x_{3} &=0 .\n\\end{aligned}\\right.\n$$\n最终我们可以计算得到：\n$$\nW^{\\perp}=\\operatorname{Span}\\left\\{\\left(\\begin{array}{c}\n1 \\\\\n-5 \\\\\n17\n\\end{array}\\right)\\right\\}\n$$\n而如果我们考虑矩阵$A$的行空间$\\text{Row}(A)=\\text{Col}(A^T)$，则有$\\operatorname{Row}(A)^{\\perp}=\\operatorname{Nul}(A)$。总结有：\n$$\n\\begin{aligned}\n\\operatorname{Row}(A)^{\\perp} &=\\operatorname{Nul}(A) & \\operatorname{Nul}(A)^{\\perp} &=\\operatorname{Row}(A) \\\\\n\\operatorname{Col}(A)^{\\perp} &=\\operatorname{Nul}\\left(A^{T}\\right) & \\operatorname{Nul}\\left(A^{T}\\right)^{\\perp} &=\\operatorname{Col}(A) .\n\\end{aligned}\n$$\n利用上述的性质，我们有时候可以很方便地求解正交补空间。如求解矩阵$A=\\left(\\begin{array}{ccc}\n2 & 4 & -1 \\\\\n3 & 2 & 0 \\\\\n-2 & 4 & 3\n\\end{array}\\right)$的5-特征空间的正交补空间：<br>\n先求解5-特征空间：\n$$\nW=\\operatorname{Nul}\\left(A-5 I_{3}\\right)=\\operatorname{Nul}\\left(\\begin{array}{ccc}\n-3 & 4 & -1 \\\\\n3 & -3 & 0 \\\\\n-2 & 4 & -2\n\\end{array}\\right)\n$$\n所以我们直接有：\n$$\nW^{\\perp}=\\operatorname{Row}\\left(\\begin{array}{ccc}\n-3 & 4 & -1 \\\\\n3 & -3 & 0 \\\\\n-2 & 4 & -2\n\\end{array}\\right)=\\operatorname{Span}\\left\\{\\left(\\begin{array}{c}\n-3 \\\\\n4 \\\\\n-1\n\\end{array}\\right),\\left(\\begin{array}{c}\n3 \\\\\n-3 \\\\\n0\n\\end{array}\\right),\\left(\\begin{array}{c}\n-2 \\\\\n4 \\\\\n-2\n\\end{array}\\right)\\right\\}\n$$\n\n 所以对于$R^n$中某个向量$x$，其可以分解为在$R^n$中一个子空间$W$上的投影$x_W$及其正交补空间$W^{\\perp}$上的一个向量$x_{W^{\\perp}}$之和，如下图所示，向量$x$到子空间$W$的距离即为$\\|x_{W^{\\perp}}\\|$。<br>\n ![](la-73.png)<br>\n\n接下来我们讨论如何求解$x_W$与$x_{W^{\\perp}}$。这里要引入一个定理（证明见参考书籍6.3）：设$W=\\operatorname{Col}(A)$，$A$为一个$m \\times n$矩阵，$x$为$R^m$中一个向量，则有：\n$$\nA^TAc = A^Tx\n$$\n其中，$c$为待求解的未知向量，$x_W=Ac$，$x_{W^{\\perp}}=x-x_W$。举个例子，设$L=\\operatorname{Span}\\{u\\}$是$R^n$中一条直线，$x$是$R^n$中一个向量，求解其在$L$上的投影$x_L$。于是我们的问题变为求解$u^Tuc=u^Tx$。在一维的情况下，$u^Tu=u·u， u^Tx=u·x$，所以：\n$$\nx_L=uc=\\frac{(u·x)}{(u·u)}u\n$$\n如下图所示：<br>\n![](la-74.png)<br>\n而当$W$维度更高不只是一条直线，我们要做的也很简单：首先求解$A^TA$与$A^Tx$，然后求解线性方程组得到$c$，然后就可以得到$x_W=Ac，x_{W^{\\perp}}=x-x_W$。举个例子，比如在$R^3$中将向量$x$投影到一个二维平面$W$：\n$$\nW=\\operatorname{Span}\\left\\{\\left(\\begin{array}{c}\n1 \\\\\n0 \\\\\n-1\n\\end{array}\\right),\\left(\\begin{array}{l}\n1 \\\\\n1 \\\\\n0\n\\end{array}\\right)\\right\\} \\quad x=\\left(\\begin{array}{l}\n1 \\\\\n2 \\\\\n3\n\\end{array}\\right)\n$$\n首先求解$A^TA$与$A^Tx$：\n$$\nA^{T} A=\\left(\\begin{array}{ll}\n2 & 1 \\\\\n1 & 2\n\\end{array}\\right) \\quad A^{T} x=\\left(\\begin{array}{c}\n-2 \\\\\n3\n\\end{array}\\right)\n$$\n接着求解$c$：\n$$\n\\left(\\begin{array}{ll|r}\n2 & 1 & -2 \\\\\n1 & 2 & 3\n\\end{array}\\right) \\stackrel{\\mathrm{RREF}}{\\longrightarrow}\\left(\\begin{array}{ll|r}\n1 & 0 & -7 / 3 \\\\\n0 & 1 & 8 / 3\n\\end{array}\\right) \\Longrightarrow c=\\frac{1}{3}\\left(\\begin{array}{c}\n-7 \\\\\n8\n\\end{array}\\right)\n$$\n于是我们得到：\n$$\nx_{W}=A c=\\frac{1}{3}\\left(\\begin{array}{l}\n1 \\\\\n8 \\\\\n7\n\\end{array}\\right) \\quad x_{W^{\\perp}}=x-x_{W}=\\frac{1}{3}\\left(\\begin{array}{c}\n2 \\\\\n-2 \\\\\n2\n\\end{array}\\right)\n$$\n将其可视化如下图所示：<br>\n![](la-75.png)<br>\n\n接下来我们换个角度去思考正交投影这个问题，它本身是一个线性空间变换，那其背后的矩阵应该是什么呢？这里的逻辑其实也很简单，只要我们对原来空间的基向量分别投影到新的空间中，我们就可以知道每个维度上的空间变换，拼接起来就得到正交投影对应的矩阵。举个例子，比如我们要得到代表将$R^3$中标准正交基表示的向量正交投影到直线$W=\\operatorname{Span}\\{u\\}$上的矩阵$B$，其中：\n$$\nu=\\left(\\begin{array}{c}\n-1 \\\\\n1 \\\\\n1\n\\end{array}\\right)\n$$\n那么我们要求的就是$(e_1)_L, (e_2)_L, (e_3)_L$，根据我们上面得到的公式：\n$$\n\\left.\\begin{array}{l}\n\\left(e_{1}\\right)_{L}=\\frac{u \\cdot e_{1}}{u \\cdot u} u=\\frac{1}{3}\\left(\\begin{array}{c}\n1 \\\\\n-1 \\\\\n-1\n\\end{array}\\right) \\\\\n\\left(e_{2}\\right)_{L}=\\frac{u \\cdot e_{2}}{u \\cdot u} u=\\frac{1}{3}\\left(\\begin{array}{c}\n-1 \\\\\n1 \\\\\n1\n\\end{array}\\right) \\\\\n\\left(e_{3}\\right)_{L}=\\frac{u \\cdot e_{3}}{u \\cdot u} u=\\frac{1}{3}\\left(\\begin{array}{c}\n-1 \\\\\n1 \\\\\n1\n\\end{array}\\right)\n\\end{array}\\right\\} \\quad \\Longrightarrow \\quad B=\\frac{1}{3}\\left(\\begin{array}{ccc}\n1 & -1 & -1 \\\\\n-1 & 1 & 1 \\\\\n-1 & 1 & 1\n\\end{array}\\right)\n$$\n类似地，再考虑上面将向量投影到$R^3$中二维平面的例子：\n$$\nW=\\operatorname{Span}\\left\\{\\left(\\begin{array}{c}\n1 \\\\\n0 \\\\\n-1\n\\end{array}\\right),\\left(\\begin{array}{l}\n1 \\\\\n1 \\\\\n0\n\\end{array}\\right)\\right\\}\n$$\n要求其背后的变换矩阵，我们依次求解$(e_1)_W, (e_2)_W, (e_3)_W$。我们先求解$A^TA$与$A^Te_i$，然后求解$c$，接着我们就能得到$(e_i)_W=Ac$：\n$$\n\\left(\\begin{array}{ll|l}\n2 & 1 & 1 \\\\\n1 & 2 & 1\n\\end{array}\\right) \\stackrel{\\mathrm{RREF}}{\\longrightarrow}\\left(\\begin{array}{ll|l}\n1 & 0 & 1 / 3 \\\\\n0 & 1 & 1 / 3\n\\end{array}\\right) \\Longrightarrow\\left(e_{1}\\right)_{W}=A\\left(\\begin{array}{c}\n1 / 3 \\\\\n1 / 3\n\\end{array}\\right)=\\frac{1}{3}\\left(\\begin{array}{c}\n2 \\\\\n1 \\\\\n-1\n\\end{array}\\right)\n$$\n$$\n\\left(\\begin{array}{ll|l}\n2 & 1 & 0 \\\\\n1 & 2 & 1\n\\end{array}\\right) \\stackrel{\\mathrm{RREF}}{\\longrightarrow}\\left(\\begin{array}{ll|r}\n1 & 0 & -1 / 3 \\\\\n0 & 1 & 2 / 3\n\\end{array}\\right) \\Longrightarrow\\left(e_{2}\\right)_{W}=A\\left(\\begin{array}{c}\n-1 / 3 \\\\\n2 / 3\n\\end{array}\\right)=\\frac{1}{3}\\left(\\begin{array}{l}\n1 \\\\\n2 \\\\\n1\n\\end{array}\\right)\n$$\n$$\n\\left(\\begin{array}{ll|r}\n2 & 1 & -1 \\\\\n1 & 2 & 0\n\\end{array}\\right) \\stackrel{\\text { RREF }}{\\longrightarrow}\\left(\\begin{array}{ll|r}\n1 & 0 & -2 / 3 \\\\\n0 & 1 & 1 / 3\n\\end{array}\\right) \\Longrightarrow\\left(e_{3}\\right)_{W}=A\\left(\\begin{array}{c}\n-2 / 3 \\\\\n1 / 3\n\\end{array}\\right)=\\frac{1}{3}\\left(\\begin{array}{c}\n-1 \\\\\n1 \\\\\n2\n\\end{array}\\right)\n$$\n将它们拼接起来，我们可以得到求解的正交投影变换矩阵$B$：\n$$\nB=\\frac{1}{3}\\left(\\begin{array}{ccc}\n2 & 1 & -1 \\\\\n1 & 2 & 1 \\\\\n-1 & 1 & 2\n\\end{array}\\right)\n$$\n或者我们直接从代数角度出发，根据上面的定理，我们有：\n$$\nx_{W}=A\\left(A^{T} A\\right)^{-1} A^{T} x\n$$\n那么正交投影变换矩阵即为：$B = A\\left(A^{T} A\\right)^{-1} A^{T}$。<br>\n\n那么代表着将向量投影到空间$W$的正交投影变换矩阵$B$有什么性质呢？<br>\n1. $\\operatorname{Col}(B)=W$，任何向量经过$B$变换后都在$W$上；<br>\n2. $\\operatorname{Nul}(B)=W^{\\perp}$，$W^{\\perp}$上的向量经过$B$变换后都为0向量；<br>\n3. $B^n=B$，一个向量经过$B$变换到$W$上后，再经过$B$变换会维持不变（$W$上的向量正交投影仍是其自身）；<br>\n4. 如果$W \\neq 0$，那么1是$B$的特征值，$W$就是$B$的1-特征空间；<br>\n5. 如果$W \\neq R^n$，那么0是$B$的特征值，$W^{\\perp}$就是$B$的0-特征空间；<br>\n6. $B$与对角线上有$m$个1与$n-m$个0的对角阵相似，其中$m$是$W$维度，$n$为空间维度。<br>\n\n基于这些性质，我们对上面例子的的正交投影矩阵$B$做进一步讨论，首先可以验证：\n$$\nB^n = B=\\frac{1}{3}\\left(\\begin{array}{ccc}\n2 & 1 & -1 \\\\\n1 & 2 & 1 \\\\\n-1 & 1 & 2\n\\end{array}\\right)\n$$\n接着我们可以求解$W^{\\perp}=\\operatorname{Nul}(B)$，有：\n$$\nW^{\\perp}=\\operatorname{Span}\\left\\{\\left(\\begin{array}{c}\n1 \\\\\n-1 \\\\\n1\n\\end{array}\\right)\\right\\}\n$$\n又我们已知（或者再通过$W=\\operatorname{Col}(B)$求解）：\n$$\nW=\\operatorname{Span}\\left\\{\\left(\\begin{array}{c}\n1 \\\\\n0 \\\\\n-1\n\\end{array}\\right),\\left(\\begin{array}{l}\n1 \\\\\n1 \\\\\n0\n\\end{array}\\right)\\right\\}\n$$\n那么我们就可以直接写出其特征分解式：\n$$\nB=\\left(\\begin{array}{ccc}\n1 & 1 & 1 \\\\\n0 & 1 & -1 \\\\\n-1 & 0 & 1\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right)\\left(\\begin{array}{ccc}\n1 & 1 & 1 \\\\\n0 & 1 & -1 \\\\\n-1 & 0 & 1\n\\end{array}\\right)^{-1}\n$$\n\n接着我们考虑一种特殊情况，$W$中已知的基为正交基（$\\left\\{u_{1}, u_{2}, \\ldots, u_{m}\\right\\}$，每两个向量点乘都为0），那我们可以很轻易地写出$x_W$的形式：\n$$\nx_{W}=\\frac{x \\cdot u_{1}}{u_{1} \\cdot u_{1}} u_{1}+\\frac{x \\cdot u_{2}}{u_{2} \\cdot u_{2}} u_{2}+\\cdots+\\frac{x \\cdot u_{m}}{u_{m} \\cdot u_{m}} u_{m}\n$$\n这个公式其实很好理解，$x \\cdot u_i = \\|x\\|\\cos(\\theta)\\|u_i\\|$，其中$\\|x\\|\\cos(\\theta)$表示$x$投影到$u_i$方向的长度，所以$\\frac{x \\cdot u_{i}}{u_{i} \\cdot u_{i}} u_{i}=\\|x\\|\\cos(\\theta) \\frac{u_i}{\\|u_i\\|}$，$\\frac{u_i}{\\|u_i\\|}$为单位向量，即该式表示$x$分解后在$u_i$方向上的向量，而这些分解后的向量相加之后即可得到$x$。而如果已知的基为标准正交基（每两个向量点乘都为0，向量的大小为1），则这个公式会变得更加简单：\n$$\nx_{W}=\\left(x \\cdot u_{1}\\right) u_{1}+\\left(x \\cdot u_{2}\\right) u_{2}+\\cdots+\\left(x \\cdot u_{m}\\right) u_{m}\n$$\n下面举个例子，比如要将$x$转到以$\\mathcal{B}$为基的空间中得到$x_{\\mathcal{B}}$，其中：\n$$\n\\mathcal{B}=\\left\\{\\left(\\begin{array}{l}\n1 \\\\\n2\n\\end{array}\\right),\\left(\\begin{array}{c}\n-4 \\\\\n2\n\\end{array}\\right)\\right\\} \\quad x=\\left(\\begin{array}{l}\n0 \\\\\n3\n\\end{array}\\right)\n$$\n因为$\\mathcal{B}$中$u_1, u_2$是正交的，我们直接有：\n$$\nx_{\\mathcal{B}}=\\left(\\frac{x \\cdot u_{1}}{u_{1} \\cdot u_{1}}, \\frac{x \\cdot u_{2}}{u_{2} \\cdot u_{2}}\\right)=\\left(\\frac{3 \\cdot 2}{1^{2}+2^{2}}, \\frac{3 \\cdot 2}{(-4)^{2}+2^{2}}\\right)=\\left(\\frac{6}{5}, \\frac{3}{10}\\right)\n$$\n如下图所示：<br>\n![](la-76.png)<br>\n从上面的讨论中，我们可以看到如果知道一个子空间的正交基，要求一个向量到其上的投影是很简便的。那么，如果我们只知道一个子空间的不正交的基，我们有没有可能将它们转换为一组等价的正交基呢？这我们就要引入Gram-Schmidt过程：假设$\\left\\{v_{1}, v_{2}, \\ldots, v_{m}\\right\\}$为$R^n$子空间的$W$的一组基，定义：\n$$\n\\begin{aligned}\n&1.   u_{1}=v_{1} \\\\\n&2.   u_{2}=\\left(v_{2}\\right)_{\\operatorname{span}\\left\\{u_{1}\\right\\}^{\\perp}} \\quad=v_{2}-\\frac{v_{2} \\cdot u_{1}}{u_{1} \\cdot u_{1}} u_{1} \\\\\n&3.  u_{3}=\\left(v_{3}\\right)_{\\text {span }\\left\\{u_{1}, u_{2}\\right\\}^{\\perp}} \\quad=v_{3}-\\frac{v_{3} \\cdot u_{1}}{u_{1} \\cdot u_{1}} u_{1}-\\frac{v_{3} \\cdot u_{2}}{u_{2} \\cdot u_{2}} u_{2}\\\\\n& \\vdots \\\\\n&m.  u_{m}=\\left(v_{m}\\right)_{\\operatorname{span}\\left\\{u_{1}, u_{2}, \\ldots, u_{m-1}\\right\\}^{\\perp}}=v_{m}-\\sum_{i=1}^{m-1} \\frac{v_{m} \\cdot u_{i}}{u_{i} \\cdot u_{i}} u_{i} \\\\\n\\end{aligned}\n$$\n这样我们得到的$\\left\\{u_{1}, u_{2}, \\ldots, u_{m}\\right\\}$就是$W$的一组正交基。举个例子，比如对于$W=\\operatorname{Span}\\left\\{v_{1}, v_{2}, v_{3}\\right\\}=\\mathbf{R}^{3}$，其中：\n$$\nv_{1}=\\left(\\begin{array}{l}\n1 \\\\\n1 \\\\\n0\n\\end{array}\\right) \\quad v_{2}=\\left(\\begin{array}{l}\n1 \\\\\n1 \\\\\n1\n\\end{array}\\right) \\quad v_{3}=\\left(\\begin{array}{l}\n3 \\\\\n1 \\\\\n1\n\\end{array}\\right)\n$$\n基于上面的Gram-Schmidt过程，我们有：\n![](la-77.png)<br>\n当然，如果在这个过程中，$\\left\\{v_{1}, v_{2}, \\ldots, v_{m}\\right\\}$存在线性依赖项，则我们可能得到$u_i=0$，这时候我们丢弃$u_i$即可。<br>\n\n所以总结一下，到目前为止，我们有两种方式去求解一个向量$x$正交投影到子空间$W$上的向量$x_W$：一种需要我们进行矩阵行操作（$A^TAc=A^Tx, x_W=Ac$），另一种则需要一组正交基（直接投影到正交基的各个基上进行组合）。<br>\n\n最后简单介绍一下，如果方程$Ax=b$无解，我们如何通过最小二乘法求解其最优近似解。我们首先在几何上理解这个问题：既然$Ax=b$无解，那么$b$不在$\\operatorname{Col}(A)$空间中，我们将其投影到$\\operatorname{Col}(A)$上得到$b_{\\operatorname{Col}(A)}$，于是我们的问题就变为求解$A\\hat x=b_{\\operatorname{Col}(A)}$，其中$\\hat x$为我们要求的近似最优解。假设$v_1, v_2, \\ldots, v_n$为$A$的列向量，则有：\n$$\nA \\widehat{x}=A\\left(\\begin{array}{c}\n\\widehat{x}_{1} \\\\\n\\widehat{x}_{2} \\\\\n\\vdots \\\\\n\\widehat{x}_{n}\n\\end{array}\\right)=\\widehat{x}_{1} v_{1}+\\widehat{x}_{2} v_{2}+\\cdots+\\widehat{x}_{n} v_{n}\n$$\n上述各变量的关系如下图所示：<br>\n![](la-78.png)<br>\n也就是说，只要应用上面我们讨论的正交投影的方法得到$b_{\\operatorname{Col}(A)}$，接下来我们需要做的就是求解方程组$A\\hat x=b_{\\operatorname{Col}(A)}$。当然，如果$A$的列向量的个数高于子空间$\\operatorname{Col}(A)$的维度，则方程的解不唯一，如下图所示：<br>\n![](la-79.png)<br>\n在实际应用中，我们需要做的就是把任意函数中未知的参数（一次函数、二次函数、三角函数...）视为$x$，数据构成矩阵$A$与向量$b$，接着就可以进行求解了。下面给出一个简单的例子：求拟合数据点$(-1, 1/2), (-1, 1), (2, -1/2), (3, 2)$的最优二次函数$y=Bx^2+Cx+D$。代入数据，我们有：\n$$\n\\begin{aligned}\n\\frac{1}{2} &=B(-1)^{2}+C(-1)+D \\\\\n-1 &=B(1)^{2}+C(1)+D \\\\\n-\\frac{1}{2} &=B(2)^{2}+C(2)+D \\\\\n2 &=B(3)^{2}+C(3)+D\n\\end{aligned}\n$$\n于是：\n$$\nA=\\left(\\begin{array}{rrr}\n1 & -1 & 1 \\\\\n1 & 1 & 1 \\\\\n4 & 2 & 1 \\\\\n9 & 3 & 1\n\\end{array}\\right) \\quad x=\\left(\\begin{array}{l}\nB \\\\\nC \\\\\nD\n\\end{array}\\right) \\quad b=\\left(\\begin{array}{r}\n1 / 2 \\\\\n-1 \\\\\n-1 / 2 \\\\\n2\n\\end{array}\\right)\n$$\n这样我们就定义出了一个求最优近似解的方程组，应用上面我们讨论的正交投影的方法，可以得到$B, C, D$大小，即有：\n$$\ny=\\frac{53}{88} x^{2}-\\frac{379}{440} x-\\frac{41}{44}\n$$\n\n\n\n---\n> 参考书籍：Interactive Linear Algebra - by Dan Margalit, Joseph Rabinoff (https://textbooks.math.gatech.edu/ila/index2.html)\n","tags":["基本数学"]},{"title":"《网络科学》 第二章 图论 - Albert-Laszlo Barabasi （TBD）","url":"/2021/11/24/network-science-2/","content":"\n## 1. 七桥问题(The Bridges of Konigsberg)\n\n在真正开始讲解图论的内容之前，我们可以先回答一个问题：图论起源于什么？这可以追溯到1975年的Konigsberg这座当时的大都市。当时的政府想要在几片不相连的土地（下图中的A、B、C和D）上建造7座桥，以方便居民们的生活，如下图所示。<br>\n![](figure-2-1.jpg)<br>\n而当时爱思考的人就提出了这么一个问题：一个人有没有可能经过所有七座桥，但过程中不经过任何一座桥两次呢？针对这个问题，尽管有许多人做了尝试，但都以失败告终。最后是欧拉（对，就是那个大名鼎鼎的数学家）从数学上证明了这样的路径根本不存在，而这也意味着图论的诞生。<br>\n为什么这么说呢？欧拉是这么解构这个问题的。<br>\n\n\n\n上面的问题构成了一个经典的图论问题，用现在图论的语言来说，当时欧拉给出的结论是：<br>\n- 如果一个图中有大于两个结点的度是奇数，那么这样的路径不存在；\n- 如果一个图中没有一个结点的度为奇数，那么至少有一条这样的路径。<br>\n","tags":["网络科学"]},{"title":"《网络科学》 序言 & 第一章 简介 - Albert-Laszlo Barabasi","url":"/2021/11/23/network-science-1/","content":"\n## 序言\n\n在正式介绍书的内容之前，作者首先对如何使用这本书提出了自己的教学建议，也给了一些线上的资源以供参考。\n\n### 教学建议：\n书里首先给出了一些《网络科学》教学的建议，建议包括内容为作业（Homework Problems）、Wiki任务（Wiki Assignment）、社交网络分析（Social Network Analysis）、终期研究项目（Final Research Project）、软件使用（Software）、电影夜（Movie Night）以及客座讲授（Guest Speakers）等。\n课程可分为14周，具体安排如下表所示：<br>\n![](syllabus.jpg)<br>\n\n### 资源：\n[在线电子书及配套资源](http://barabasi.com/NetworkScienceBook \"快点我学习\") ：网站上的资源包括：在线电子书、PPT Slides、数据集等。<br>\n\n   推荐专业书籍：<br>\n![](books1.png)<br>\n![](books2.png)<br>\n\n   推荐科普书籍：<br>\n![](books3.png)<br>\n\n\n## 第一章 简介\n### 1. 相互连接导致的脆弱性 （Vulnerability Due to Interconnectivity）\n我们生活中何处会有网络科学的身影？让我们首先从2003年北美大规模断电(blackout)的故事讲起。<br>\n\n![](2003blackout.png)<br>\n\n这是当时大断电前（左）后（右）的卫星图，仔细看的话，会发现，大断电后，Toronto, Detroit, Cleveland, Columbus, Long Island几个城市/地区在卫星图上看不到了，换句话说，这些城市断电了，这也导致了当时北美几百万人回归无电时代。但这件事是怎么发生的？以后又要怎么避免类似的事情发生呢？<br>\n\n在电力**网络**系统中，一个局部的电力过载（失败）会转移到其他**结点**上，如果这个系统过载结点外的其他部分能够“消化”掉这个过载，那么这个过载就是无所谓的，这个系统就是鲁棒（Robust）的；而如果这个过载（失败）太强，当前过载结点的相邻结点也无法承载，那么相邻结点也会过载，进而造成连锁反应，使得整片的电力网络瘫痪，而这就是上述事件发生的原因。更一般地说，相互连接引起了足够大的非局部性（Non-locality），它允许了如电力等能够在其所在网络上进行传播，不管距离有多远。一句话概之：这反映了网络中**相互连接导致的脆弱性**。<br>\n\n而如何避免这种情况呢？这与**网络结构**有关。而什么样的网络结构会更鲁棒、稳健？怎么定量描述网络结构与其动力过程，及其对失败（failure）的影响？又如何预测失败的发生？此处先不具体回答，具体在之后的章节中我们都会得到答案。<br>\n\n上文所述的连锁反应的失败（Cascading failure）其实在大多复杂系统中都可能存在，如互联网中的网络过载、金融网络连锁崩溃（金融危机）、社交网络中的谣言传播等等，这也可见网络科学其实广泛存在于我们的生活中。<br>\n\n那么，这就引出一个问题了，什么是复杂系统呢？<br>\n\n### 2. 网络是复杂系统的核心（Networks at the Heart of Complex Systems）\n霍金说过：“下个世纪会是复杂性科学的世纪。”<br>\n\n![](Hawking.png)<br>\n\n如上文所述，在我们生活中，随处可见都是非常复杂的系统，如由几百亿个体合作构成的社会、连接着几百亿通讯工具的通讯系统、每个人由几百亿神经元协同运作的大脑、细胞中几千种基因与代谢物质的交互网络等等，这都是复杂系统的例子。简单来说，在复杂系统中，只知道系统各组分的性质，几乎不可能去推测系统的集体行为，总体并不只是个体的堆砌，具体定义可见[维基百科](https://zh.wikipedia.org/wiki/%E5%A4%8D%E6%9D%82%E7%B3%BB%E7%BB%9F \"复杂系统-维基百科\")。由于复杂系统在我们生活中如此常见，理解它们显然变得十分重要，而这也会是21世纪人类将面临的巨大科学难题。<br>\n\n**而在每个复杂系统背后，总会有一个决定了其中各组分如何交互的网络。** 因此，如果我们没有对网络有一个深刻的理解，就更不可能去理解复杂系统的行为了。<br>\n\n![](humanGenes.png)<br>\n\n那当前去发展网络科学时机是否已经成熟？<br>\n\n### 3. 为什么当前网络科学得以发展？（Two Forces Helped the Emergence of Network Science & Societal and Scientific Impacts）\n\n图论早由欧拉发展于1735年，那为什么当时没有进一步将图论发展为网络科学呢？<br>\n\n- 当前网络图谱的涌现（The emergence of network maps）。以建立一个社交网络为例，我需要知道你的朋友，你的朋友的朋友，你的朋友的朋友的朋友，以此类推。这在过去需要花费非常多的精力，是一个几乎不可能的任务。而在现在，有了网络社交的加持，用用爬虫技术得到社交帐户的好友列表，这件事竟也变得易如反掌。同理，有了现代技术的加持，我们还能得到线虫（C. Elegans)全神经元连接网络，引文网络，蛋白质代谢网络等等。而这些在两个世纪前并不可能做到。<br>\n\n但有了建立网络的条件，我们就一定要研究网络科学吗？这就不得不提它下面这个特点。<br>\n\n- 网络的普适性（The unibersality of network characteristics）。网络科学中一个关键的发现是：在不同的学科领域中建立起来的网络，竟然有令人惊讶的相似的架构，遵循着同样的组织原则。换句话说，如果我们有了一套完整的网络科学的数学工具，我们就可以用它来探索不同的系统。<br>\n\n除此之外，它又有什么重要的现实意义吗？\n- 社会影响（Societal impact）。如在网络上建模的技术/商业模型（互联网、搜索引擎、社交网络等）带来的巨大经济效益；建立传染病网络模型对其传播进行预测对人群健康的影响；乃至于运用网络分析找到本拉登居所这种在打击恐怖主义上的应用等等。\n- 科学影响（Scientific impact）。如建立人类大脑连接组、药物设计(要考虑细胞内的连接图)等都离不开网络科学的应用。从2000年到现在，几篇网络科学的经典文章被引用也越来越多，有关网络科学的文章发表数量也越来越多。\n\n> **几篇经典文章：**<br>\n> - 1998: Watts-Strogatz paper in the most cited Nature  publication from 1998; highlighted by ISI as one of the ten most cited papers in physics in the decade after its publication.<br>\n> - 1999: Barabasi and Albert paper  is the most cited  Science paper in 1999;highlighted by ISI as one of the ten most cited papers in physics in the decade after its publication. <br>\n> - 2001: Pastor -Satorras and Vespignani is one of the two most cited papers among the papers published in 2001 by Physical Review Letters.<br>\n> - 2002: Girvan-Newman is the most cited paper in 2002  Proceedings of the National Academy of Sciences.<br>\n\n吹了这么多，也还是可以看出网络科学确实有发展的必要，也是比较有前景的。最后从一个比较大的角度总结一下网络科学的特点，然后就正式开始网络科学的学习。\n\n\n### 4. 网络科学的特点（The characteristics of Network Science）\n- 学科交叉性（Interdisciplinary）。正如上文所述，在不同的学科中都能看到网络科学的影子，网络科学也能在很多领域发挥巨大的作用。\n- 经验性、数据驱动性（Empirical, data-driven）。这一点是网络科学与图论最大的不同，网络科学不只是为了发展抽象的理论数学工具，而是为了洞见真实数据背后的价值（Insight）。\n- 定量与数学性（Quantitative and Mathematical）。网络科学背后最重要的图论（数学）中图的形式与统计物理中对随机性处理的概念框架。当然，在其中我们也可以看到一些控制论、信息论的影子。\n- 计算性（Computational）。一般而言，网络科学家不可避免会遇到“大数据”的问题，因此相关研究也对计算机算力提出了比较高的要求。\n\n\n\n\n\n\n---\n> 参考书籍：Network Science - by Albert-Laszlo Barabasi (http://networksciencebook.com/)\n","tags":["网络科学"]},{"title":"Learning what we know and knowing what we learn Gaussian Process priors for neural data analysis - Kris Jensen & Guillaume Hennequin (TBD)","url":"/2021/11/09/Learning-what-we-know-and-knowing-what-we-learn/","content":"\nTopic: Probabilistic latent variable models\n\n## 1. Latent variable models in neuroscience\n### 1.1 Principle component analysis (PCA)\nsensitive to trial-to-trial noise, so it needs a large amount of trials.\nLinear Assumption between the latent variables and the observations of nerual activity.\n### 1.2 Gassuain process factor analysis - Developed by Byron Yu, et al.\ncommonly used to analyze single trial data. It allows information sharing across time, which can help denoise single trial latent trajectories.\nLinear Assumption between the latent variables and the observations of nerual activity.\n### 1.3 Gausian process latent variable models (LVMs)\nAllow to fit non-linear observations data.\n### 1.4 PfLDS\nRely on the rise of deep learning.\nComine a linear dynamical system with like Kalman Filter, with a readout that is basically a deep network, which can learn by fitting it to a large quantity of neural data.\nlinear dynamical system with a nonlinear readout.\n### 1.5 LFADS\nnonlinear dynamical system with a linear readout.\n\n前三个 non-parametric methods, 后两个dynamical system (parametric) methods\n\nbrige this gap: GPFADS (Gaussian Process Factor Analysis with Dynmaical Systems)\n\n## 2. Where do we begin? Bayesian Inference!\n\njpg.\n\nPrimer on Gaussian Processes\n\nGP1.png\nClearly the answer depends on prior beliefs.\n比如如果知道noise很大，那么可能就是一条横线。\n\nGP4\n用covariance来表示smoothness。\n","tags":["计算神经科学","神经数据分析"]},{"title":"最优控制（Optimal Control) - Maurice Smith (Harvard University) NMA (TBD)","url":"/2021/10/26/Optimal-control/","content":"\n生物要有大脑就是因为需要运动！但大脑怎么计划、协调、执行运动命令呢？最优控制（Optimal Control）是一种可能的解释。但在开始深入讨论之前，我们需要先定下一个数学框架以便讨论。\n\n## 1. 最优控制的数学框架\n\n假设有一个动力系统，它的 **状态** 为 $(s)$ ，这里的状态指的是：用于描述、决定系统行为的变量的集合。\n\n如果这个动力系统中，状态的变化只由状态本身决定，一个通用的描述此动力系统的方程如下：\n\n$$ \\dot{s}(t) = \\frac{\\mathrm{d}s}{\\mathrm{d}t} = D(s(t)) $$\n\n如果简化为线性动力系统，则可以描述为：\n\n$$ \\dot{s}(t) = \\frac{\\mathrm{d}s}{\\mathrm{d}t} = D·s(t) $$\n\n那么这个动力系统即被称为 **自治动力系统（Autonomous Dynamical System)** 。\n\n然而这是非常简单的形式，现实中我们经常遇到的动力系统都会接收外部的输入,记为 $(a)$ (action, 在动力系统上采取的行动)， 那么类似地， **有外部输入 $(a)$ 的动力系统** 的通用描述方程和线性描述方程分别为：\n\n$$ \\dot{s}(t) = \\frac{\\mathrm{d}s}{\\mathrm{d}t} = D(s(t), a(t)) $$\n\n$$ \\dot{s}(t) = \\frac{\\mathrm{d}s}{\\mathrm{d}t} = D·s(t) + B·a(t)$$\n\n\n下面引入一个简单的例子。假设有一只松鼠在一棵树上，它想要吃到另一棵树上的坚果，这时候它很饥饿，所以它想要尽可能快地吃到那个坚果，\n\n\n![](SquirrelJump.jpg)\n","tags":["计算神经科学","运动理论","Neuromatch Academy (NMA)"]},{"title":"基于智能计算的脑机制研究-邬霞","url":"/2021/08/21/基于智能计算的脑机制研究-邬霞/","content":"\n## 脑机制的发展方向：\n### 1. 认识脑\n\n现在的研究认为人脑是以网络的形式相互作用的。结合图论的方法分析大脑功能连接网络的拓扑机构，探索功能区域大的相互作用关系，进而揭示人脑内部的工作原理。\n\n### 2. 保护脑\n\n了解脑发育和衰老过程；神经性、神经性疾病的康复和预防。\n### 3. 创造脑\n\n开发脑型计算机、提升大脑功能（脑机接口）。\n\n---\n\n## 脑机制的研究方法：\n\n解剖学方法、生物化学方法、无创脑影像技术、电生理学方法、分子生物学方法等不同尺度层面的研究方法。（研究技术的发展促进脑机制的研究）\n\n---\n\n## 复杂脑网络的研究主要依赖于测量数据与计算模型：\n1. 数据驱动的脑网络分析（脑影像数据信噪比低、高维小样本的特性，使得脑网络精确构建难）；\n2. 基于模型的脑网络计算（计算模型抽象、脑网络复杂，使得脑网络分析与行为关联刻画难）。\n\n---\n\n## 引入智能计算方法（研究内容）：\n### 1. 脑网络成分识别（哪些脑区构成一个网络）\n- 稀疏模块化可重叠的高斯图模型算法（引入先验知识：稀疏、模块化，找出不同组）\n- 希尔伯特-黄变换，提取时频指标（数据驱动分组）\n\n### 2. 脑连接效应/有效连接分析（上述网络之间如何连接）\n- 基于fMRI的因果网络构建与验证 （AD与正常人群的不同）\n- 构建情感冲突状态下抑郁症患者的脑功能连接 （EEG，动态时间规整，功能连接）\n- 基于EEG源信号的有向连接网络构建 （先溯源，再分频段建立连接）\n\n### 3. 脑认知行为预测（挖掘有效生物标记物）\n- 正常认知行为——自恋人格的个体化预测 （算各种网络指标，之后相关分析：Exploratory Correlation Analysis, Pearson Correlation Analysis）\n- 正常认知行为——情绪预测 （挖掘影响个体之间对不公平忍受度的网络标志物）\n- 正常认知行为——认知负荷评估 （不同认知负荷下网络标志物不同）\n- 脑疾病辅助预测——阿尔兹海默症、抑郁症等\n\n### 4. 脑状态解码研究\n- 高密度EEG数据的情感计算\n- 基于大脑时空共变特性的脑状态评估 （fMRI, 考虑时间信息，方法：Deep Sparse Recurrent Auto-encoder）\n\n\n\n\n---\n>记录自：2021.8.21 基于智能计算的脑机制研究-邬霞-北京师范大学（多模态认知计算与脑机智能）\n","tags":["讲座记录"]},{"title":"Markdown的基本语法（英文稿）","url":"/2019/03/11/Basic-grammar-of-Markdown/","content":"\n# 1. Header:\n  The symbol \"#\" indicates the level of a header, which is shown as follows:\n  # first-level header\n  >\"# first-level header\": first-level header\n\n  ## second-level header\n  >\"## second-level header\": second-level header\n\n  ##### fifth-level Header\n  >\"##### fifth-level header\": fifth-level header\n\n  And so on.\n\n# 2. Typeface:\n- Bold: \\**The text to be bold**\n\n  Result: **The text to be bold**\n- Italic: \\*The test to be italic*\n\n  Result: *The text to be italic*\n- Bold and Italic: \\*\\*\\*The test to be bold and italic*\\**\n\n  Result: ***The test to be bold and italic***\n\n- Strikethrough: \\~~The test to be strikethrough~~\n\n  Result: ~~The test to be trikethough~~\n\n- Newline: two ' ' in the gap between the 2 texts. Or < br> (No ' ').\n\n  Result:\n  **Tip:**\n    If you want to share your library by copying the files, you need to copy both the .data file and the .enl file. They need to work together.\n\n    <br>\n\n- Be placed in the middle: < center>markdown< /center> (No ' ').\n<center>markdown</center>\n\n# 3. Quotation:\n  The symbol \">\" indicates the level of a quotation just as what \"#\" does.\n\n  Grammar: > the quotation\n\n    >This is the first-level quotation\n\n  And so on.\n\n# 4. Split Line:\n  Symbol: more than 3 \"-\" or \"\\*\", the results are shown as follows:\n\n---\n***\n\n# 5.Figure:\n  Grammar: \\!\\[the caption(shown below the figure)](the address of the figure (can be online or offline)\"the title of the figure (shown when the mouse is over the figure)\")\n\n  ![Online](https://pic.36krcnd.com/201801/25121602/9hk589zocc2etptd!heading \"BCI1\")\n\n  ![Offline](https://raw.githubusercontent.com/1997Jonas/1997Jonas.github.io/master/images/BCI2.jpg \"BCI2\")\n\n  However, the offline way is not recommended. (Offline way cannot be use in deployment of a website)\n\n  Place the figure in the middle:\n  < div align=center> (No the first ' ').\n\n  \\!\\[  ](   )\n\n  < div align=left> (No the first ' ')\n\n  Result:\n\n  <div align=center>\n\n  ![](https://latex.codecogs.com/gif.latex?X%5Ccdot%20%5Cfrac%7BX%7D%7BY%7D)\n\n<div align=left>\n\n  set the figure's size:\n < img **width=200** src=\"the link of the figure.png\" >\n\n  Result:\n\n![](https://raw.githubusercontent.com/1997Jonas/1997Jonas.github.io/master/images/ERDERS6.png)\n\n\n  <img width=200 src=\"https://raw.githubusercontent.com/1997Jonas/1997Jonas.github.io/master/images/ERDERS6.png\" >\n\n\n# 6. Hyperlink:\n  Grammar: \\[Hyperlink's name](Address \"title\")\n\n  [Emoji's website](https://www.webfx.com/tools/emoji-cheat-sheet/ \"Emoji's website\")\n\n\n\n# 7. List:\n- Unordered: symbol ‘-’ or '+' or '\\*', the result is just like this line.\n\n\n1. Ordered: 1. or 2. or 3. and so on, the result is just like this line.\n\n\n- nesting: three ' '(blank) between the indexes.\n\n# 8. Table:\nGrammar:(No blank line!)\n\nheader 1|header 2|deader 3\n\n---|:--:|---:\n\ncontent 11|content 21|content 31\n\ncontent 12|content 22|content 32\n\nThe result is as follows:\n\n\n\n  header 1|header 2|deader 3\n  ---|:--:|---:\n  content 11|content 21|content 31\n  content 12|content 22|content 32\n\n# 9. Code:\nGrammar:\n- 'One-line code'\n\n  `create database hero;`\n- ''' A passage of code'''\n\n  ```\n      function fun(){\n           echo \"Hello world!\";\n      }\n      fun();\n  ```\n\n# 10. Formula:\n  Use the website: [LateXFormula](https://www.codecogs.com/latex/eqneditor.php)\n  Create the formula and then Choose 'URL Encoded' option below.\n  Copy the URL and then paste it in Markdown.\n\n  Example: ![](https://latex.codecogs.com/gif.latex?X%5Ccdot%20%5Cfrac%7BX%7D%7BY%7D)\n  <br/>\n","tags":["科研基本技能"]}]